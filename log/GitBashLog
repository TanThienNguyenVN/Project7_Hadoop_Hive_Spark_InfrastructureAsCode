thien@TAM-PC MINGW64 /
$ git --version
git version 2.33.1.windows.1

thien@TAM-PC MINGW64 /
$ cd d:

thien@TAM-PC MINGW64 /d
$ cd MScDataScience/

thien@TAM-PC MINGW64 /d/MScDataScience
$ cd 04.InfrastructureBigData/

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData
$ pwd
/d/MScDataScience/04.InfrastructureBigData

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData
$ mkdir CA3

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData
$ cd CA3

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3
$ pwd
/d/MScDataScience/04.InfrastructureBigData/CA3

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3
$ git init
Initialized empty Git repository in D:/MScDataScience/04.InfrastructureBigData/CA3/.git/

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (master)
$ echo "# ca3-TanThienNguyenVN" >> README.md

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (master)
$ git add README.md
warning: LF will be replaced by CRLF in README.md.
The file will have its original line endings in your working directory

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (master)
$ git commit -m "first commit"
Author identity unknown

*** Please tell me who you are.

Run

  git config --global user.email "you@example.com"
  git config --global user.name "Your Name"

to set your account's default identity.
Omit --global to set the identity only in this repository.

fatal: unable to auto-detect email address (got 'thien@TAM-PC.(none)')

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (master)
$
$ git config --global user.name "TanThienNguyenVN"
$ git config --global user.email "thientannguyen209@gmail.com"


$ git commit -m "first commit"
[master (root-commit) 161bbe3] first commit
 1 file changed, 1 insertion(+)
 create mode 100644 README.md
 
 ------ 03/12/2021 ---
 
 thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        modified:   README.md

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        NoteLog.txt
        existingArchitecture.PNG
        proposalArchitecture.PNG

no changes added to commit (use "git add" and/or "git commit -a")

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is ahead of 'origin/main' by 1 commit.
  (use "git push" to publish your local commits)

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        modified:   README.md

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        NoteLog.txt
        proposalArchitecture.PNG

no changes added to commit (use "git add" and/or "git commit -a")

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git push
Enumerating objects: 4, done.
Counting objects: 100% (4/4), done.
Delta compression using up to 4 threads
Compressing objects: 100% (3/3), done.
Writing objects: 100% (3/3), 33.82 KiB | 11.27 MiB/s, done.
Total 3 (delta 0), reused 0 (delta 0), pack-reused 0
To https://github.com/ITC-IFBD/ca3-TanThienNguyenVN.git
   c920e03..d766c2c  main -> main

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$
thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add README.md
warning: LF will be replaced by CRLF in README.md.
The file will have its original line endings in your working directory

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git commit -m "Adding Problem Statement 03/12/2021" README.md
warning: LF will be replaced by CRLF in README.md.
The file will have its original line endings in your working directory
[main bd98b91] Adding Problem Statement 03/12/2021
 1 file changed, 80 insertions(+), 3 deletions(-)
 rewrite README.md (96%)

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git push
Enumerating objects: 5, done.
Counting objects: 100% (5/5), done.
Delta compression using up to 4 threads
Compressing objects: 100% (3/3), done.
Writing objects: 100% (3/3), 1.30 KiB | 1.30 MiB/s, done.
Total 3 (delta 0), reused 0 (delta 0), pack-reused 0
To https://github.com/ITC-IFBD/ca3-TanThienNguyenVN.git
   d766c2c..bd98b91  main -> main

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$
thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        modified:   README.md

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        NoteLog.txt
        proposalArchitecture.PNG

no changes added to commit (use "git add" and/or "git commit -a")

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add proposalArchitecture.PNG

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git commit -m "Add proposal Architecture into git" proposalArchitecture.PNG
[main 324eaa6] Add proposal Architecture into git
 1 file changed, 0 insertions(+), 0 deletions(-)
 create mode 100644 proposalArchitecture.PNG

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git push
Enumerating objects: 4, done.
Counting objects: 100% (4/4), done.
Delta compression using up to 4 threads
Compressing objects: 100% (3/3), done.
Writing objects: 100% (3/3), 85.25 KiB | 17.05 MiB/s, done.
Total 3 (delta 0), reused 0 (delta 0), pack-reused 0
To https://github.com/ITC-IFBD/ca3-TanThienNguyenVN.git
   bd98b91..324eaa6  main -> main

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        modified:   README.md

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        NoteLog.txt

no changes added to commit (use "git add" and/or "git commit -a")

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add README.md
warning: LF will be replaced by CRLF in README.md.
The file will have its original line endings in your working directory

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git commit -m "Add Solution Proposal and other stuffs into README.md 03/12/2021" README.md
warning: LF will be replaced by CRLF in README.md.
The file will have its original line endings in your working directory
[main 42d2a7d] Add Solution Proposal and other stuffs into README.md 03/12/2021
 1 file changed, 38 insertions(+), 26 deletions(-)

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git push
Enumerating objects: 5, done.
Counting objects: 100% (5/5), done.
Delta compression using up to 4 threads
Compressing objects: 100% (3/3), done.
Writing objects: 100% (3/3), 1.12 KiB | 1.12 MiB/s, done.
Total 3 (delta 1), reused 0 (delta 0), pack-reused 0
remote: Resolving deltas: 100% (1/1), completed with 1 local object.
To https://github.com/ITC-IFBD/ca3-TanThienNguyenVN.git
   324eaa6..42d2a7d  main -> main

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        GitBashLog

nothing added to commit but untracked files present (use "git add" to track)

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add GitBashLog

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git commit -m "Add GitBashLog into git" GitBashLog
[main dcf5a27] Add GitBashLog into git
 1 file changed, 224 insertions(+)
 create mode 100644 GitBashLog

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git push
Enumerating objects: 4, done.
Counting objects: 100% (4/4), done.
Delta compression using up to 4 threads
Compressing objects: 100% (3/3), done.
Writing objects: 100% (3/3), 1.67 KiB | 1.67 MiB/s, done.
Total 3 (delta 1), reused 0 (delta 0), pack-reused 0
remote: Resolving deltas: 100% (1/1), completed with 1 local object.
To https://github.com/ITC-IFBD/ca3-TanThienNguyenVN.git
   42d2a7d..dcf5a27  main -> main

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$


---- 12 December 2021 
thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        modified:   GitBashLog

no changes added to commit (use "git add" and/or "git commit -a")

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git remote show origin
* remote origin
  Fetch URL: https://github.com/ITC-IFBD/ca3-TanThienNguyenVN.git
  Push  URL: https://github.com/ITC-IFBD/ca3-TanThienNguyenVN.git
  HEAD branch: main
  Remote branch:
    main tracked
  Local branch configured for 'git pull':
    main merges with remote main
  Local ref configured for 'git push':
    main pushes to main (up to date)

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)


------ 15 December 

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ vagrant ssh node01
Welcome to Ubuntu 20.04.3 LTS (GNU/Linux 5.4.0-90-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Wed Dec 15 19:08:17 UTC 2021

  System load:  1.42              Processes:               127
  Usage of /:   6.4% of 38.71GB   Users logged in:         0
  Memory usage: 15%               IPv4 address for enp0s3: 10.0.2.15
  Swap usage:   0%                IPv4 address for enp0s8: 172.16.0.11

 * Super-optimized for small spaces - read how we shrank the memory
   footprint of MicroK8s to make it the smallest full K8s around.

   https://ubuntu.com/blog/microk8s-memory-optimisation

8 updates can be applied immediately.
8 of these updates are standard security updates.
To see these additional updates run: apt list --upgradable


*** System restart required ***
vagrant@node01:~$ java -version
openjdk version "1.8.0_292"
OpenJDK Runtime Environment (build 1.8.0_292-8u292-b10-0ubuntu1~20.04-b10)
OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode)
vagrant@node01:~$ echo $JAVA_HOME
/usr/lib/jvm/jdk
vagrant@node01:~$ echo $HADOOP_HOME
/usr/local/hadoop
vagrant@node01:~$ echo $PATH
/usr/lib/jvm/jdk/bin:/usr/local/hadoop/sbin:/usr/local/hadoop/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin
vagrant@node01:~$ ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 02:65:a5:66:b2:98 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic enp0s3
       valid_lft 86118sec preferred_lft 86118sec
    inet6 fe80::65:a5ff:fe66:b298/64 scope link
       valid_lft forever preferred_lft forever
3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 08:00:27:1f:c3:ed brd ff:ff:ff:ff:ff:ff
    inet 172.16.0.11/24 brd 172.16.0.255 scope global enp0s8
       valid_lft forever preferred_lft forever
    inet6 fe80::a00:27ff:fe1f:c3ed/64 scope link
       valid_lft forever preferred_lft forever
vagrant@node01:~$ cd /etc
vagrant@node01:/etc$ more environment
PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/u
sr/local/games:/snap/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin"

JAVA_HOME ="/usr/lib/jvm/jdk"
vagrant@node01:/etc$
vagrant@node01:/etc$ cd $HADOOP_HOME
vagrant@node01:/usr/local/hadoop$ cd etc
vagrant@node01:/usr/local/hadoop/etc$ cd hadoop/
vagrant@node01:/usr/local/hadoop/etc/hadoop$ ls
capacity-scheduler.xml            kms-log4j.properties
configuration.xsl                 kms-site.xml
container-executor.cfg            log4j.properties
core-site.xml                     mapred-env.cmd
hadoop-env.cmd                    mapred-env.sh
hadoop-env.sh                     mapred-queues.xml.template
hadoop-metrics2.properties        mapred-site.xml
hadoop-policy.xml                 shellprofile.d
hadoop-user-functions.sh.example  ssl-client.xml.example
hdfs-site.xml                     ssl-server.xml.example
httpfs-env.sh                     user_ec_policies.xml.template
httpfs-log4j.properties           workers
httpfs-signature.secret           yarn-env.cmd
httpfs-site.xml                   yarn-env.sh
kms-acls.xml                      yarn-site.xml
kms-env.sh                        yarnservice-log4j.properties
vagrant@node01:/usr/local/hadoop/etc/hadoop$ more workers
localhost
vagrant@node01:/usr/local/hadoop/etc/hadoop$ more core-site.xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
  <property>
    <name>fs.default.name</name>
    <value>hdfs://node01:9000</value>
  </property>
</configuration>
vagrant@node01:/usr/local/hadoop/etc/hadoop$
vagrant@node01:/usr/local/hadoop/etc/hadoop$
vagrant@node01:/usr/local/hadoop/etc/hadoop$ ping google.com
PING google.com (209.85.203.138) 56(84) bytes of data.
64 bytes from dh-in-f138.1e100.net (209.85.203.138): icmp_seq=1 ttl=108 time=15.1 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 1 received, 50% packet loss, time 1020ms
rtt min/avg/max/mdev = 15.138/15.138/15.138/0.000 ms
vagrant@node01:/usr/local/hadoop/etc/hadoop$ ping node02
PING node02 (172.16.0.12) 56(84) bytes of data.
64 bytes from node02 (172.16.0.12): icmp_seq=1 ttl=64 time=1.42 ms
64 bytes from node02 (172.16.0.12): icmp_seq=2 ttl=64 time=0.472 ms
^C
--- node02 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 0.472/0.944/1.416/0.472 ms
vagrant@node01:/usr/local/hadoop/etc/hadoop$
vagrant@node01:/usr/local/hadoop/etc/hadoop$
vagrant@node01:/usr/local/hadoop/etc/hadoop$
vagrant@node01:/usr/local/hadoop/etc/hadoop$
vagrant@node01:/usr/local/hadoop/etc/hadoop$ sudo cp -f /app/code/workers /usr/local/hadoop/etc/hadoop/workers
vagrant@node01:/usr/local/hadoop/etc/hadoop$
vagrant@node01:/usr/local/hadoop/etc/hadoop$
vagrant@node01:/usr/local/hadoop/etc/hadoop$ cd ..
vagrant@node01:/usr/local/hadoop/etc$ cd ..
vagrant@node01:/usr/local/hadoop$ cd ..
vagrant@node01:/usr/local$ cd
vagrant@node01:~$ pwd
/home/vagrant
vagrant@node01:~$
vagrant@node01:~$
vagrant@node01:~$ sudo adduser hadoop
Adding user `hadoop' ...
Adding new group `hadoop' (1002) ...
Adding new user `hadoop' (1002) with group `hadoop' ...
Creating home directory `/home/hadoop' ...
Copying files from `/etc/skel' ...
New password:
Retype new password:
passwd: password updated successfully
Changing the user information for hadoop
Enter the new value, or press ENTER for the default
        Full Name []:
        Room Number []:
        Work Phone []:
        Home Phone []:
        Other []:
Is the information correct? [Y/n] y
vagrant@node01:~$
vagrant@node01:~$
vagrant@node01:~$ sudo usermod -aG hadoop hadoop
vagrant@node01:~$ sudo chown hadoop:root -R /usr/local/hadoop
vagrant@node01:~$ sudo chmod g+rwx -R /usr/local/hadoop
vagrant@node01:~$ sudo adduser hadoop sudo
Adding user `hadoop' to group `sudo' ...
Adding user hadoop to group sudo
Done.
vagrant@node01:~$ su - hadoop
Password:
To run a command as administrator (user "root"), use "sudo <command>".
See "man sudo_root" for details.

hadoop@node01:~$ ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/home/hadoop/.ssh/id_rsa):
Created directory '/home/hadoop/.ssh'.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /home/hadoop/.ssh/id_rsa
Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub
The key fingerprint is:
SHA256:HHk9+Ar7hIsg0ZiEqFqrnzIBZ9JZPiaqlJYLxvYU/dY hadoop@node01
The key's randomart image is:
+---[RSA 3072]----+
|                 |
|..  .    . o     |
|o..+.   o o o    |
|+.*=+. . o . .   |
|+=Boo.. S   .    |
|=X +   o E .     |
|O.* . . o o      |
|+o + . . +       |
|.+o   . . .      |
+----[SHA256]-----+
hadoop@node01:~$ ssh-copy-id hadoop@node01
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/hadoop/.ssh/id_rsa.pub"
The authenticity of host 'node01 (127.0.2.1)' can't be established.
ECDSA key fingerprint is SHA256:tVI8wDcrDP8uYs9lCi6jmg1gTXP+TpALlsu/+WG4lto.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
hadoop@node01: Permission denied (publickey).
hadoop@node01:~$
hadoop@node01:~$ sudo sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/g' /etc/ssh/sshd_config
[sudo] password for hadoop:
hadoop@node01:~$ sudo systemctl restart sshd
hadoop@node01:~$ ssh-copy-id hadoop@node01                                                /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/hadoop/.ssh/id_rsa.pub"
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
hadoop@node01's password:

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'hadoop@node01'"
and check to make sure that only the key(s) you wanted were added.

hadoop@node01:~$ ssh-copy-id hadoop@node02
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/hadoop/.ssh/id_rsa.pub"
The authenticity of host 'node02 (172.16.0.12)' can't be established.
ECDSA key fingerprint is SHA256:eA4aPR5xWP1TxKGRf9cX0oHQce3x3uQUpnaTJNepGz0.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
hadoop@node02's password:

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'hadoop@node02'"
and check to make sure that only the key(s) you wanted were added.

hadoop@node01:~$ ssh-copy-id hadoop@node03
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/hadoop/.ssh/id_rsa.pub"
The authenticity of host 'node03 (172.16.0.13)' can't be established.
ECDSA key fingerprint is SHA256:SQtPuT8a0/AJvgPAUTlv2UYh+0OY56aSa2s0k+yO9+8.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
hadoop@node03's password:

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'hadoop@node03'"
and check to make sure that only the key(s) you wanted were added.

hadoop@node01:~$ scp /usr/local/hadoop/etc/hadoop/* node02:/usr/local/hadoop/etc/hadoop/
capacity-scheduler.xml                                  100% 8260     1.2MB/s   00:00
configuration.xsl                                       100% 1335   599.7KB/s   00:00
container-executor.cfg                                  100% 1940     1.0MB/s   00:00
core-site.xml                                           100%  872   857.2KB/s   00:00
hadoop-env.cmd                                          100% 3999     2.0MB/s   00:00
hadoop-env.sh                                           100%   16KB   6.2MB/s   00:00
hadoop-metrics2.properties                              100% 3321     2.1MB/s   00:00
hadoop-policy.xml                                       100%   11KB   5.4MB/s   00:00
hadoop-user-functions.sh.example                        100% 3414     2.0MB/s   00:00
hdfs-site.xml                                           100% 1090   974.4KB/s   00:00
httpfs-env.sh                                           100% 1484   816.9KB/s   00:00
httpfs-log4j.properties                                 100% 1657   973.0KB/s   00:00
httpfs-signature.secret                                 100%   21    12.5KB/s   00:00
httpfs-site.xml                                         100%  620   379.5KB/s   00:00
kms-acls.xml                                            100% 3518     2.1MB/s   00:00
kms-env.sh                                              100% 1351   833.9KB/s   00:00
kms-log4j.properties                                    100% 1860   681.0KB/s   00:00
kms-site.xml                                            100%  682   389.6KB/s   00:00
log4j.properties                                        100%   13KB   6.2MB/s   00:00
mapred-env.cmd                                          100%  951   637.1KB/s   00:00
mapred-env.sh                                           100% 1764     1.0MB/s   00:00
mapred-queues.xml.template                              100% 4113     2.0MB/s   00:00
mapred-site.xml                                         100%  906     1.1MB/s   00:00
/usr/local/hadoop/etc/hadoop/shellprofile.d: not a regular file
ssl-client.xml.example                                  100% 2316     1.2MB/s   00:00
ssl-server.xml.example                                  100% 2697     1.6MB/s   00:00
user_ec_policies.xml.template                           100% 2642     1.4MB/s   00:00
workers                                                 100%   16    17.4KB/s   00:00
yarn-env.cmd                                            100% 2250     1.4MB/s   00:00
yarn-env.sh                                             100% 6056     3.1MB/s   00:00
yarn-site.xml                                           100%  735   688.0KB/s   00:00
yarnservice-log4j.properties                            100% 2591   936.5KB/s   00:00
hadoop@node01:~$
hadoop@node01:~$
hadoop@node01:~$ scp /usr/local/hadoop/etc/hadoop/* node03:/usr/local/hadoop/etc/hadoop/
capacity-scheduler.xml                                  100% 8260     1.8MB/s   00:00
configuration.xsl                                       100% 1335   687.2KB/s   00:00
container-executor.cfg                                  100% 1940     1.0MB/s   00:00
core-site.xml                                           100%  872   823.1KB/s   00:00
hadoop-env.cmd                                          100% 3999     2.2MB/s   00:00
hadoop-env.sh                                           100%   16KB   7.2MB/s   00:00
hadoop-metrics2.properties                              100% 3321     1.8MB/s   00:00
hadoop-policy.xml                                       100%   11KB   5.4MB/s   00:00
hadoop-user-functions.sh.example                        100% 3414     1.4MB/s   00:00
hdfs-site.xml                                           100% 1090   895.7KB/s   00:00
httpfs-env.sh                                           100% 1484   779.2KB/s   00:00
httpfs-log4j.properties                                 100% 1657   968.7KB/s   00:00
httpfs-signature.secret                                 100%   21    10.2KB/s   00:00
httpfs-site.xml                                         100%  620   357.0KB/s   00:00
kms-acls.xml                                            100% 3518     1.4MB/s   00:00
kms-env.sh                                              100% 1351   787.7KB/s   00:00
kms-log4j.properties                                    100% 1860     1.0MB/s   00:00
kms-site.xml                                            100%  682   413.0KB/s   00:00
log4j.properties                                        100%   13KB   4.9MB/s   00:00
mapred-env.cmd                                          100%  951   490.2KB/s   00:00
mapred-env.sh                                           100% 1764     1.0MB/s   00:00
mapred-queues.xml.template                              100% 4113     2.0MB/s   00:00
mapred-site.xml                                         100%  906   998.3KB/s   00:00
/usr/local/hadoop/etc/hadoop/shellprofile.d: not a regular file
ssl-client.xml.example                                  100% 2316     1.3MB/s   00:00
ssl-server.xml.example                                  100% 2697     1.7MB/s   00:00
user_ec_policies.xml.template                           100% 2642     1.6MB/s   00:00
workers                                                 100%   16    10.0KB/s   00:00
yarn-env.cmd                                            100% 2250     1.2MB/s   00:00
yarn-env.sh                                             100% 6056     2.7MB/s   00:00
yarn-site.xml                                           100%  735   553.6KB/s   00:00
yarnservice-log4j.properties                            100% 2591     1.4MB/s   00:00
hadoop@node01:~$



vagrant@node01:/usr/lib/jvm$ ls
vagrant@node01:/usr/lib/jvm$ sudo tar -xzf /app/code/jdk-8u91-linux-x64.tar.gz -C /usr/lib/jvm
vagrant@node01:/usr/lib/jvm$ ls
jdk1.8.0_91
vagrant@node01:/usr/lib/jvm$ cd jdk1.8.0_91/
vagrant@node01:/usr/lib/jvm/jdk1.8.0_91$ ls
COPYRIGHT    THIRDPARTYLICENSEREADME-JAVAFX.txt  db              jre  release
LICENSE      THIRDPARTYLICENSEREADME.txt         include         lib  src.zip
README.html  bin                                 javafx-src.zip  man
vagrant@node01:/usr/lib/jvm/jdk1.8.0_91$ cd bin
vagrant@node01:/usr/lib/jvm/jdk1.8.0_91/bin$ ls
ControlPanel  java-rmi.cgi    javaws    jinfo       jsadebugd     orbd         serialver
appletviewer  javac           jcmd      jjs         jstack        pack200      servertool
extcheck      javadoc         jconsole  jmap        jstat         policytool   tnameserv
idlj          javafxpackager  jcontrol  jmc         jstatd        rmic         unpack200
jar           javah           jdb       jmc.ini     jvisualvm     rmid         wsgen
jarsigner     javap           jdeps     jps         keytool       rmiregistry  wsimport
java          javapackager    jhat      jrunscript  native2ascii  schemagen    xjc
vagrant@node01:/usr/lib/jvm/jdk1.8.0_91/bin$ cd ..
vagrant@node01:/usr/lib/jvm/jdk1.8.0_91$ cd jre
vagrant@node01:/usr/lib/jvm/jdk1.8.0_91/jre$ ls
COPYRIGHT  README                              THIRDPARTYLICENSEREADME.txt  bin  plugin
LICENSE    THIRDPARTYLICENSEREADME-JAVAFX.txt  Welcome.html                 lib
vagrant@node01:/usr/lib/jvm/jdk1.8.0_91/jre$ cd bin
vagrant@node01:/usr/lib/jvm/jdk1.8.0_91/jre/bin$ ls
ControlPanel  javaws    jjs      orbd     policytool  rmiregistry  tnameserv
java          jcontrol  keytool  pack200  rmid        servertool   unpack200
vagrant@node01:/usr/lib/jvm/jdk1.8.0_91/jre/bin$ cd java
-bash: cd: java: Not a directory
vagrant@node01:/usr/lib/jvm/jdk1.8.0_91/jre/bin$ cd ..
vagrant@node01:/usr/lib/jvm/jdk1.8.0_91/jre$ cd ..
vagrant@node01:/usr/lib/jvm/jdk1.8.0_91$ ls
COPYRIGHT    THIRDPARTYLICENSEREADME-JAVAFX.txt  db              jre  release
LICENSE      THIRDPARTYLICENSEREADME.txt         include         lib  src.zip
README.html  bin                                 javafx-src.zip  man
vagrant@node01:/usr/lib/jvm/jdk1.8.0_91$ cd ..
vagrant@node01:/usr/lib/jvm$ ls
jdk1.8.0_91
vagrant@node01:/usr/lib/jvm$ mv jdk1.8.0_91 jdk
mv: cannot move 'jdk1.8.0_91' to 'jdk': Permission denied
vagrant@node01:/usr/lib/jvm$ sudo mv jdk1.8.0_91 jdk
vagrant@node01:/usr/lib/jvm$ ls
jdk
vagrant@node01:/usr/lib/jvm$ cd $JAVA_HOME
vagrant@node01:/usr/lib/jvm/jdk$ ls
COPYRIGHT    THIRDPARTYLICENSEREADME-JAVAFX.txt  db              jre  release
LICENSE      THIRDPARTYLICENSEREADME.txt         include         lib  src.zip
README.html  bin                                 javafx-src.zip  man
vagrant@node01:/usr/lib/jvm/jdk$
vagrant@node01:/usr/lib/jvm/jdk$
vagrant@node01:/usr/lib/jvm/jdk$ update-alternatives --display java
update-alternatives: warning: alternative /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java (part of link group java) doesn't exist; removing from list of alternatives
java - auto mode
  link best version not available
  link currently points to /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java
  link java is /usr/bin/java
  slave java.1.gz is /usr/share/man/man1/java.1.gz
vagrant@node01:/usr/lib/jvm/jdk$
vagrant@node01:/usr/lib/jvm/jdk$ java -version
java version "1.8.0_91"
Java(TM) SE Runtime Environment (build 1.8.0_91-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)
vagrant@node01:/usr/lib/jvm/jdk$
vagrant@node01:/usr/lib/jvm/jdk$ su - hadoop
Password:
hadoop@node01:~$ java -version
java version "1.8.0_91"
Java(TM) SE Runtime Environment (build 1.8.0_91-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)
hadoop@node01:~$ jps
18026 SecondaryNameNode
17805 NameNode
23535 Jps
hadoop@node01:~$

------ ERROR NEED CHECK 

hadoop@node01:~$ start-dfs.sh
Starting namenodes on [node01]
Starting datanodes
: Name or service not knownstname node02
: Name or service not knownstname node03
Starting secondary namenodes [node01]
hadoop@node01:~$ jps

Command 'jps' not found, but can be installed with:

sudo apt install openjdk-11-jdk-headless  # version 11.0.11+9-0ubuntu2~20.04, or
sudo apt install openjdk-16-jdk-headless  # version 16.0.1+9-1~20.04
sudo apt install openjdk-17-jdk-headless  # version 17.0.1+12-1~20.04
sudo apt install openjdk-8-jdk-headless   # version 8u292-b10-0ubuntu1~20.04
sudo apt install openjdk-13-jdk-headless  # version 13.0.7+5-0ubuntu1~20.04

hadoop@node01:~$ java -version

Command 'java' not found, but can be installed with:

sudo apt install default-jre              # version 2:1.11-72, or
sudo apt install openjdk-11-jre-headless  # version 11.0.11+9-0ubuntu2~20.04
sudo apt install openjdk-16-jre-headless  # version 16.0.1+9-1~20.04
sudo apt install openjdk-17-jre-headless  # version 17.0.1+12-1~20.04
sudo apt install openjdk-8-jre-headless   # version 8u292-b10-0ubuntu1~20.04
sudo apt install openjdk-13-jre-headless  # version 13.0.7+5-0ubuntu1~20.04

hadoop@node01:~$ exit
logout
vagrant@node01:~$ su - hadoop
Password:
To run a command as administrator (user "root"), use "sudo <command>".
See "man sudo_root" for details.

hadoop@node01:~$ java -version
java version "1.8.0_171"
Java(TM) SE Runtime Environment (build 1.8.0_171-b11)
Java HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode)
hadoop@node01:~$ jps
12080 Jps
11649 NameNode
11861 SecondaryNameNode
hadoop@node01:~$

----> THERE IS NO Datanodes started. ---> need to check hosts file and edit 

127.0.0.1       localhost

# The following lines are desirable for IPv6 capable hosts
::1     ip6-localhost   ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
ff02::3 ip6-allhosts
#127.0.1.1      ubuntu-focal    ubuntu-focal

172.16.0.11 node01
172.16.0.12 node02
172.16.0.13 node03


---> one more thing is that the file workers contain CRLF
hadoop@node01:~$stop-dfs.sh
hadoop@node01:~$hadoop namenode -fortmat
y
hadoop@node01:~$ start-dfs.sh
Starting namenodes on [node01]
Starting datanodes
node02: WARNING: /usr/local/hadoop/logs does not exist. Creating.
node03: WARNING: /usr/local/hadoop/logs does not exist. Creating.
Starting secondary namenodes [node01]
hadoop@node01:~$ jps
17232 NameNode
17614 Jps
17503 SecondaryNameNode
hadoop@node01:~$
hadoop@node01:~$
hadoop@node01:~$
hadoop@node01:~$
hadoop@node01:~$ export HADOOP_HOME="/usr/local/hadoop"
hadoop@node01:~$ export HADOOP_COMMON_HOME=$HADOOP_HOME
hadoop@node01:~$ export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
hadoop@node01:~$ export HADOOP_HDFS_HOME=$HADOOP_HOME
hadoop@node01:~$ export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOMEhadoop@node01:~$ export HADOOP_YARN_HOME=$HA
hadoop@node01:~$ echo $HADOOP_HOME
/usr/local/hadoop
hadoop@node01:~$ echo $HADOOP_COMMON_HOME
/usr/local/hadoop
hadoop@node01:~$ echo $HADOOP_CONF_DIR
/usr/local/hadoop/etc/hadoop
hadoop@node01:~$ echo $HADOOP_HDFS_HOME
/usr/local/hadoop
hadoop@node01:~$ echo $HADOOP_MAPRED_HOME
/usr/local/hadoop
hadoop@node01:~$ echo $HADOOP_YARN_HOME

hadoop@node01:~$ export HADOOP_YARN_HOME=$HADOOP_HOME
hadoop@node01:~$ echo $HADOOP_YARN_HOME
/usr/local/hadoop
hadoop@node01:~$
hadoop@node01:~$ start-yarn.sh
Starting resourcemanager
Starting nodemanagers
hadoop@node01:~$ yarn node -list
2021-12-15 23:21:10,587 INFO client.RMProxy: Connecting to ResourceManager at node01/172.16.0.11:8032
Total Nodes:0
         Node-Id             Node-State Node-Http-Address       Number-of-Running-Containers
hadoop@node01:~$


---- the last run 

hadoop@node01:~$ jps
11555 NameNode
11987 Jps
11817 SecondaryNameNode
hadoop@node01:~$
hadoop@node01:~$ export HADOOP_HOME="/usr/local/hadoop"
hadoop@node01:~$ export HADOOP_COMMON_HOME=$HADOOP_HOME
hadoop@node01:~$ export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
hadoop@node01:~$ export HADOOP_HDFS_HOME=$HADOOP_HOME
hadoop@node01:~$ export HADOOP_MAPRED_HOME=$HADOOP_HOME
hadoop@node01:~$ export HADOOP_YARN_HOME=$HADOOP_HOME
hadoop@node01:~$ start-yarn.sh
Starting resourcemanager
Starting nodemanagers
hadoop@node01:~$ yarn node -list
2021-12-16 00:16:25,774 INFO client.RMProxy: Connecting to ResourceManager at node01/172.16.0.11:8032
Total Nodes:0
         Node-Id             Node-State Node-Http-Address       Number-of-Running-Containers
hadoop@node01:~$ yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar pi 16 1000
Number of Maps  = 16
Samples per Map = 1000
2021-12-16 00:16:41,145 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #0
2021-12-16 00:16:43,079 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #1
2021-12-16 00:16:43,158 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #2
2021-12-16 00:16:43,230 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #3
2021-12-16 00:16:43,308 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #4
2021-12-16 00:16:43,569 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #5
2021-12-16 00:16:43,635 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #6
2021-12-16 00:16:43,714 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #7
2021-12-16 00:16:43,792 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #8
2021-12-16 00:16:43,854 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #9
2021-12-16 00:16:43,943 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #10
2021-12-16 00:16:44,023 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #11
2021-12-16 00:16:44,089 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #12
2021-12-16 00:16:44,163 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #13
2021-12-16 00:16:44,224 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #14
2021-12-16 00:16:44,321 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #15
Starting Job
2021-12-16 00:16:44,707 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2021-12-16 00:16:44,871 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2021-12-16 00:16:44,872 INFO impl.MetricsSystemImpl: JobTracker metrics system started
2021-12-16 00:16:45,809 INFO input.FileInputFormat: Total input files to process : 16
2021-12-16 00:16:45,830 INFO mapreduce.JobSubmitter: number of splits:16
2021-12-16 00:16:46,862 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1269224262_0001
2021-12-16 00:16:46,863 INFO mapreduce.JobSubmitter: Executing with tokens: []
2021-12-16 00:16:47,196 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
2021-12-16 00:16:47,198 INFO mapreduce.Job: Running job: job_local1269224262_0001
2021-12-16 00:16:47,209 INFO mapred.LocalJobRunner: OutputCommitter set in config null
2021-12-16 00:16:47,235 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-16 00:16:47,236 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-16 00:16:47,241 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2021-12-16 00:16:47,394 INFO mapred.LocalJobRunner: Waiting for map tasks
2021-12-16 00:16:47,397 INFO mapred.LocalJobRunner: Starting task: attempt_local1269224262_0001_m_000000_0
2021-12-16 00:16:47,474 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-16 00:16:47,475 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-16 00:16:47,639 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-16 00:16:47,647 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/hadoop/QuasiMonteCarlo_1639613797242_1267710974/in/part0:0+118
2021-12-16 00:16:47,804 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-16 00:16:47,804 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-16 00:16:47,804 INFO mapred.MapTask: soft limit at 83886080
2021-12-16 00:16:47,804 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-16 00:16:47,804 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-16 00:16:47,812 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-16 00:16:47,930 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2021-12-16 00:16:48,065 INFO mapred.LocalJobRunner:
2021-12-16 00:16:48,070 INFO mapred.MapTask: Starting flush of map output
2021-12-16 00:16:48,070 INFO mapred.MapTask: Spilling map output
2021-12-16 00:16:48,071 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-16 00:16:48,072 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-16 00:16:48,093 INFO mapred.MapTask: Finished spill 0
2021-12-16 00:16:48,119 INFO mapred.Task: Task:attempt_local1269224262_0001_m_000000_0 is done. And is in the process of committing
2021-12-16 00:16:48,132 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-16 00:16:48,134 INFO mapred.Task: Task 'attempt_local1269224262_0001_m_000000_0' done.
2021-12-16 00:16:48,156 INFO mapred.Task: Final Counters for attempt_local1269224262_0001_m_000000_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=319218
                FILE: Number of bytes written=845148
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=118
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=7
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=276824064
        File Input Format Counters
                Bytes Read=118
2021-12-16 00:16:48,167 INFO mapred.LocalJobRunner: Finishing task: attempt_local1269224262_0001_m_000000_0
2021-12-16 00:16:48,169 INFO mapred.LocalJobRunner: Starting task: attempt_local1269224262_0001_m_000001_0
2021-12-16 00:16:48,173 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-16 00:16:48,174 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-16 00:16:48,176 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-16 00:16:48,179 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/hadoop/QuasiMonteCarlo_1639613797242_1267710974/in/part1:0+118
2021-12-16 00:16:48,204 INFO mapreduce.Job: Job job_local1269224262_0001 running in uber mode : false
2021-12-16 00:16:48,322 INFO mapreduce.Job:  map 100% reduce 0%
2021-12-16 00:16:48,329 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-16 00:16:48,329 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-16 00:16:48,329 INFO mapred.MapTask: soft limit at 83886080
2021-12-16 00:16:48,329 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-16 00:16:48,329 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-16 00:16:48,337 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-16 00:16:48,355 INFO mapred.LocalJobRunner:
2021-12-16 00:16:48,363 INFO mapred.MapTask: Starting flush of map output
2021-12-16 00:16:48,366 INFO mapred.MapTask: Spilling map output
2021-12-16 00:16:48,366 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-16 00:16:48,366 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-16 00:16:48,372 INFO mapred.MapTask: Finished spill 0
2021-12-16 00:16:48,383 INFO mapred.Task: Task:attempt_local1269224262_0001_m_000001_0 is done. And is in the process of committing
2021-12-16 00:16:48,394 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-16 00:16:48,398 INFO mapred.Task: Task 'attempt_local1269224262_0001_m_000001_0' done.
2021-12-16 00:16:48,399 INFO mapred.Task: Final Counters for attempt_local1269224262_0001_m_000001_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=321579
                FILE: Number of bytes written=845208
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=236
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=10
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=382205952
        File Input Format Counters
                Bytes Read=118
2021-12-16 00:16:48,401 INFO mapred.LocalJobRunner: Finishing task: attempt_local1269224262_0001_m_000001_0
2021-12-16 00:16:48,402 INFO mapred.LocalJobRunner: Starting task: attempt_local1269224262_0001_m_000002_0
2021-12-16 00:16:48,408 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-16 00:16:48,408 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-16 00:16:48,410 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-16 00:16:48,421 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/hadoop/QuasiMonteCarlo_1639613797242_1267710974/in/part10:0+118
2021-12-16 00:16:48,561 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-16 00:16:48,561 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-16 00:16:48,561 INFO mapred.MapTask: soft limit at 83886080
2021-12-16 00:16:48,561 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-16 00:16:48,561 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-16 00:16:48,563 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-16 00:16:48,572 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2021-12-16 00:16:48,687 INFO mapred.LocalJobRunner:
2021-12-16 00:16:48,689 INFO mapred.MapTask: Starting flush of map output
2021-12-16 00:16:48,689 INFO mapred.MapTask: Spilling map output
2021-12-16 00:16:48,690 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-16 00:16:48,691 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-16 00:16:48,694 INFO mapred.MapTask: Finished spill 0
2021-12-16 00:16:48,706 INFO mapred.Task: Task:attempt_local1269224262_0001_m_000002_0 is done. And is in the process of committing
2021-12-16 00:16:48,716 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-16 00:16:48,716 INFO mapred.Task: Task 'attempt_local1269224262_0001_m_000002_0' done.
2021-12-16 00:16:48,718 INFO mapred.Task: Final Counters for attempt_local1269224262_0001_m_000002_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=323940
                FILE: Number of bytes written=845268
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=354
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=13
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=146
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=487587840
        File Input Format Counters
                Bytes Read=118
2021-12-16 00:16:48,718 INFO mapred.LocalJobRunner: Finishing task: attempt_local1269224262_0001_m_000002_0
2021-12-16 00:16:48,718 INFO mapred.LocalJobRunner: Starting task: attempt_local1269224262_0001_m_000003_0
2021-12-16 00:16:48,722 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-16 00:16:48,722 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-16 00:16:48,723 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-16 00:16:48,726 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/hadoop/QuasiMonteCarlo_1639613797242_1267710974/in/part11:0+118
2021-12-16 00:16:48,819 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-16 00:16:48,820 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-16 00:16:48,820 INFO mapred.MapTask: soft limit at 83886080
2021-12-16 00:16:48,820 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-16 00:16:48,820 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-16 00:16:48,836 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-16 00:16:48,858 INFO mapred.LocalJobRunner:
2021-12-16 00:16:48,859 INFO mapred.MapTask: Starting flush of map output
2021-12-16 00:16:48,859 INFO mapred.MapTask: Spilling map output
2021-12-16 00:16:48,859 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-16 00:16:48,859 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-16 00:16:48,863 INFO mapred.MapTask: Finished spill 0
2021-12-16 00:16:48,868 INFO mapred.Task: Task:attempt_local1269224262_0001_m_000003_0 is done. And is in the process of committing
2021-12-16 00:16:48,876 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-16 00:16:48,876 INFO mapred.Task: Task 'attempt_local1269224262_0001_m_000003_0' done.
2021-12-16 00:16:48,881 INFO mapred.Task: Final Counters for attempt_local1269224262_0001_m_000003_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=326301
                FILE: Number of bytes written=845328
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=472
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=16
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=146
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=44
                Total committed heap usage (bytes)=495976448
        File Input Format Counters
                Bytes Read=118
2021-12-16 00:16:48,881 INFO mapred.LocalJobRunner: Finishing task: attempt_local1269224262_0001_m_000003_0
2021-12-16 00:16:48,882 INFO mapred.LocalJobRunner: Starting task: attempt_local1269224262_0001_m_000004_0
2021-12-16 00:16:48,884 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-16 00:16:48,884 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-16 00:16:48,885 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-16 00:16:48,889 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/hadoop/QuasiMonteCarlo_1639613797242_1267710974/in/part12:0+118
2021-12-16 00:16:48,957 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-16 00:16:48,957 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-16 00:16:48,957 INFO mapred.MapTask: soft limit at 83886080
2021-12-16 00:16:48,957 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-16 00:16:48,957 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-16 00:16:48,959 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-16 00:16:48,982 INFO mapred.LocalJobRunner:
2021-12-16 00:16:48,982 INFO mapred.MapTask: Starting flush of map output
2021-12-16 00:16:48,983 INFO mapred.MapTask: Spilling map output
2021-12-16 00:16:48,983 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-16 00:16:48,983 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-16 00:16:48,992 INFO mapred.MapTask: Finished spill 0
2021-12-16 00:16:48,998 INFO mapred.Task: Task:attempt_local1269224262_0001_m_000004_0 is done. And is in the process of committing
2021-12-16 00:16:49,013 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-16 00:16:49,013 INFO mapred.Task: Task 'attempt_local1269224262_0001_m_000004_0' done.
2021-12-16 00:16:49,016 INFO mapred.Task: Final Counters for attempt_local1269224262_0001_m_000004_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=328150
                FILE: Number of bytes written=845388
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=590
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=19
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=146
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=32
                Total committed heap usage (bytes)=514326528
        File Input Format Counters
                Bytes Read=118
2021-12-16 00:16:49,025 INFO mapred.LocalJobRunner: Finishing task: attempt_local1269224262_0001_m_000004_0
2021-12-16 00:16:49,026 INFO mapred.LocalJobRunner: Starting task: attempt_local1269224262_0001_m_000005_0
2021-12-16 00:16:49,030 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-16 00:16:49,030 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-16 00:16:49,031 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-16 00:16:49,034 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/hadoop/QuasiMonteCarlo_1639613797242_1267710974/in/part13:0+118
2021-12-16 00:16:49,266 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-16 00:16:49,266 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-16 00:16:49,266 INFO mapred.MapTask: soft limit at 83886080
2021-12-16 00:16:49,267 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-16 00:16:49,267 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-16 00:16:49,268 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-16 00:16:49,284 INFO mapred.LocalJobRunner:
2021-12-16 00:16:49,287 INFO mapred.MapTask: Starting flush of map output
2021-12-16 00:16:49,287 INFO mapred.MapTask: Spilling map output
2021-12-16 00:16:49,287 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-16 00:16:49,288 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-16 00:16:49,292 INFO mapred.MapTask: Finished spill 0
2021-12-16 00:16:49,304 INFO mapred.Task: Task:attempt_local1269224262_0001_m_000005_0 is done. And is in the process of committing
2021-12-16 00:16:49,315 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-16 00:16:49,318 INFO mapred.Task: Task 'attempt_local1269224262_0001_m_000005_0' done.
2021-12-16 00:16:49,320 INFO mapred.Task: Final Counters for attempt_local1269224262_0001_m_000005_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=329999
                FILE: Number of bytes written=845448
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=708
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=22
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=146
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=193
                Total committed heap usage (bytes)=209715200
        File Input Format Counters
                Bytes Read=118
2021-12-16 00:16:49,322 INFO mapred.LocalJobRunner: Finishing task: attempt_local1269224262_0001_m_000005_0
2021-12-16 00:16:49,322 INFO mapred.LocalJobRunner: Starting task: attempt_local1269224262_0001_m_000006_0
2021-12-16 00:16:49,325 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-16 00:16:49,325 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-16 00:16:49,326 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-16 00:16:49,328 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/hadoop/QuasiMonteCarlo_1639613797242_1267710974/in/part14:0+118
2021-12-16 00:16:49,420 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-16 00:16:49,420 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-16 00:16:49,420 INFO mapred.MapTask: soft limit at 83886080
2021-12-16 00:16:49,420 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-16 00:16:49,420 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-16 00:16:49,422 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-16 00:16:49,446 INFO mapred.LocalJobRunner:
2021-12-16 00:16:49,446 INFO mapred.MapTask: Starting flush of map output
2021-12-16 00:16:49,446 INFO mapred.MapTask: Spilling map output
2021-12-16 00:16:49,446 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-16 00:16:49,447 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-16 00:16:49,454 INFO mapred.MapTask: Finished spill 0
2021-12-16 00:16:49,459 INFO mapred.Task: Task:attempt_local1269224262_0001_m_000006_0 is done. And is in the process of committing
2021-12-16 00:16:49,475 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-16 00:16:49,479 INFO mapred.Task: Task 'attempt_local1269224262_0001_m_000006_0' done.
2021-12-16 00:16:49,485 INFO mapred.Task: Final Counters for attempt_local1269224262_0001_m_000006_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=331848
                FILE: Number of bytes written=845508
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=826
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=25
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=146
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=315097088
        File Input Format Counters
                Bytes Read=118
2021-12-16 00:16:49,492 INFO mapred.LocalJobRunner: Finishing task: attempt_local1269224262_0001_m_000006_0
2021-12-16 00:16:49,493 INFO mapred.LocalJobRunner: Starting task: attempt_local1269224262_0001_m_000007_0
2021-12-16 00:16:49,510 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-16 00:16:49,510 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-16 00:16:49,511 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-16 00:16:49,514 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/hadoop/QuasiMonteCarlo_1639613797242_1267710974/in/part15:0+118
2021-12-16 00:16:49,635 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-16 00:16:49,636 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-16 00:16:49,637 INFO mapred.MapTask: soft limit at 83886080
2021-12-16 00:16:49,638 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-16 00:16:49,638 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-16 00:16:49,641 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-16 00:16:49,676 INFO mapred.LocalJobRunner:
2021-12-16 00:16:49,677 INFO mapred.MapTask: Starting flush of map output
2021-12-16 00:16:49,677 INFO mapred.MapTask: Spilling map output
2021-12-16 00:16:49,677 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-16 00:16:49,677 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-16 00:16:49,681 INFO mapred.MapTask: Finished spill 0
2021-12-16 00:16:49,690 INFO mapred.Task: Task:attempt_local1269224262_0001_m_000007_0 is done. And is in the process of committing
2021-12-16 00:16:49,704 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-16 00:16:49,704 INFO mapred.Task: Task 'attempt_local1269224262_0001_m_000007_0' done.
2021-12-16 00:16:49,706 INFO mapred.Task: Final Counters for attempt_local1269224262_0001_m_000007_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=333185
                FILE: Number of bytes written=845568
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=944
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=28
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=146
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=420478976
        File Input Format Counters
                Bytes Read=118
2021-12-16 00:16:49,708 INFO mapred.LocalJobRunner: Finishing task: attempt_local1269224262_0001_m_000007_0
2021-12-16 00:16:49,708 INFO mapred.LocalJobRunner: Starting task: attempt_local1269224262_0001_m_000008_0
2021-12-16 00:16:49,713 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-16 00:16:49,713 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-16 00:16:49,717 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-16 00:16:49,720 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/hadoop/QuasiMonteCarlo_1639613797242_1267710974/in/part2:0+118
2021-12-16 00:16:49,830 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-16 00:16:49,831 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-16 00:16:49,831 INFO mapred.MapTask: soft limit at 83886080
2021-12-16 00:16:49,831 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-16 00:16:49,831 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-16 00:16:49,835 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-16 00:16:49,848 INFO mapred.LocalJobRunner:
2021-12-16 00:16:49,849 INFO mapred.MapTask: Starting flush of map output
2021-12-16 00:16:49,849 INFO mapred.MapTask: Spilling map output
2021-12-16 00:16:49,849 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-16 00:16:49,850 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-16 00:16:49,853 INFO mapred.MapTask: Finished spill 0
2021-12-16 00:16:49,857 INFO mapred.Task: Task:attempt_local1269224262_0001_m_000008_0 is done. And is in the process of committing
2021-12-16 00:16:49,863 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-16 00:16:49,863 INFO mapred.Task: Task 'attempt_local1269224262_0001_m_000008_0' done.
2021-12-16 00:16:49,865 INFO mapred.Task: Final Counters for attempt_local1269224262_0001_m_000008_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=334522
                FILE: Number of bytes written=845628
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1062
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=31
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=514326528
        File Input Format Counters
                Bytes Read=118
2021-12-16 00:16:49,874 INFO mapred.LocalJobRunner: Finishing task: attempt_local1269224262_0001_m_000008_0
2021-12-16 00:16:49,874 INFO mapred.LocalJobRunner: Starting task: attempt_local1269224262_0001_m_000009_0
2021-12-16 00:16:49,878 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-16 00:16:49,878 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-16 00:16:49,881 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-16 00:16:49,890 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/hadoop/QuasiMonteCarlo_1639613797242_1267710974/in/part3:0+118
2021-12-16 00:16:50,167 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-16 00:16:50,168 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-16 00:16:50,168 INFO mapred.MapTask: soft limit at 83886080
2021-12-16 00:16:50,169 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-16 00:16:50,169 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-16 00:16:50,174 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-16 00:16:50,191 INFO mapred.LocalJobRunner:
2021-12-16 00:16:50,192 INFO mapred.MapTask: Starting flush of map output
2021-12-16 00:16:50,192 INFO mapred.MapTask: Spilling map output
2021-12-16 00:16:50,192 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-16 00:16:50,193 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-16 00:16:50,196 INFO mapred.MapTask: Finished spill 0
2021-12-16 00:16:50,204 INFO mapred.Task: Task:attempt_local1269224262_0001_m_000009_0 is done. And is in the process of committing
2021-12-16 00:16:50,215 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-16 00:16:50,216 INFO mapred.Task: Task 'attempt_local1269224262_0001_m_000009_0' done.
2021-12-16 00:16:50,218 INFO mapred.Task: Final Counters for attempt_local1269224262_0001_m_000009_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=335859
                FILE: Number of bytes written=845688
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1180
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=34
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=228
                Total committed heap usage (bytes)=209715200
        File Input Format Counters
                Bytes Read=118
2021-12-16 00:16:50,231 INFO mapred.LocalJobRunner: Finishing task: attempt_local1269224262_0001_m_000009_0
2021-12-16 00:16:50,231 INFO mapred.LocalJobRunner: Starting task: attempt_local1269224262_0001_m_000010_0
2021-12-16 00:16:50,234 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-16 00:16:50,234 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-16 00:16:50,235 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-16 00:16:50,239 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/hadoop/QuasiMonteCarlo_1639613797242_1267710974/in/part4:0+118
2021-12-16 00:16:50,340 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-16 00:16:50,340 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-16 00:16:50,340 INFO mapred.MapTask: soft limit at 83886080
2021-12-16 00:16:50,340 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-16 00:16:50,340 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-16 00:16:50,343 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-16 00:16:50,360 INFO mapred.LocalJobRunner:
2021-12-16 00:16:50,360 INFO mapred.MapTask: Starting flush of map output
2021-12-16 00:16:50,360 INFO mapred.MapTask: Spilling map output
2021-12-16 00:16:50,360 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-16 00:16:50,360 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-16 00:16:50,364 INFO mapred.MapTask: Finished spill 0
2021-12-16 00:16:50,376 INFO mapred.Task: Task:attempt_local1269224262_0001_m_000010_0 is done. And is in the process of committing
2021-12-16 00:16:50,381 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-16 00:16:50,382 INFO mapred.Task: Task 'attempt_local1269224262_0001_m_000010_0' done.
2021-12-16 00:16:50,383 INFO mapred.Task: Final Counters for attempt_local1269224262_0001_m_000010_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=337196
                FILE: Number of bytes written=845748
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1298
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=37
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=315097088
        File Input Format Counters
                Bytes Read=118
2021-12-16 00:16:50,383 INFO mapred.LocalJobRunner: Finishing task: attempt_local1269224262_0001_m_000010_0
2021-12-16 00:16:50,383 INFO mapred.LocalJobRunner: Starting task: attempt_local1269224262_0001_m_000011_0
2021-12-16 00:16:50,386 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-16 00:16:50,386 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-16 00:16:50,387 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-16 00:16:50,390 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/hadoop/QuasiMonteCarlo_1639613797242_1267710974/in/part5:0+118
2021-12-16 00:16:50,517 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-16 00:16:50,517 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-16 00:16:50,517 INFO mapred.MapTask: soft limit at 83886080
2021-12-16 00:16:50,517 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-16 00:16:50,517 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-16 00:16:50,522 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-16 00:16:50,538 INFO mapred.LocalJobRunner:
2021-12-16 00:16:50,539 INFO mapred.MapTask: Starting flush of map output
2021-12-16 00:16:50,539 INFO mapred.MapTask: Spilling map output
2021-12-16 00:16:50,541 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-16 00:16:50,542 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-16 00:16:50,546 INFO mapred.MapTask: Finished spill 0
2021-12-16 00:16:50,550 INFO mapred.Task: Task:attempt_local1269224262_0001_m_000011_0 is done. And is in the process of committing
2021-12-16 00:16:50,556 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-16 00:16:50,557 INFO mapred.Task: Task 'attempt_local1269224262_0001_m_000011_0' done.
2021-12-16 00:16:50,559 INFO mapred.Task: Final Counters for attempt_local1269224262_0001_m_000011_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=338021
                FILE: Number of bytes written=845808
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1416
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=40
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=420478976
        File Input Format Counters
                Bytes Read=118
2021-12-16 00:16:50,566 INFO mapred.LocalJobRunner: Finishing task: attempt_local1269224262_0001_m_000011_0
2021-12-16 00:16:50,566 INFO mapred.LocalJobRunner: Starting task: attempt_local1269224262_0001_m_000012_0
2021-12-16 00:16:50,573 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-16 00:16:50,573 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-16 00:16:50,575 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-16 00:16:50,581 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/hadoop/QuasiMonteCarlo_1639613797242_1267710974/in/part6:0+118
2021-12-16 00:16:50,688 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-16 00:16:50,688 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-16 00:16:50,688 INFO mapred.MapTask: soft limit at 83886080
2021-12-16 00:16:50,688 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-16 00:16:50,688 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-16 00:16:50,691 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-16 00:16:50,705 INFO mapred.LocalJobRunner:
2021-12-16 00:16:50,706 INFO mapred.MapTask: Starting flush of map output
2021-12-16 00:16:50,706 INFO mapred.MapTask: Spilling map output
2021-12-16 00:16:50,707 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-16 00:16:50,708 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-16 00:16:50,711 INFO mapred.MapTask: Finished spill 0
2021-12-16 00:16:50,720 INFO mapred.Task: Task:attempt_local1269224262_0001_m_000012_0 is done. And is in the process of committing
2021-12-16 00:16:50,728 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-16 00:16:50,729 INFO mapred.Task: Task 'attempt_local1269224262_0001_m_000012_0' done.
2021-12-16 00:16:50,731 INFO mapred.Task: Final Counters for attempt_local1269224262_0001_m_000012_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=338846
                FILE: Number of bytes written=845868
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1534
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=43
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=514326528
        File Input Format Counters
                Bytes Read=118
2021-12-16 00:16:50,746 INFO mapred.LocalJobRunner: Finishing task: attempt_local1269224262_0001_m_000012_0
2021-12-16 00:16:50,747 INFO mapred.LocalJobRunner: Starting task: attempt_local1269224262_0001_m_000013_0
2021-12-16 00:16:50,751 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-16 00:16:50,751 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-16 00:16:50,753 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-16 00:16:50,756 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/hadoop/QuasiMonteCarlo_1639613797242_1267710974/in/part7:0+118
2021-12-16 00:16:50,936 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-16 00:16:50,936 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-16 00:16:50,936 INFO mapred.MapTask: soft limit at 83886080
2021-12-16 00:16:50,936 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-16 00:16:50,936 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-16 00:16:50,938 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-16 00:16:50,955 INFO mapred.LocalJobRunner:
2021-12-16 00:16:50,955 INFO mapred.MapTask: Starting flush of map output
2021-12-16 00:16:50,955 INFO mapred.MapTask: Spilling map output
2021-12-16 00:16:50,956 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-16 00:16:50,956 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-16 00:16:50,959 INFO mapred.MapTask: Finished spill 0
2021-12-16 00:16:50,963 INFO mapred.Task: Task:attempt_local1269224262_0001_m_000013_0 is done. And is in the process of committing
2021-12-16 00:16:50,975 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-16 00:16:50,976 INFO mapred.Task: Task 'attempt_local1269224262_0001_m_000013_0' done.
2021-12-16 00:16:50,978 INFO mapred.Task: Final Counters for attempt_local1269224262_0001_m_000013_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=339671
                FILE: Number of bytes written=845928
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1652
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=46
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=141
                Total committed heap usage (bytes)=215482368
        File Input Format Counters
                Bytes Read=118
2021-12-16 00:16:50,981 INFO mapred.LocalJobRunner: Finishing task: attempt_local1269224262_0001_m_000013_0
2021-12-16 00:16:50,981 INFO mapred.LocalJobRunner: Starting task: attempt_local1269224262_0001_m_000014_0
2021-12-16 00:16:50,994 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-16 00:16:50,994 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-16 00:16:50,995 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-16 00:16:51,000 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/hadoop/QuasiMonteCarlo_1639613797242_1267710974/in/part8:0+118
2021-12-16 00:16:51,102 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-16 00:16:51,102 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-16 00:16:51,102 INFO mapred.MapTask: soft limit at 83886080
2021-12-16 00:16:51,102 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-16 00:16:51,102 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-16 00:16:51,117 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-16 00:16:51,135 INFO mapred.LocalJobRunner:
2021-12-16 00:16:51,135 INFO mapred.MapTask: Starting flush of map output
2021-12-16 00:16:51,135 INFO mapred.MapTask: Spilling map output
2021-12-16 00:16:51,135 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-16 00:16:51,135 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-16 00:16:51,139 INFO mapred.MapTask: Finished spill 0
2021-12-16 00:16:51,143 INFO mapred.Task: Task:attempt_local1269224262_0001_m_000014_0 is done. And is in the process of committing
2021-12-16 00:16:51,149 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-16 00:16:51,149 INFO mapred.Task: Task 'attempt_local1269224262_0001_m_000014_0' done.
2021-12-16 00:16:51,150 INFO mapred.Task: Final Counters for attempt_local1269224262_0001_m_000014_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=340496
                FILE: Number of bytes written=845988
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1770
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=49
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=320864256
        File Input Format Counters
                Bytes Read=118
2021-12-16 00:16:51,154 INFO mapred.LocalJobRunner: Finishing task: attempt_local1269224262_0001_m_000014_0
2021-12-16 00:16:51,154 INFO mapred.LocalJobRunner: Starting task: attempt_local1269224262_0001_m_000015_0
2021-12-16 00:16:51,162 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-16 00:16:51,163 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-16 00:16:51,164 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-16 00:16:51,170 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/hadoop/QuasiMonteCarlo_1639613797242_1267710974/in/part9:0+118
2021-12-16 00:16:51,300 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-16 00:16:51,300 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-16 00:16:51,300 INFO mapred.MapTask: soft limit at 83886080
2021-12-16 00:16:51,300 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-16 00:16:51,300 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-16 00:16:51,303 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-16 00:16:51,329 INFO mapred.LocalJobRunner:
2021-12-16 00:16:51,329 INFO mapred.MapTask: Starting flush of map output
2021-12-16 00:16:51,330 INFO mapred.MapTask: Spilling map output
2021-12-16 00:16:51,331 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-16 00:16:51,331 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-16 00:16:51,334 INFO mapred.MapTask: Finished spill 0
2021-12-16 00:16:51,339 INFO mapreduce.Job:  map 94% reduce 0%
2021-12-16 00:16:51,344 INFO mapred.Task: Task:attempt_local1269224262_0001_m_000015_0 is done. And is in the process of committing
2021-12-16 00:16:51,351 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-16 00:16:51,351 INFO mapred.Task: Task 'attempt_local1269224262_0001_m_000015_0' done.
2021-12-16 00:16:51,352 INFO mapred.Task: Final Counters for attempt_local1269224262_0001_m_000015_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=340809
                FILE: Number of bytes written=846048
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1888
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=52
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=426246144
        File Input Format Counters
                Bytes Read=118
2021-12-16 00:16:51,352 INFO mapred.LocalJobRunner: Finishing task: attempt_local1269224262_0001_m_000015_0
2021-12-16 00:16:51,358 INFO mapred.LocalJobRunner: map task executor complete.
2021-12-16 00:16:51,365 INFO mapred.LocalJobRunner: Waiting for reduce tasks
2021-12-16 00:16:51,366 INFO mapred.LocalJobRunner: Starting task: attempt_local1269224262_0001_r_000000_0
2021-12-16 00:16:51,389 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-16 00:16:51,389 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-16 00:16:51,391 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-16 00:16:51,400 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@201c7296
2021-12-16 00:16:51,404 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2021-12-16 00:16:51,551 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=325163424, maxSingleShuffleLimit=81290856, mergeThreshold=214607872, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2021-12-16 00:16:51,566 INFO reduce.EventFetcher: attempt_local1269224262_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2021-12-16 00:16:51,665 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1269224262_0001_m_000000_0 decomp: 24 len: 28 to MEMORY
2021-12-16 00:16:51,693 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1269224262_0001_m_000000_0
2021-12-16 00:16:51,694 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->24
2021-12-16 00:16:51,700 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1269224262_0001_m_000006_0 decomp: 24 len: 28 to MEMORY
2021-12-16 00:16:51,713 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1269224262_0001_m_000006_0
2021-12-16 00:16:51,713 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 2, commitMemory -> 24, usedMemory ->48
2021-12-16 00:16:51,716 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1269224262_0001_m_000007_0 decomp: 24 len: 28 to MEMORY
2021-12-16 00:16:51,720 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1269224262_0001_m_000007_0
2021-12-16 00:16:51,720 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 3, commitMemory -> 48, usedMemory ->72
2021-12-16 00:16:51,723 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1269224262_0001_m_000013_0 decomp: 24 len: 28 to MEMORY
2021-12-16 00:16:51,726 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1269224262_0001_m_000013_0
2021-12-16 00:16:51,726 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 4, commitMemory -> 72, usedMemory ->96
2021-12-16 00:16:51,734 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1269224262_0001_m_000008_0 decomp: 24 len: 28 to MEMORY
2021-12-16 00:16:51,735 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1269224262_0001_m_000008_0
2021-12-16 00:16:51,735 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 5, commitMemory -> 96, usedMemory ->120
2021-12-16 00:16:51,738 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1269224262_0001_m_000001_0 decomp: 24 len: 28 to MEMORY
2021-12-16 00:16:51,745 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1269224262_0001_m_000001_0
2021-12-16 00:16:51,745 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 6, commitMemory -> 120, usedMemory ->144
2021-12-16 00:16:51,748 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1269224262_0001_m_000014_0 decomp: 24 len: 28 to MEMORY
2021-12-16 00:16:51,749 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1269224262_0001_m_000014_0
2021-12-16 00:16:51,749 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 7, commitMemory -> 144, usedMemory ->168
2021-12-16 00:16:51,754 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1269224262_0001_m_000015_0 decomp: 24 len: 28 to MEMORY
2021-12-16 00:16:51,756 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1269224262_0001_m_000015_0
2021-12-16 00:16:51,757 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 8, commitMemory -> 168, usedMemory ->192
2021-12-16 00:16:51,760 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1269224262_0001_m_000002_0 decomp: 24 len: 28 to MEMORY
2021-12-16 00:16:51,765 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1269224262_0001_m_000002_0
2021-12-16 00:16:51,765 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 9, commitMemory -> 192, usedMemory ->216
2021-12-16 00:16:51,769 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1269224262_0001_m_000003_0 decomp: 24 len: 28 to MEMORY
2021-12-16 00:16:51,771 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1269224262_0001_m_000003_0
2021-12-16 00:16:51,771 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 10, commitMemory -> 216, usedMemory ->240
2021-12-16 00:16:51,775 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1269224262_0001_m_000009_0 decomp: 24 len: 28 to MEMORY
2021-12-16 00:16:51,776 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1269224262_0001_m_000009_0
2021-12-16 00:16:51,777 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 11, commitMemory -> 240, usedMemory ->264
2021-12-16 00:16:51,780 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1269224262_0001_m_000004_0 decomp: 24 len: 28 to MEMORY
2021-12-16 00:16:51,781 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1269224262_0001_m_000004_0
2021-12-16 00:16:51,782 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 12, commitMemory -> 264, usedMemory ->288
2021-12-16 00:16:51,786 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1269224262_0001_m_000010_0 decomp: 24 len: 28 to MEMORY
2021-12-16 00:16:51,788 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1269224262_0001_m_000010_0
2021-12-16 00:16:51,788 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 13, commitMemory -> 288, usedMemory ->312
2021-12-16 00:16:51,792 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1269224262_0001_m_000011_0 decomp: 24 len: 28 to MEMORY
2021-12-16 00:16:51,793 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1269224262_0001_m_000011_0
2021-12-16 00:16:51,793 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 14, commitMemory -> 312, usedMemory ->336
2021-12-16 00:16:51,797 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1269224262_0001_m_000012_0 decomp: 24 len: 28 to MEMORY
2021-12-16 00:16:51,799 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1269224262_0001_m_000012_0
2021-12-16 00:16:51,799 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 15, commitMemory -> 336, usedMemory ->360
2021-12-16 00:16:51,802 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1269224262_0001_m_000005_0 decomp: 24 len: 28 to MEMORY
2021-12-16 00:16:51,805 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1269224262_0001_m_000005_0
2021-12-16 00:16:51,805 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 16, commitMemory -> 360, usedMemory ->384
2021-12-16 00:16:51,806 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning
2021-12-16 00:16:51,808 INFO mapred.LocalJobRunner: 16 / 16 copied.
2021-12-16 00:16:51,810 INFO reduce.MergeManagerImpl: finalMerge called with 16 in-memory map-outputs and 0 on-disk map-outputs
2021-12-16 00:16:51,831 INFO mapred.Merger: Merging 16 sorted segments
2021-12-16 00:16:51,833 INFO mapred.Merger: Down to the last merge-pass, with 16 segments left of total size: 336 bytes
2021-12-16 00:16:51,839 INFO reduce.MergeManagerImpl: Merged 16 segments, 384 bytes to disk to satisfy reduce memory limit
2021-12-16 00:16:51,842 INFO reduce.MergeManagerImpl: Merging 1 files, 358 bytes from disk
2021-12-16 00:16:51,845 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
2021-12-16 00:16:51,846 INFO mapred.Merger: Merging 1 sorted segments
2021-12-16 00:16:51,848 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 351 bytes
2021-12-16 00:16:51,850 INFO mapred.LocalJobRunner: 16 / 16 copied.
2021-12-16 00:16:51,881 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2021-12-16 00:16:51,924 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2021-12-16 00:16:52,003 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2021-12-16 00:16:52,061 INFO mapred.Task: Task:attempt_local1269224262_0001_r_000000_0 is done. And is in the process of committing
2021-12-16 00:16:52,070 INFO mapred.LocalJobRunner: 16 / 16 copied.
2021-12-16 00:16:52,071 INFO mapred.Task: Task attempt_local1269224262_0001_r_000000_0 is allowed to commit now
2021-12-16 00:16:52,269 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1269224262_0001_r_000000_0' to hdfs://node01:9000/user/hadoop/QuasiMonteCarlo_1639613797242_1267710974/out
2021-12-16 00:16:52,272 INFO mapred.LocalJobRunner: reduce > reduce
2021-12-16 00:16:52,273 INFO mapred.Task: Task 'attempt_local1269224262_0001_r_000000_0' done.
2021-12-16 00:16:52,275 INFO mapred.Task: Final Counters for attempt_local1269224262_0001_r_000000_0: Counters: 30
        File System Counters
                FILE: Number of bytes read=342127
                FILE: Number of bytes written=846406
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1888
                HDFS: Number of bytes written=2103
                HDFS: Number of read operations=57
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=21
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Combine input records=0
                Combine output records=0
                Reduce input groups=2
                Reduce shuffle bytes=448
                Reduce input records=32
                Reduce output records=0
                Spilled Records=32
                Shuffled Maps =16
                Failed Shuffles=0
                Merged Map outputs=16
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=426246144
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Output Format Counters
                Bytes Written=97
2021-12-16 00:16:52,299 INFO mapred.LocalJobRunner: Finishing task: attempt_local1269224262_0001_r_000000_0
2021-12-16 00:16:52,302 INFO mapred.LocalJobRunner: reduce task executor complete.
2021-12-16 00:16:52,340 INFO mapreduce.Job:  map 100% reduce 100%
2021-12-16 00:16:53,342 INFO mapreduce.Job: Job job_local1269224262_0001 completed successfully
2021-12-16 00:16:53,408 INFO mapreduce.Job: Counters: 36
        File System Counters
                FILE: Number of bytes read=5661767
                FILE: Number of bytes written=14375974
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=17936
                HDFS: Number of bytes written=32311
                HDFS: Number of read operations=529
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=309
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=16
                Map output records=32
                Map output bytes=288
                Map output materialized bytes=448
                Input split bytes=2326
                Combine input records=0
                Combine output records=0
                Reduce input groups=2
                Reduce shuffle bytes=448
                Reduce input records=32
                Reduce output records=0
                Spilled Records=64
                Shuffled Maps =16
                Failed Shuffles=0
                Merged Map outputs=16
                GC time elapsed (ms)=638
                Total committed heap usage (bytes)=6464995328
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=1888
        File Output Format Counters
                Bytes Written=97
Job Finished in 9.047 seconds
Estimated value of Pi is 3.14250000000000000000



thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ vagrant suspend
==> node01: Saving VM state and suspending execution...
==> node02: Saving VM state and suspending execution...
==> node03: Saving VM state and suspending execution...

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$
g
thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        modified:   GitBashLog
        modified:   README.md
        deleted:    existingArchitecture.PNG
        deleted:    proposalArchitecture.PNG

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        IssueLogSummary.txt
        Vagrantfile
        Vagrantfile_1
        code/
        configs/
        packages/
        pictures/
        scripts/

no changes added to commit (use "git add" and/or "git commit -a")

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add GitBashLog

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add README.md
warning: LF will be replaced by CRLF in README.md.
The file will have its original line endings in your working directory

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        modified:   GitBashLog
        modified:   README.md

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        deleted:    existingArchitecture.PNG
        deleted:    proposalArchitecture.PNG

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        IssueLogSummary.txt
        Vagrantfile
        Vagrantfile_1
        code/
        configs/
        packages/
        pictures/
        scripts/


thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add IssueLogSummary.txt

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add Vagrantfile

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add code/
warning: LF will be replaced by CRLF in code/core-site.xml.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in code/environment.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in code/hdfs-site.xml.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in code/workers.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in code/yarn-site.xml.
The file will have its original line endings in your working directory

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add configs/
warning: LF will be replaced by CRLF in configs/environment.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in configs/hadoop/core-site.xml.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in configs/hadoop/hdfs-site.xml.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in configs/hadoop/workers.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in configs/hadoop/yarn-site.xml.
The file will have its original line endings in your working directory

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        modified:   GitBashLog
        new file:   IssueLogSummary.txt
        modified:   README.md
        new file:   Vagrantfile
        new file:   code/core-site.xml
        new file:   code/environment
        new file:   code/hdfs-site.xml
        new file:   code/workers
        new file:   code/yarn-site.xml
        new file:   configs/environment
        new file:   configs/hadoop/core-site.xml
        new file:   configs/hadoop/hdfs-site.xml
        new file:   configs/hadoop/workers
        new file:   configs/hadoop/yarn-site.xml

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        deleted:    existingArchitecture.PNG
        deleted:    proposalArchitecture.PNG

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        Vagrantfile_1
        packages/
        pictures/
        scripts/


thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add packages/

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        modified:   GitBashLog
        new file:   IssueLogSummary.txt
        modified:   README.md
        new file:   Vagrantfile
        new file:   code/core-site.xml
        new file:   code/environment
        new file:   code/hdfs-site.xml
        new file:   code/workers
        new file:   code/yarn-site.xml
        new file:   configs/environment
        new file:   configs/hadoop/core-site.xml
        new file:   configs/hadoop/hdfs-site.xml
        new file:   configs/hadoop/workers
        new file:   configs/hadoop/yarn-site.xml
        new file:   packages/hadoop-3.2.1.tar.gz
        new file:   packages/jdk-8u171-linux-x64.tar.gz
        new file:   packages/jdk-8u91-linux-x64.tar.gz
        new file:   packages/spark-3.2.0-bin-hadoop3.2.tgz

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        deleted:    existingArchitecture.PNG
        deleted:    proposalArchitecture.PNG

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        pictures/
        scripts/


thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add pictures/

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add scripts/
warning: LF will be replaced by CRLF in scripts/setup-hadoop-package.sh.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in scripts/setup-hadoop-user-and-keygen.sh.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in scripts/setup-hosts.sh.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in scripts/setup-java.sh.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in scripts/variable.sh.
The file will have its original line endings in your working directory

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        modified:   GitBashLog
        new file:   IssueLogSummary.txt
        modified:   README.md
        new file:   Vagrantfile
        new file:   code/core-site.xml
        new file:   code/environment
        new file:   code/hdfs-site.xml
        new file:   code/workers
        new file:   code/yarn-site.xml
        new file:   configs/environment
        new file:   configs/hadoop/core-site.xml
        new file:   configs/hadoop/hdfs-site.xml
        new file:   configs/hadoop/workers
        new file:   configs/hadoop/yarn-site.xml
        new file:   packages/hadoop-3.2.1.tar.gz
        new file:   packages/jdk-8u171-linux-x64.tar.gz
        new file:   packages/jdk-8u91-linux-x64.tar.gz
        new file:   packages/spark-3.2.0-bin-hadoop3.2.tgz
        new file:   pictures/existingArchitecture.PNG
        new file:   pictures/proposalArchitecture.PNG
        new file:   scripts/setup-hadoop-package.sh
        new file:   scripts/setup-hadoop-user-and-keygen.sh
        new file:   scripts/setup-hosts.sh
        new file:   scripts/setup-java.sh
        new file:   scripts/setup-ssh.sh
        new file:   scripts/variable.sh

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        deleted:    existingArchitecture.PNG
        deleted:    proposalArchitecture.PNG

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/


thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git rm existingArchitecture.PNG
rm 'existingArchitecture.PNG'

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git rm proposalArchitecture.PNG
rm 'proposalArchitecture.PNG'

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        modified:   GitBashLog
        new file:   IssueLogSummary.txt
        modified:   README.md
        new file:   Vagrantfile
        new file:   code/core-site.xml
        new file:   code/environment
        new file:   code/hdfs-site.xml
        new file:   code/workers
        new file:   code/yarn-site.xml
        new file:   configs/environment
        new file:   configs/hadoop/core-site.xml
        new file:   configs/hadoop/hdfs-site.xml
        new file:   configs/hadoop/workers
        new file:   configs/hadoop/yarn-site.xml
        new file:   packages/hadoop-3.2.1.tar.gz
        new file:   packages/jdk-8u171-linux-x64.tar.gz
        new file:   packages/jdk-8u91-linux-x64.tar.gz
        new file:   packages/spark-3.2.0-bin-hadoop3.2.tgz
        renamed:    existingArchitecture.PNG -> pictures/existingArchitecture.PNG
        renamed:    proposalArchitecture.PNG -> pictures/proposalArchitecture.PNG
        new file:   scripts/setup-hadoop-package.sh
        new file:   scripts/setup-hadoop-user-and-keygen.sh
        new file:   scripts/setup-hosts.sh
        new file:   scripts/setup-java.sh
        new file:   scripts/setup-ssh.sh
        new file:   scripts/variable.sh

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/


thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git commit -m "Add Vagrant File and code, configs, packages, pictures, scripts folders to git"
[main da8442c] Add Vagrant File and code, configs, packages, pictures, scripts folders to git
 26 files changed, 2010 insertions(+), 28 deletions(-)
 create mode 100644 IssueLogSummary.txt
 create mode 100644 Vagrantfile
 create mode 100644 code/core-site.xml
 create mode 100644 code/environment
 create mode 100644 code/hdfs-site.xml
 create mode 100644 code/workers
 create mode 100644 code/yarn-site.xml
 create mode 100644 configs/environment
 create mode 100644 configs/hadoop/core-site.xml
 create mode 100644 configs/hadoop/hdfs-site.xml
 create mode 100644 configs/hadoop/workers
 create mode 100644 configs/hadoop/yarn-site.xml
 create mode 100644 packages/hadoop-3.2.1.tar.gz
 create mode 100644 packages/jdk-8u171-linux-x64.tar.gz
 create mode 100644 packages/jdk-8u91-linux-x64.tar.gz
 create mode 100644 packages/spark-3.2.0-bin-hadoop3.2.tgz
 rename existingArchitecture.PNG => pictures/existingArchitecture.PNG (100%)
 rename proposalArchitecture.PNG => pictures/proposalArchitecture.PNG (100%)
 create mode 100644 scripts/setup-hadoop-package.sh
 create mode 100644 scripts/setup-hadoop-user-and-keygen.sh
 create mode 100644 scripts/setup-hosts.sh
 create mode 100644 scripts/setup-java.sh
 create mode 100644 scripts/setup-ssh.sh
 create mode 100644 scripts/variable.sh

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is ahead of 'origin/main' by 1 commit.
  (use "git push" to publish your local commits)

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/

nothing added to commit but untracked files present (use "git add" to track)

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git push
Enumerating objects: 30, done.
Counting objects: 100% (30/30), done.
Delta compression using up to 4 threads
Compressing objects: 100% (26/26), done.
Writing objects: 100% (27/27), 982.87 MiB | 2.10 MiB/s, done.
Total 27 (delta 4), reused 0 (delta 0), pack-reused 0
remote: Resolving deltas: 100% (4/4), completed with 1 local object.
remote: error: Trace: e84a86a044eebeac326fb2dea8183430ff3d6f977eec1698989cb692e170d010
remote: error: See http://git.io/iEPt8g for more information.
remote: error: File packages/hadoop-3.2.1.tar.gz is 342.56 MB; this exceeds GitHub's file size limit of 100.00 MB
remote: error: File packages/jdk-8u171-linux-x64.tar.gz is 182.05 MB; this exceeds GitHub's file size limit of 100.00 MB
remote: error: File packages/jdk-8u91-linux-x64.tar.gz is 172.97 MB; this exceeds GitHub's file size limit of 100.00 MB
remote: error: File packages/spark-3.2.0-bin-hadoop3.2.tgz is 287.02 MB; this exceeds GitHub's file size limit of 100.00 MB
remote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.
To https://github.com/ITC-IFBD/ca3-TanThienNguyenVN.git
 ! [remote rejected] main -> main (pre-receive hook declined)
error: failed to push some refs to 'https://github.com/ITC-IFBD/ca3-TanThienNguyenVN.git'

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is ahead of 'origin/main' by 1 commit.
  (use "git push" to publish your local commits)

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/

nothing added to commit but untracked files present (use "git add" to track)

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$


thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$


thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$
g
thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is ahead of 'origin/main' by 1 commit.
  (use "git push" to publish your local commits)

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        deleted:    packages/hadoop-3.2.1.tar.gz
        deleted:    packages/jdk-8u171-linux-x64.tar.gz
        deleted:    packages/jdk-8u91-linux-x64.tar.gz
        deleted:    packages/spark-3.2.0-bin-hadoop3.2.tgz

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        package_big/

no changes added to commit (use "git add" and/or "git commit -a")

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git rm packages/hadoop-3.2.1.tar.gz
rm 'packages/hadoop-3.2.1.tar.gz'

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git rm packages/jdk-8u171-linux-x64.tar.gz
rm 'packages/jdk-8u171-linux-x64.tar.gz'

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is ahead of 'origin/main' by 1 commit.
  (use "git push" to publish your local commits)

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        deleted:    packages/hadoop-3.2.1.tar.gz
        deleted:    packages/jdk-8u171-linux-x64.tar.gz

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        deleted:    packages/jdk-8u91-linux-x64.tar.gz
        deleted:    packages/spark-3.2.0-bin-hadoop3.2.tgz

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        package_big/


thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git rm packages/jdk-8u91-linux-x64.tar.gz
rm 'packages/jdk-8u91-linux-x64.tar.gz'

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git rm packages/spark-3.2.0-bin-hadoop3.2.tgz
rm 'packages/spark-3.2.0-bin-hadoop3.2.tgz'

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is ahead of 'origin/main' by 1 commit.
  (use "git push" to publish your local commits)

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        deleted:    packages/hadoop-3.2.1.tar.gz
        deleted:    packages/jdk-8u171-linux-x64.tar.gz
        deleted:    packages/jdk-8u91-linux-x64.tar.gz
        deleted:    packages/spark-3.2.0-bin-hadoop3.2.tgz

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        package_big/


thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git commit -m "remove big files"
[main a7ee077] remove big files
 4 files changed, 0 insertions(+), 0 deletions(-)
 delete mode 100644 packages/hadoop-3.2.1.tar.gz
 delete mode 100644 packages/jdk-8u171-linux-x64.tar.gz
 delete mode 100644 packages/jdk-8u91-linux-x64.tar.gz
 delete mode 100644 packages/spark-3.2.0-bin-hadoop3.2.tgz

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is ahead of 'origin/main' by 2 commits.
  (use "git push" to publish your local commits)

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        package_big/

nothing added to commit but untracked files present (use "git add" to track)

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git push
Enumerating objects: 32, done.
Counting objects: 100% (32/32), done.
Delta compression using up to 4 threads
Compressing objects: 100% (28/28), done.
Writing objects: 100% (29/29), 982.87 MiB | 2.05 MiB/s, done.
Total 29 (delta 5), reused 0 (delta 0), pack-reused 0
remote: Resolving deltas: 100% (5/5), completed with 1 local object.
remote: error: Trace: 139f63a2a6632070b6c9b65348e796d5f73b4530469563302832b6913078d274
remote: error: See http://git.io/iEPt8g for more information.
remote: error: File packages/hadoop-3.2.1.tar.gz is 342.56 MB; this exceeds GitHub's file size limit of 100.00 MB
remote: error: File packages/jdk-8u171-linux-x64.tar.gz is 182.05 MB; this exceeds GitHub's file size limit of 100.00 MB
remote: error: File packages/jdk-8u91-linux-x64.tar.gz is 172.97 MB; this exceeds GitHub's file size limit of 100.00 MB
remote: error: File packages/spark-3.2.0-bin-hadoop3.2.tgz is 287.02 MB; this exceeds GitHub's file size limit of 100.00 MB
remote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.
To https://github.com/ITC-IFBD/ca3-TanThienNguyenVN.git
 ! [remote rejected] main -> main (pre-receive hook declined)
error: failed to push some refs to 'https://github.com/ITC-IFBD/ca3-TanThienNguyenVN.git'

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is ahead of 'origin/main' by 2 commits.
  (use "git push" to publish your local commits)

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        package_big/

nothing added to commit but untracked files present (use "git add" to track)

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git rm --cached packages/jdk-8u171-linux-x64.tar.gz
fatal: pathspec 'packages/jdk-8u171-linux-x64.tar.gz' did not match any files

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is ahead of 'origin/main' by 2 commits.
  (use "git push" to publish your local commits)

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        package_big/

nothing added to commit but untracked files present (use "git add" to track)

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git reset --hard HEAD
HEAD is now at a7ee077 remove big files

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is ahead of 'origin/main' by 2 commits.
  (use "git push" to publish your local commits)

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        package_big/

nothing added to commit but untracked files present (use "git add" to track)

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git reset --hard HEAD
HEAD is now at a7ee077 remove big files

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$


thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is ahead of 'origin/main' by 2 commits.
  (use "git push" to publish your local commits)

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        package_big/

nothing added to commit but untracked files present (use "git add" to track)

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ ^C


thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git reset da8442c
Unstaged changes after reset:
D       packages/hadoop-3.2.1.tar.gz
D       packages/jdk-8u171-linux-x64.tar.gz
D       packages/jdk-8u91-linux-x64.tar.gz
D       packages/spark-3.2.0-bin-hadoop3.2.tgz

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is ahead of 'origin/main' by 1 commit.
  (use "git push" to publish your local commits)

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        deleted:    packages/hadoop-3.2.1.tar.gz
        deleted:    packages/jdk-8u171-linux-x64.tar.gz
        deleted:    packages/jdk-8u91-linux-x64.tar.gz
        deleted:    packages/spark-3.2.0-bin-hadoop3.2.tgz

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        package_big/

no changes added to commit (use "git add" and/or "git commit -a")

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git reset --hard
Updating files: 100% (4/4), done.
HEAD is now at da8442c Add Vagrant File and code, configs, packages, pictures, scripts folders to git

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is ahead of 'origin/main' by 1 commit.
  (use "git push" to publish your local commits)

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        package_big/

nothing added to commit but untracked files present (use "git add" to track)

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git reset --soft HEAD~1

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        modified:   GitBashLog
        new file:   IssueLogSummary.txt
        modified:   README.md
        new file:   Vagrantfile
        new file:   code/core-site.xml
        new file:   code/environment
        new file:   code/hdfs-site.xml
        new file:   code/workers
        new file:   code/yarn-site.xml
        new file:   configs/environment
        new file:   configs/hadoop/core-site.xml
        new file:   configs/hadoop/hdfs-site.xml
        new file:   configs/hadoop/workers
        new file:   configs/hadoop/yarn-site.xml
        new file:   packages/hadoop-3.2.1.tar.gz
        new file:   packages/jdk-8u171-linux-x64.tar.gz
        new file:   packages/jdk-8u91-linux-x64.tar.gz
        new file:   packages/spark-3.2.0-bin-hadoop3.2.tgz
        renamed:    existingArchitecture.PNG -> pictures/existingArchitecture.PNG
        renamed:    proposalArchitecture.PNG -> pictures/proposalArchitecture.PNG
        new file:   scripts/setup-hadoop-package.sh
        new file:   scripts/setup-hadoop-user-and-keygen.sh
        new file:   scripts/setup-hosts.sh
        new file:   scripts/setup-java.sh
        new file:   scripts/setup-ssh.sh
        new file:   scripts/variable.sh

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        package_big/


thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        modified:   GitBashLog
        new file:   IssueLogSummary.txt
        modified:   README.md
        new file:   Vagrantfile
        new file:   code/core-site.xml
        new file:   code/environment
        new file:   code/hdfs-site.xml
        new file:   code/workers
        new file:   code/yarn-site.xml
        new file:   configs/environment
        new file:   configs/hadoop/core-site.xml
        new file:   configs/hadoop/hdfs-site.xml
        new file:   configs/hadoop/workers
        new file:   configs/hadoop/yarn-site.xml
        new file:   packages/hadoop-3.2.1.tar.gz
        new file:   packages/jdk-8u171-linux-x64.tar.gz
        new file:   packages/jdk-8u91-linux-x64.tar.gz
        new file:   packages/spark-3.2.0-bin-hadoop3.2.tgz
        renamed:    existingArchitecture.PNG -> pictures/existingArchitecture.PNG
        renamed:    proposalArchitecture.PNG -> pictures/proposalArchitecture.PNG
        new file:   scripts/setup-hadoop-package.sh
        new file:   scripts/setup-hadoop-user-and-keygen.sh
        new file:   scripts/setup-hosts.sh
        new file:   scripts/setup-java.sh
        new file:   scripts/setup-ssh.sh
        new file:   scripts/variable.sh

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/


thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git restore
fatal: you must specify path(s) to restore

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git restore --staged
fatal: you must specify path(s) to restore

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git restore --staged scripts

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git restore --staged packages

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        modified:   GitBashLog
        new file:   IssueLogSummary.txt
        modified:   README.md
        new file:   Vagrantfile
        new file:   code/core-site.xml
        new file:   code/environment
        new file:   code/hdfs-site.xml
        new file:   code/workers
        new file:   code/yarn-site.xml
        new file:   configs/environment
        new file:   configs/hadoop/core-site.xml
        new file:   configs/hadoop/hdfs-site.xml
        new file:   configs/hadoop/workers
        new file:   configs/hadoop/yarn-site.xml
        renamed:    existingArchitecture.PNG -> pictures/existingArchitecture.PNG
        renamed:    proposalArchitecture.PNG -> pictures/proposalArchitecture.PNG

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        packages/
        scripts/


thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add scripts/
warning: LF will be replaced by CRLF in scripts/setup-hadoop-package.sh.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in scripts/setup-hadoop-user-and-keygen.sh.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in scripts/setup-hosts.sh.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in scripts/setup-java.sh.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in scripts/variable.sh.
The file will have its original line endings in your working directory

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        modified:   GitBashLog
        new file:   IssueLogSummary.txt
        modified:   README.md
        new file:   Vagrantfile
        new file:   code/core-site.xml
        new file:   code/environment
        new file:   code/hdfs-site.xml
        new file:   code/workers
        new file:   code/yarn-site.xml
        new file:   configs/environment
        new file:   configs/hadoop/core-site.xml
        new file:   configs/hadoop/hdfs-site.xml
        new file:   configs/hadoop/workers
        new file:   configs/hadoop/yarn-site.xml
        renamed:    existingArchitecture.PNG -> pictures/existingArchitecture.PNG
        renamed:    proposalArchitecture.PNG -> pictures/proposalArchitecture.PNG
        new file:   scripts/setup-hadoop-package.sh
        new file:   scripts/setup-hadoop-user-and-keygen.sh
        new file:   scripts/setup-hosts.sh
        new file:   scripts/setup-java.sh
        new file:   scripts/setup-ssh.sh
        new file:   scripts/variable.sh

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        packages/


thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git commit -m "Add Vagrantfile, code, configs and scripts folder to git"
[main ee50279] Add Vagrantfile, code, configs and scripts folder to git
 22 files changed, 2010 insertions(+), 28 deletions(-)
 create mode 100644 IssueLogSummary.txt
 create mode 100644 Vagrantfile
 create mode 100644 code/core-site.xml
 create mode 100644 code/environment
 create mode 100644 code/hdfs-site.xml
 create mode 100644 code/workers
 create mode 100644 code/yarn-site.xml
 create mode 100644 configs/environment
 create mode 100644 configs/hadoop/core-site.xml
 create mode 100644 configs/hadoop/hdfs-site.xml
 create mode 100644 configs/hadoop/workers
 create mode 100644 configs/hadoop/yarn-site.xml
 rename existingArchitecture.PNG => pictures/existingArchitecture.PNG (100%)
 rename proposalArchitecture.PNG => pictures/proposalArchitecture.PNG (100%)
 create mode 100644 scripts/setup-hadoop-package.sh
 create mode 100644 scripts/setup-hadoop-user-and-keygen.sh
 create mode 100644 scripts/setup-hosts.sh
 create mode 100644 scripts/setup-java.sh
 create mode 100644 scripts/setup-ssh.sh
 create mode 100644 scripts/variable.sh

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is ahead of 'origin/main' by 1 commit.
  (use "git push" to publish your local commits)

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        packages/

nothing added to commit but untracked files present (use "git add" to track)

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git push
Enumerating objects: 25, done.
Counting objects: 100% (25/25), done.
Delta compression using up to 4 threads
Compressing objects: 100% (21/21), done.
Writing objects: 100% (22/22), 18.58 KiB | 3.10 MiB/s, done.
Total 22 (delta 4), reused 0 (delta 0), pack-reused 0
remote: Resolving deltas: 100% (4/4), completed with 1 local object.
To https://github.com/ITC-IFBD/ca3-TanThienNguyenVN.git
   dcf5a27..ee50279  main -> main

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        packages/

nothing added to commit but untracked files present (use "git add" to track)

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$


----------- 16 Dec 

Initialization script completed
schemaTool completed
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 26c2ef84-8dc4-4852-a518-e5e54e830e65

Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = f05539c1-46ac-424d-b7b2-4f8776a8d669
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive>
    >
    > show tables;
OK
Time taken: 2.685 seconds
hive>
    >
    > show databases;
OK
default
Time taken: 0.102 seconds, Fetched: 1 row(s)
hive> create table emp(id int);
OK
Time taken: 1.975 seconds
hive> use default;
OK
Time taken: 0.106 seconds
hive> show tables;
OK
emp
Time taken: 0.085 seconds, Fetched: 1 row(s)
hive> create table emp2(id int, name string);
OK
Time taken: 0.232 seconds
hive> show tables;
OK
emp
emp2
Time taken: 0.077 seconds, Fetched: 2 row(s)

hive> insert into emp2(id, name) values(1,'jonathan');
Query ID = vagrant_20211216201501_c347c26c-ed90-4b4f-820d-0431f94b25d5
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-16 20:15:14,895 Stage-1 map = 0%,  reduce = 0%
Ended Job = job_local1589817452_0001 with errors
Error during job, obtaining debugging information...
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 FAIL
Total MapReduce CPU Time Spent: 0 msec
hive> select * from emp2;
OK
Time taken: 1.486 seconds
hive> quit;
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$ sudo cp -f /app/code/hive-config.sh /usr/local/hive/bin/hive-config.sh
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$ exit
logout
vagrant@node01:~$ exit
logout
Connection to 127.0.0.1 closed.


---------------- 17 Dec 2021 
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 9bea5542-5e42-41e3-9a10-4bd76e8a43ab

Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Hive Session ID = d871cc7f-e8f2-47da-afcd-82ff734b9176
hive> show databases;
OK
default
Time taken: 2.661 seconds, Fetched: 1 row(s)
hive> show tables;
OK
Time taken: 0.135 seconds
hive> use default;
OK
Time taken: 0.075 seconds
hive> show tables;
OK
Time taken: 0.089 seconds
hive> create tables student(id int, name string);
NoViableAltException(302@[917:1: ddlStatement : ( createDatabaseStatement | switchDatabaseStatement | dropDatabaseStatement | createTableStatement | dropTableStatement | truncateTableStatement | alterStatement | descStatement | showStatement | metastoreCheck | createViewStatement | createMaterializedViewStatement | dropViewStatement | dropMaterializedViewStatement | createFunctionStatement | createMacroStatement | dropFunctionStatement | reloadFunctionStatement | dropMacroStatement | analyzeStatement | lockStatement | unlockStatement | lockDatabase | unlockDatabase | createRoleStatement | dropRoleStatement | ( grantPrivileges )=> grantPrivileges | ( revokePrivileges )=> revokePrivileges | showGrants | showRoleGrants | showRolePrincipals | showRoles | grantRole | revokeRole | setRole | showCurrentRole | abortTransactionStatement | killQueryStatement | resourcePlanDdlStatements );])
        at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
        at org.antlr.runtime.DFA.predict(DFA.java:116)
        at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:4244)
        at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2494)
        at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1420)
        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:220)
        at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:74)
        at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
FAILED: ParseException line 1:7 cannot recognize input near 'create' 'tables' 'student' in ddl statement
hive> create tables student(id int, name str);
NoViableAltException(302@[917:1: ddlStatement : ( createDatabaseStatement | switchDatabaseStatement | dropDatabaseStatement | createTableStatement | dropTableStatement | truncateTableStatement | alterStatement | descStatement | showStatement | metastoreCheck | createViewStatement | createMaterializedViewStatement | dropViewStatement | dropMaterializedViewStatement | createFunctionStatement | createMacroStatement | dropFunctionStatement | reloadFunctionStatement | dropMacroStatement | analyzeStatement | lockStatement | unlockStatement | lockDatabase | unlockDatabase | createRoleStatement | dropRoleStatement | ( grantPrivileges )=> grantPrivileges | ( revokePrivileges )=> revokePrivileges | showGrants | showRoleGrants | showRolePrincipals | showRoles | grantRole | revokeRole | setRole | showCurrentRole | abortTransactionStatement | killQueryStatement | resourcePlanDdlStatements );])
        at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
        at org.antlr.runtime.DFA.predict(DFA.java:116)
        at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:4244)
        at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2494)
        at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1420)
        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:220)
        at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:74)
        at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
FAILED: ParseException line 1:7 cannot recognize input near 'create' 'tables' 'student' in ddl statement
hive> create table student(id int, name string);
OK
Time taken: 1.903 seconds
hive>
    >
    > show tables;
OK
student
Time taken: 0.095 seconds, Fetched: 1 row(s)
hive>
    >
    > insert into student(id, name) values(1,'Tan');
Query ID = vagrant_20211217114324_de581daa-f8c3-4512-9f67-b5a66775795c
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 11:43:35,485 Stage-1 map = 0%,  reduce = 0%
2021-12-17 11:43:37,815 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local171194880_0001
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to directory hdfs://node01:9000/user/hive/warehouse/student/.hive-staging_hive_2021-12-17_11-43-24_336_5374324444857991258-1/-ext-10000
Loading data to table default.student
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 0 HDFS Write: 154 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 15.275 seconds
hive>
    > select * from student;
OK
1       Tan
Time taken: 0.411 seconds, Fetched: 1 row(s)
hive>
    >
    > insert into student(id, name) values(2,'Michael');
Query ID = vagrant_20211217121048_73b726ed-cffd-4889-b4cd-bcf2b82e326b
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 12:10:50,889 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local2108682540_0002
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to directory hdfs://node01:9000/user/hive/warehouse/student/.hive-staging_hive_2021-12-17_12-10-48_314_6642290871077150158-1/-ext-10000
Loading data to table default.student
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 154 HDFS Write: 316 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 3.354 seconds
hive> delete from student where id = 1;
FAILED: SemanticException [Error 10294]: Attempt to do update or delete using transaction manager that does not support these operations.
hive> select * from student;
OK
1       Tan
2       Michael
Time taken: 0.268 seconds, Fetched: 2 row(s)
hive> update student set name = 'TANNNNNNN' where id = 1;
FAILED: SemanticException [Error 10294]: Attempt to do update or delete using transaction manager that does not support these operations.
hive> insert into student(id, name) values(3,'chloe');
Query ID = vagrant_20211217121426_22aef6e8-248d-4288-9272-8fb98e443ef7
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 12:14:29,821 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1882206019_0003
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to directory hdfs://node01:9000/user/hive/warehouse/student/.hive-staging_hive_2021-12-17_12-14-26_622_8666976546970702503-1/-ext-10000
Loading data to table default.student
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 328 HDFS Write: 474 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 3.747 seconds
hive> quit;
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$ ls
beeline    ext   hive-config.sh  hplsql            metastore_db  schematool
derby.log  hive  hiveserver2     init-hive-dfs.sh  metatool
vagrant@node01:/usr/local/hive/bin$ cd ..
vagrant@node01:/usr/local/hive$ ls
LICENSE  RELEASE_NOTES.txt  binary-package-licenses  examples  jdbc  scripts
NOTICE   bin                conf                     hcatalog  lib
vagrant@node01:/usr/local/hive$ cd conf/
vagrant@node01:/usr/local/hive/conf$ ls
beeline-log4j2.properties.template    ivysettings.xml
hive-default.xml.template             llap-cli-log4j2.properties.template
hive-env.sh.template                  llap-daemon-log4j2.properties.template
hive-exec-log4j2.properties.template  parquet-logging.properties
hive-log4j2.properties.template
vagrant@node01:/usr/local/hive/conf$
vagrant@node01:/usr/local/hive/conf$ cp /app/code/hive-site.xml /usr/local/hive/conf/hive-site.xml
vagrant@node01:/usr/local/hive/conf$
vagrant@node01:/usr/local/hive/conf$ cd ..
vagrant@node01:/usr/local/hive$ cd bin
vagrant@node01:/usr/local/hive/bin$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Exception in thread "main" java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Illegal character entity: expansion character (code 0x8
 at [row,col,system-id]: [3215,96,"file:/usr/local/hive/conf/hive-site.xml"]
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3024)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2973)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2848)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1460)
        at org.apache.hadoop.hive.conf.HiveConf.getVar(HiveConf.java:4996)
        at org.apache.hadoop.hive.conf.HiveConf.getVar(HiveConf.java:5069)
        at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:5156)
        at org.apache.hadoop.hive.conf.HiveConf.<init>(HiveConf.java:5099)
        at org.apache.hadoop.hive.common.LogUtils.initHiveLog4jCommon(LogUtils.java:97)
        at org.apache.hadoop.hive.common.LogUtils.initHiveLog4j(LogUtils.java:81)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:699)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
Caused by: com.ctc.wstx.exc.WstxParsingException: Illegal character entity: expansion character (code 0x8
 at [row,col,system-id]: [3215,96,"file:/usr/local/hive/conf/hive-site.xml"]
        at com.ctc.wstx.sr.StreamScanner.constructWfcException(StreamScanner.java:621)
        at com.ctc.wstx.sr.StreamScanner.throwParseError(StreamScanner.java:491)
        at com.ctc.wstx.sr.StreamScanner.reportIllegalChar(StreamScanner.java:2456)
        at com.ctc.wstx.sr.StreamScanner.validateChar(StreamScanner.java:2403)
        at com.ctc.wstx.sr.StreamScanner.resolveCharEnt(StreamScanner.java:2369)
        at com.ctc.wstx.sr.StreamScanner.fullyResolveEntity(StreamScanner.java:1515)
        at com.ctc.wstx.sr.BasicStreamReader.nextFromTree(BasicStreamReader.java:2828)
        at com.ctc.wstx.sr.BasicStreamReader.next(BasicStreamReader.java:1123)
        at org.apache.hadoop.conf.Configuration$Parser.parseNext(Configuration.java:3320)
        at org.apache.hadoop.conf.Configuration$Parser.parse(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3007)
        ... 17 more
vagrant@node01:/usr/local/hive/bin$ cp /app/code/hive-site.xml /usr/local/hive/conf/hive-site.xml
vagrant@node01:/usr/local/hive/bin$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 8b55f00d-0c0b-483a-aeaa-433fce05c0e0

Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Exception in thread "main" java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D
        at org.apache.hadoop.fs.Path.initialize(Path.java:263)
        at org.apache.hadoop.fs.Path.<init>(Path.java:221)
        at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:710)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:627)
        at org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:591)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:747)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D
        at java.net.URI.checkPath(URI.java:1823)
        at java.net.URI.<init>(URI.java:745)
        at org.apache.hadoop.fs.Path.initialize(Path.java:260)
        ... 12 more
vagrant@node01:/usr/local/hive/bin$ cp /app/code/hive-site.xml /usr/local/hive/conf/hive-site.xml
vagrant@node01:/usr/local/hive/bin$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 700774ad-092a-4eff-8af8-e209c6420196

Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Hive Session ID = 61e1bd69-1f14-4dda-a331-a0476dd207c9
hive> show tables;
OK
student
Time taken: 3.5 seconds, Fetched: 1 row(s)
hive>
    >
    > select * from student;
OK
1       Tan
2       Michael
3       chloe
Time taken: 8.18 seconds, Fetched: 3 row(s)
hive>
    >
    >
    > update student set name = 'Bui Thi To Tam' where id = 3;
FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table default.student that is not transactional
hive> insert into student(id, name) values(4, 'Derry');
Query ID = vagrant_20211217123102_0733b02c-8b91-44c7-b54e-52cc333ab937
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 12:31:08,589 Stage-1 map = 0%,  reduce = 0%
2021-12-17 12:31:09,637 Stage-1 map = 100%,  reduce = 0%
2021-12-17 12:31:10,658 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local596842800_0001
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to directory hdfs://node01:9000/user/hive/warehouse/student/.hive-staging_hive_2021-12-17_12-31-02_238_5555990086122599136-1/-ext-10000
Loading data to table default.student
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 48 HDFS Write: 158 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 11.573 seconds
hive> select * from student;
OK
1       Tan
2       Michael
3       chloe
4       Derry
Time taken: 0.49 seconds, Fetched: 4 row(s)
hive> delete from student where id = 4;
FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table default.student that is not transactional
hive> Set hive.enforce.bucketing = true
    > delete from student where id = 4;
Warning: Value had a \n character in it.
hive>
    >
    > update student set name = 'Bui Thi To Tam' where id = 3;
FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table default.student that is not transactional
hive> delete from student where id = 4;
FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table default.student that is not transactional
hive> Set hive.enforce.bucketing=true;
hive> delete from student where id = 4;
FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table default.student that is not transactional
hive> set hive.enforce.bucketing=true;
hive> delete from student where id = 4;
FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table default.student that is not transactional
hive> quit;
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$ cp /app/code/hive-site.xml /usr/local/hive/conf/hive-site.xml
vagrant@node01:/usr/local/hive/bin$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = d5c731fe-804d-43d9-8ecb-c7af0d6f0ba9

Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Hive Session ID = e9f1b774-02e1-4a10-8588-669f3a617fed
hive>
    > show tables;
OK
student
Time taken: 2.92 seconds, Fetched: 1 row(s)
hive> select * from student;
OK
1       Tan
2       Michael
3       chloe
4       Derry
Time taken: 6.277 seconds, Fetched: 4 row(s)
hive> delete from student where id = 4;
FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table default.student that is not transactional
hive> ALTER TABLE student SET TBLPROPERTIES ('transactional'='true');
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Unable to alter table. The table must be stored using an ACID compliant format (such as ORC): default.student
hive> use default;
OK
Time taken: 0.051 seconds
hive> ALTER TABLE student SET TBLPROPERTIES ('transactional'='true');
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Unable to alter table. The table must be stored using an ACID compliant format (such as ORC): default.student
hive> ALTER TABLE default.student SET TBLPROPERTIES ('transactional'='true');
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Unable to alter table. The table must be stored using an ACID compliant format (such as ORC): default.student
hive>
    >
    > DROP TABLE IF EXISTS hive_acid_demo;
OK
Time taken: 0.365 seconds
hive>
    > CREATE TABLE hive_acid_demo (key int, value int)
    > CLUSTERED BY(key) INTO 3 BUCKETS
    > STORED AS ORC
    > TBLPROPERTIES ('transactional'='true');
OK
Time taken: 2.909 seconds
hive> INSERT INTO hive_acid_demo  VALUES (1, 1);
Query ID = vagrant_20211217124442_a242ac65-bf38-4d71-b4b2-e8bb4dcb584e
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 12:44:51,666 Stage-1 map = 100%,  reduce = 0%
2021-12-17 12:44:54,302 Stage-1 map = 100%,  reduce = 33%
2021-12-17 12:44:55,341 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local203778061_0001
Loading data to table default.hive_acid_demo
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 12:44:58,825 Stage-3 map = 0%,  reduce = 0%
2021-12-17 12:44:59,840 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_local1939811958_0002
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 128 HDFS Write: 1674 SUCCESS
Stage-Stage-3:  HDFS Read: 64 HDFS Write: 1674 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 18.02 seconds
hive> INSERT INTO hive_acid_demo  VALUES (2, 2);
Query ID = vagrant_20211217124500_c07ff94f-2425-4c27-88c1-4a82b71e719d
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 12:45:04,326 Stage-1 map = 100%,  reduce = 33%
2021-12-17 12:45:05,384 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local671004215_0003
Loading data to table default.hive_acid_demo
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 12:45:08,301 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_local317315130_0004
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 824 HDFS Write: 4329 SUCCESS
Stage-Stage-3:  HDFS Read: 412 HDFS Write: 3348 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 8.153 seconds
hive> INSERT INTO hive_acid_demo  VALUES (3, 3);
Query ID = vagrant_20211217124508_d7475d0d-90f4-4d1e-a56f-851e69ff4de1
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 12:45:12,489 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local555784457_0005
Loading data to table default.hive_acid_demo
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 12:45:15,477 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_local332035184_0006
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 1520 HDFS Write: 7677 SUCCESS
Stage-Stage-3:  HDFS Read: 760 HDFS Write: 5022 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 7.149 seconds
hive> INSERT INTO hive_acid_demo  VALUES (4, 4);
Query ID = vagrant_20211217124523_42fc0550-9abb-42bf-83bc-8cf31ee782f8
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 12:45:26,593 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1386117200_0007
Loading data to table default.hive_acid_demo
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 12:45:29,714 Stage-3 map = 0%,  reduce = 0%
2021-12-17 12:45:30,720 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_local901265395_0008
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 2216 HDFS Write: 11025 SUCCESS
Stage-Stage-3:  HDFS Read: 1108 HDFS Write: 6696 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 8.052 seconds
hive> select * from hive_acid;
FAILED: SemanticException [Error 10001]: Line 1:14 Table not found 'hive_acid'
hive> select * from hive_acid_demo;
OK
1       1
2       2
3       3
4       4
Time taken: 0.765 seconds, Fetched: 4 row(s)
hive> update table hive_acid_demo update value = 444 where key = 4;
NoViableAltException(301@[212:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])
        at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
        at org.antlr.runtime.DFA.predict(DFA.java:116)
        at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4513)
        at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:45148)
        at org.apache.hadoop.hive.ql.parse.HiveParser.updateStatement(HiveParser.java:42589)
        at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2522)
        at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1420)
        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:220)
        at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:74)
        at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
FAILED: ParseException line 1:7 cannot recognize input near 'table' 'hive_acid_demo' 'update' in table name
hive> update table hive_acid_demo set value = 444 where key = 4;
NoViableAltException(301@[212:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])
        at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
        at org.antlr.runtime.DFA.predict(DFA.java:116)
        at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4513)
        at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:45148)
        at org.apache.hadoop.hive.ql.parse.HiveParser.updateStatement(HiveParser.java:42589)
        at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2522)
        at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1420)
        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:220)
        at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:74)
        at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
FAILED: ParseException line 1:7 cannot recognize input near 'table' 'hive_acid_demo' 'set' in table name
hive> update hive_acid_demo set value = 444 where key = 4;
Query ID = vagrant_20211217124656_a2b1563e-0e98-4928-9154-7a6cb1aeb926
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 12:47:00,564 Stage-1 map = 100%,  reduce = 0%
2021-12-17 12:47:02,760 Stage-1 map = 100%,  reduce = 67%
2021-12-17 12:47:03,787 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local922096767_0009
Loading data to table default.hive_acid_demo
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 43764 HDFS Write: 25150 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 7.717 seconds
hive> select * from hive_acid_demo;
OK
1       1
2       2
3       3
4       444
Time taken: 1.311 seconds, Fetched: 4 row(s)
hive>




--- git status 
thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        modified:   GitBashLog
        deleted:    IssueLogSummary.txt
        modified:   README.md
        modified:   Vagrantfile
        modified:   scripts/setup-hadoop-package.sh
        modified:   scripts/setup-hadoop-user-and-keygen.sh
        modified:   scripts/setup-hosts.sh
        modified:   scripts/setup-java.sh
        modified:   scripts/variable.sh

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        Testing Check List and Issue Log Summary.sh
        backup/
        code/hadoop.sh
        code/hive-config.sh
        code/hive.sh
        configs/hadoop/hadoop.sh
        configs/hive/
        packages/
        scripts/setup-hadoop-user-and-keygen_16Dec2021.sh
        scripts/setup-hive-package.sh

no changes added to commit (use "git add" and/or "git commit -a")

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add Vagrantfile

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add scripts
warning: LF will be replaced by CRLF in scripts/setup-hadoop-package.sh.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in scripts/setup-hadoop-user-and-keygen.sh.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in scripts/setup-hosts.sh.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in scripts/setup-java.sh.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in scripts/variable.sh.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in scripts/setup-hadoop-user-and-keygen_16Dec2021.sh.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in scripts/setup-hive-package.sh.
The file will have its original line endings in your working directory

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add code
warning: LF will be replaced by CRLF in code/hadoop.sh.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in code/hive-config.sh.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in code/hive.sh.
The file will have its original line endings in your working directory

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add configs
warning: LF will be replaced by CRLF in configs/hadoop/hadoop.sh.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in configs/hive/hive-config.sh.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in configs/hive/hive-default.xml.template.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in configs/hive/hive-env.sh.template.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in configs/hive/hive.sh.
The file will have its original line endings in your working directory

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        modified:   Vagrantfile
        new file:   code/hadoop.sh
        new file:   code/hive-config.sh
        new file:   code/hive.sh
        new file:   configs/hadoop/hadoop.sh
        new file:   configs/hive/hive-config.sh
        new file:   configs/hive/hive-default.xml.template
        new file:   configs/hive/hive-env.sh.template
        new file:   configs/hive/hive.sh
        modified:   scripts/setup-hadoop-package.sh
        modified:   scripts/setup-hadoop-user-and-keygen.sh
        new file:   scripts/setup-hadoop-user-and-keygen_16Dec2021.sh
        new file:   scripts/setup-hive-package.sh
        modified:   scripts/setup-hosts.sh
        modified:   scripts/setup-java.sh
        modified:   scripts/variable.sh

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        modified:   GitBashLog
        deleted:    IssueLogSummary.txt
        modified:   README.md

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        Testing Check List and Issue Log Summary.sh
        backup/
        packages/


thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        modified:   Vagrantfile
        new file:   code/hadoop.sh
        new file:   code/hive-config.sh
        new file:   code/hive.sh
        new file:   configs/hadoop/hadoop.sh
        new file:   configs/hive/hive-config.sh
        new file:   configs/hive/hive-default.xml.template
        new file:   configs/hive/hive-env.sh.template
        new file:   configs/hive/hive.sh
        modified:   scripts/setup-hadoop-package.sh
        modified:   scripts/setup-hadoop-user-and-keygen.sh
        new file:   scripts/setup-hadoop-user-and-keygen_16Dec2021.sh
        new file:   scripts/setup-hive-package.sh
        modified:   scripts/setup-hosts.sh
        modified:   scripts/setup-java.sh
        modified:   scripts/variable.sh

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        modified:   GitBashLog
        deleted:    IssueLogSummary.txt
        modified:   README.md

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        Testing-Check-List-and-Issue-Log-Summary.sh
        backup/
        packages/


thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git add Testing-Check-List-and-Issue-Log-Summary.sh

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        new file:   Testing-Check-List-and-Issue-Log-Summary.sh
        modified:   Vagrantfile
        new file:   code/hadoop.sh
        new file:   code/hive-config.sh
        new file:   code/hive.sh
        new file:   configs/hadoop/hadoop.sh
        new file:   configs/hive/hive-config.sh
        new file:   configs/hive/hive-default.xml.template
        new file:   configs/hive/hive-env.sh.template
        new file:   configs/hive/hive.sh
        modified:   scripts/setup-hadoop-package.sh
        modified:   scripts/setup-hadoop-user-and-keygen.sh
        new file:   scripts/setup-hadoop-user-and-keygen_16Dec2021.sh
        new file:   scripts/setup-hive-package.sh
        modified:   scripts/setup-hosts.sh
        modified:   scripts/setup-java.sh
        modified:   scripts/variable.sh

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        modified:   GitBashLog
        deleted:    IssueLogSummary.txt
        modified:   README.md

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        backup/
        packages/


thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git commit -m "Hadoop Cluster and Hive completed"
[main 28f8422] Hadoop Cluster and Hive completed
 17 files changed, 7355 insertions(+), 73 deletions(-)
 create mode 100644 Testing-Check-List-and-Issue-Log-Summary.sh
 create mode 100644 code/hadoop.sh
 create mode 100644 code/hive-config.sh
 create mode 100644 code/hive.sh
 create mode 100644 configs/hadoop/hadoop.sh
 create mode 100644 configs/hive/hive-config.sh
 create mode 100644 configs/hive/hive-default.xml.template
 create mode 100644 configs/hive/hive-env.sh.template
 create mode 100644 configs/hive/hive.sh
 rewrite scripts/setup-hadoop-user-and-keygen.sh (71%)
 copy scripts/{setup-hadoop-user-and-keygen.sh => setup-hadoop-user-and-keygen_16Dec2021.sh} (100%)
 create mode 100644 scripts/setup-hive-package.sh

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git status
On branch main
Your branch is ahead of 'origin/main' by 1 commit.
  (use "git push" to publish your local commits)

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        modified:   GitBashLog
        deleted:    IssueLogSummary.txt
        modified:   README.md

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        .vagrant/
        backup/
        packages/

no changes added to commit (use "git add" and/or "git commit -a")

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ git push
Enumerating objects: 32, done.
Counting objects: 100% (32/32), done.
Delta compression using up to 4 threads
Compressing objects: 100% (21/21), done.
Writing objects: 100% (21/21), 69.01 KiB | 4.06 MiB/s, done.
Total 21 (delta 8), reused 0 (delta 0), pack-reused 0
remote: Resolving deltas: 100% (8/8), completed with 7 local objects.
To https://github.com/ITC-IFBD/ca3-TanThienNguyenVN.git
   ee50279..28f8422  main -> main

thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)


------------ 17 December 2021 23 PM ---
---- Fix some bug regarding hive ---- 
thien@TAM-PC MINGW64 /d/MScDataScience/04.InfrastructureBigData/CA3 (main)
$ vagrant ssh node01
Welcome to Ubuntu 20.04.3 LTS (GNU/Linux 5.4.0-90-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Fri Dec 17 21:42:19 UTC 2021

  System load:  0.16              Processes:               117
  Usage of /:   7.8% of 38.71GB   Users logged in:         0
  Memory usage: 14%               IPv4 address for enp0s3: 10.0.2.15
  Swap usage:   0%                IPv4 address for enp0s8: 172.16.0.11


8 updates can be applied immediately.
8 of these updates are standard security updates.
To see these additional updates run: apt list --upgradable


*** System restart required ***
vagrant@node01:~$ ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 02:65:a5:66:b2:98 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic enp0s3
       valid_lft 84920sec preferred_lft 84920sec
    inet6 fe80::65:a5ff:fe66:b298/64 scope link
       valid_lft forever preferred_lft forever
3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 08:00:27:45:40:3e brd ff:ff:ff:ff:ff:ff
    inet 172.16.0.11/24 brd 172.16.0.255 scope global enp0s8
       valid_lft forever preferred_lft forever
    inet6 fe80::a00:27ff:fe45:403e/64 scope link
       valid_lft forever preferred_lft forever
vagrant@node01:~$ ping node01
PING node01 (172.16.0.11) 56(84) bytes of data.
64 bytes from node01 (172.16.0.11): icmp_seq=1 ttl=64 time=0.067 ms
64 bytes from node01 (172.16.0.11): icmp_seq=2 ttl=64 time=0.049 ms
^C
--- node01 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1015ms
rtt min/avg/max/mdev = 0.049/0.058/0.067/0.009 ms
vagrant@node01:~$ ping node02
PING node02 (172.16.0.12) 56(84) bytes of data.
64 bytes from node02 (172.16.0.12): icmp_seq=1 ttl=64 time=0.930 ms
64 bytes from node02 (172.16.0.12): icmp_seq=2 ttl=64 time=0.384 ms
^C
--- node02 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1092ms
rtt min/avg/max/mdev = 0.384/0.657/0.930/0.273 ms
vagrant@node01:~$ ping node03
PING node03 (172.16.0.13) 56(84) bytes of data.
64 bytes from node03 (172.16.0.13): icmp_seq=1 ttl=64 time=1.08 ms
64 bytes from node03 (172.16.0.13): icmp_seq=2 ttl=64 time=0.467 ms
^C
--- node03 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 0.467/0.774/1.081/0.307 ms
vagrant@node01:~$ ping google.com
PING google.com (209.85.203.101) 56(84) bytes of data.
64 bytes from dh-in-f101.1e100.net (209.85.203.101): icmp_seq=1 ttl=108 time=12.2 ms
64 bytes from dh-in-f101.1e100.net (209.85.203.101): icmp_seq=2 ttl=108 time=11.6 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 11.600/11.897/12.194/0.297 ms
vagrant@node01:~$
vagrant@node01:~$ more /etc/hosts
172.16.0.11 node01
172.16.0.12 node02
172.16.0.13 node03
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
vagrant@node01:~$
vagrant@node01:~$ java -version
java version "1.8.0_171"
Java(TM) SE Runtime Environment (build 1.8.0_171-b11)
Java HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode)
vagrant@node01:~$
vagrant@node01:~$
vagrant@node01:~$ echo $JAVA_HOME
/usr/local/java
vagrant@node01:~$ echo $HADOOP_HOME
/usr/local/hadoop
vagrant@node01:~$ echo $PATH
/usr/local/java/bin:/usr/local/hive/bin:/sbin:/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin
vagrant@node01:~$ more /etc/environment
PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/u
sr/local/games:/snap/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin"

JAVA_HOME="/usr/local/java"
vagrant@node01:~$ cd $HADOOP_HOME
vagrant@node01:/usr/local/hadoop$ cd /etc/hadoop
-bash: cd: /etc/hadoop: No such file or directory
vagrant@node01:/usr/local/hadoop$ cd etc
vagrant@node01:/usr/local/hadoop/etc$ cd hadoop/
vagrant@node01:/usr/local/hadoop/etc/hadoop$ ls
capacity-scheduler.xml            kms-log4j.properties
configuration.xsl                 kms-site.xml
container-executor.cfg            log4j.properties
core-site.xml                     mapred-env.cmd
hadoop-env.cmd                    mapred-env.sh
hadoop-env.sh                     mapred-queues.xml.template
hadoop-metrics2.properties        mapred-site.xml
hadoop-policy.xml                 shellprofile.d
hadoop-user-functions.sh.example  ssl-client.xml.example
hdfs-site.xml                     ssl-server.xml.example
httpfs-env.sh                     user_ec_policies.xml.template
httpfs-log4j.properties           workers
httpfs-signature.secret           yarn-env.cmd
httpfs-site.xml                   yarn-env.sh
kms-acls.xml                      yarn-site.xml
kms-env.sh                        yarnservice-log4j.properties
vagrant@node01:/usr/local/hadoop/etc/hadoop$ more core-site.xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
  <property>
    <name>fs.default.name</name>
    <value>hdfs://node01:9000</value>
  </property>
</configuration>
vagrant@node01:/usr/local/hadoop/etc/hadoop$ more yarn-site.xml
<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>node01</value>
  </property>
</configuration>
vagrant@node01:/usr/local/hadoop/etc/hadoop$ more hdfs-site.xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>/usr/local/hadoop/data/nameNode</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>/usr/local/hadoop/data/dataNode</value>
  </property>
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
</configuration>

vagrant@node01:/usr/local/hadoop/etc/hadoop$
vagrant@node01:/usr/local/hadoop/etc/hadoop$ cd $HIVE_HOME
vagrant@node01:/usr/local/hive$ cd bin
vagrant@node01:/usr/local/hive/bin$ ls
beeline  hive            hiveserver2  init-hive-dfs.sh  schematool
ext      hive-config.sh  hplsql       metatool
vagrant@node01:/usr/local/hive/bin$ more hive-config.sh
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#
# processes --config option from command line
#

this="$0"
while [ -h "$this" ]; do
  ls=`ls -ld "$this"`
  link=`expr "$ls" : '.*-> \(.*\)$'`
  if expr "$link" : '.*/.*' > /dev/null; then
    this="$link"
  else
    this=`dirname "$this"`/"$link"
  fi
done

# convert relative path to absolute path
bin=`dirname "$this"`
script=`basename "$this"`
bin=`cd "$bin"; pwd`
this="$bin/$script"

# the root of the Hive installation
if [[ -z $HIVE_HOME ]] ; then
  export HIVE_HOME=`dirname "$bin"`
fi

#check to see if the conf dir is given as an optional argument
while [ $# -gt 0 ]; do    # Until you run out of parameters . . .
  case "$1" in
    --config)
        shift
        confdir=$1
        shift
        HIVE_CONF_DIR=$confdir
        ;;
    --auxpath)
        shift
        HIVE_AUX_JARS_PATH=$1
        shift
        ;;
    *)
        break;
        ;;
  esac
done


# Allow alternate conf dir location.
HIVE_CONF_DIR="${HIVE_CONF_DIR:-$HIVE_HOME/conf}"

export HIVE_CONF_DIR=$HIVE_CONF_DIR
export HIVE_AUX_JARS_PATH=$HIVE_AUX_JARS_PATH

# Default to use 256MB
export HADOOP_HEAPSIZE=${HADOOP_HEAPSIZE:-256}
export HADOOP_HOME=/usr/local/hadoop
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$ cd ..
vagrant@node01:/usr/local/hive$ cd conf
vagrant@node01:/usr/local/hive/conf$ more hive-site.xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?><!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
--><configuration>
  <!-- WARNING!!! This file is auto generated for documentation purposes ONLY! -
->
  <!-- WARNING!!! Any changes you make to this file will be ignored by Hive.   -
->
  <!-- WARNING!!! You must make your changes in hive-site.xml instead.         -
->
  <!-- Hive Execution Parameters -->
  <property>
    <name>hive.exec.script.wrapper</name>
    <value/>
    <description/>
  </property>
  <property>
    <name>hive.exec.plan</name>
    <value/>
    <description/>
  </property>
  <property>
    <name>hive.exec.stagingdir</name>
    <value>.hive-staging</value>
    <description>Directory name that will be created inside table locations in o
rder to support HDFS encryption. This is replaces ${hive.exec.scratchdir} for qu
ery results with the exception of read-only tables. In all cases ${hive.exec.scr
atchdir} is still used for other temporary files, such as job plans.</descriptio
n>
  </property>
  <property>
    <name>hive.exec.scratchdir</name>
    <value>/tmp/hive</value>
    <description>HDFS root scratch dir for Hive jobs which gets created with wri
te all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.e
xec.scratchdir}/&lt;username&gt; is created, with ${hive.scratch.dir.permission}
.</description>
  </property>
  <property>
    <name>hive.repl.rootdir</name>
    <value>/user/hive/repl/</value>
    <description>HDFS root dir for all replication dumps.</description>
  </property>
  <property>
    <name>hive.repl.cm.enabled</name>
    <value>false</value>
    <description>Turn on ChangeManager, so delete files will go to cmrootdir.</d
escription>
  </property>
  <property>
    <name>hive.repl.cmrootdir</name>
    <value>/user/hive/cmroot/</value>
    <description>Root dir for ChangeManager, used for deleted files.</descriptio
n>
  </property>
  <property>
    <name>hive.repl.cm.retain</name>
    <value>24h</value>
    <description>
      Expects a time value with unit (d/day, h/hour, m/min, s/sec, ms/msec, us/u
sec, ns/nsec), which is hour if not specified.
      Time to retain removed files in cmrootdir.
    </description>
  </property>
  <property>
    <name>hive.repl.cm.interval</name>
    <value>3600s</value>
    <description>
      Expects a time value with unit (d/day, h/hour, m/min, s/sec, ms/msec, us/u
sec, ns/nsec), which is sec if not specified.
      Inteval for cmroot cleanup thread.
    </description>
  </property>
  <property>
    <name>hive.repl.replica.functions.root.dir</name>
    <value>/user/hive/repl/functions/</value>
    <description>Root directory on the replica warehouse where the repl sub-syst
em will store jars from the primary warehouse</description>
  </property>
  <property>
    <name>hive.repl.approx.max.load.tasks</name>
    <value>10000</value>
    <description>
      Provide an approximation of the maximum number of tasks that should be exe
cuted before
      dynamically generating the next set of tasks. The number is approximate as
 Hive
      will stop at a slightly higher number, the reason being some events might
lead to a
      task increment that would cross the specified limit.
    </description>
  </property>
  <property>
    <name>hive.repl.partitions.dump.parallelism</name>
    <value>100</value>
    <description>Number of threads that will be used to dump partition data info
rmation during repl dump.</description>
  </property>
  <property>
    <name>hive.repl.dumpdir.clean.freq</name>
    <value>0s</value>
    <description>
      Expects a time value with unit (d/day, h/hour, m/min, s/sec, ms/msec, us/u
sec, ns/nsec), which is sec if not specified.
      Frequency at which timer task runs to purge expired dump dirs.
    </description>
  </property>
  <property>
    <name>hive.repl.dumpdir.ttl</name>
    <value>7d</value>
    <description>
      Expects a time value with unit (d/day, h/hour, m/min, s/sec, ms/msec, us/u
sec, ns/nsec), which is day if not specified.
      TTL of dump dirs before cleanup.
    </description>
  </property>
  <property>
    <name>hive.repl.dump.metadata.only</name>
    <value>false</value>
    <description>Indicates whether replication dump only metadata information or
 data + metadata.</description>
  </property>
  <property>
    <name>hive.repl.dump.include.acid.tables</name>
    <value>false</value>
    <description>
      Indicates if repl dump should include information about ACID tables. It sh
ould be
      used in conjunction with 'hive.repl.dump.metadata.only' to enable copying
of
      metadata for acid tables which do not require the corresponding transactio
n
      semantics to be applied on target. This can be removed when ACID table
      replication is supported.
    </description>
  </property>
  <property>
    <name>hive.repl.bootstrap.dump.open.txn.timeout</name>
    <value>1h</value>
    <description>
      Expects a time value with unit (d/day, h/hour, m/min, s/sec, ms/msec, us/u
sec, ns/nsec), which is hour if not specified.
      Indicates the timeout for all transactions which are opened before trigger
ing bootstrap REPL DUMP. If these open transactions are not closed within the ti
meout value, then REPL DUMP will forcefully abort those transactions and continu
e with bootstrap dump.
vagrant@node01:/usr/local/hive/conf$
vagrant@node01:/usr/local/hive/conf$
vagrant@node01:/usr/local/hive/conf$
vagrant@node01:/usr/local/hive/conf$
vagrant@node01:/usr/local/hive/conf$ cd ..
vagrant@node01:/usr/local/hive$ cd lib/
vagrant@node01:/usr/local/hive/lib$ ls
HikariCP-2.6.1.jar
ST4-4.0.4.jar
accumulo-core-1.7.3.jar
accumulo-fate-1.7.3.jar
accumulo-start-1.7.3.jar
accumulo-trace-1.7.3.jar
aircompressor-0.10.jar
ant-1.9.1.jar
ant-launcher-1.9.1.jar
antlr-runtime-3.5.2.jar
antlr4-runtime-4.5.jar
aopalliance-repackaged-2.5.0-b32.jar
apache-curator-2.12.0.pom
apache-jsp-9.3.20.v20170531.jar
apache-jstl-9.3.20.v20170531.jar
arrow-format-0.8.0.jar
arrow-memory-0.8.0.jar
arrow-vector-0.8.0.jar
asm-5.0.1.jar
asm-commons-5.0.1.jar
asm-tree-5.0.1.jar
audience-annotations-0.5.0.jar
avatica-1.11.0.jar
avro-1.7.7.jar
bonecp-0.8.0.RELEASE.jar
calcite-core-1.16.0.jar
calcite-druid-1.16.0.jar
calcite-linq4j-1.16.0.jar
commons-cli-1.2.jar
commons-codec-1.7.jar
commons-collections4-4.1.jar
commons-compiler-2.7.6.jar
commons-compress-1.9.jar
commons-crypto-1.0.0.jar
commons-dbcp-1.4.jar
commons-io-2.4.jar
commons-lang-2.6.jar
commons-lang3-3.2.jar
commons-logging-1.0.4.jar
commons-math-2.1.jar
commons-math3-3.6.1.jar
commons-pool-1.5.4.jar
commons-vfs2-2.1.jar
curator-client-2.12.0.jar
curator-framework-2.12.0.jar
curator-recipes-2.12.0.jar
datanucleus-api-jdo-4.2.4.jar
datanucleus-core-4.1.17.jar
datanucleus-rdbms-4.1.19.jar
derby-10.14.1.0.jar
disruptor-3.3.6.jar
dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar
druid-hdfs-storage-0.12.0.jar
ecj-4.4.2.jar
esri-geometry-api-2.0.0.jar
findbugs-annotations-1.3.9-1.jar
flatbuffers-1.2.0-3f79e055.jar
groovy-all-2.4.11.jar
gson-2.2.4.jar
guava-27.0-jre.jar
hbase-client-2.0.0-alpha4.jar
hbase-common-2.0.0-alpha4-tests.jar
hbase-common-2.0.0-alpha4.jar
hbase-hadoop-compat-2.0.0-alpha4.jar
hbase-hadoop2-compat-2.0.0-alpha4-tests.jar
hbase-hadoop2-compat-2.0.0-alpha4.jar
hbase-http-2.0.0-alpha4.jar
hbase-mapreduce-2.0.0-alpha4.jar
hbase-metrics-2.0.0-alpha4.jar
hbase-metrics-api-2.0.0-alpha4.jar
hbase-prefix-tree-2.0.0-alpha4.jar
hbase-procedure-2.0.0-alpha4.jar
hbase-protocol-2.0.0-alpha4.jar
hbase-protocol-shaded-2.0.0-alpha4.jar
hbase-replication-2.0.0-alpha4.jar
hbase-server-2.0.0-alpha4.jar
hbase-shaded-miscellaneous-1.0.1.jar
hbase-shaded-netty-1.0.1.jar
hbase-shaded-protobuf-1.0.1.jar
hive-accumulo-handler-3.1.2.jar
hive-beeline-3.1.2.jar
hive-classification-3.1.2.jar
hive-cli-3.1.2.jar
hive-common-3.1.2.jar
hive-contrib-3.1.2.jar
hive-druid-handler-3.1.2.jar
hive-exec-3.1.2.jar
hive-hbase-handler-3.1.2.jar
hive-hcatalog-core-3.1.2.jar
hive-hcatalog-server-extensions-3.1.2.jar
hive-hplsql-3.1.2.jar
hive-jdbc-3.1.2.jar
hive-jdbc-handler-3.1.2.jar
hive-kryo-registrator-3.1.2.jar
hive-llap-client-3.1.2.jar
hive-llap-common-3.1.2-tests.jar
hive-llap-common-3.1.2.jar
hive-llap-ext-client-3.1.2.jar
hive-llap-server-3.1.2.jar
hive-llap-tez-3.1.2.jar
hive-metastore-3.1.2.jar
hive-serde-3.1.2.jar
hive-service-3.1.2.jar
hive-service-rpc-3.1.2.jar
hive-shims-0.23-3.1.2.jar
hive-shims-3.1.2.jar
hive-shims-common-3.1.2.jar
hive-shims-scheduler-3.1.2.jar
hive-standalone-metastore-3.1.2.jar
hive-storage-api-2.7.0.jar
hive-streaming-3.1.2.jar
hive-testutils-3.1.2.jar
hive-upgrade-acid-3.1.2.jar
hive-vector-code-gen-3.1.2.jar
hk2-api-2.5.0-b32.jar
hk2-locator-2.5.0-b32.jar
hk2-utils-2.5.0-b32.jar
hppc-0.7.2.jar
htrace-core-3.2.0-incubating.jar
httpclient-4.5.2.jar
httpcore-4.4.4.jar
ivy-2.4.0.jar
jackson-annotations-2.9.5.jar
jackson-core-2.9.5.jar
jackson-core-asl-1.9.13.jar
jackson-databind-2.9.5.jar
jackson-dataformat-smile-2.9.5.jar
jackson-mapper-asl-1.9.13.jar
jamon-runtime-2.3.1.jar
janino-2.7.6.jar
javassist-3.20.0-GA.jar
javax.annotation-api-1.2.jar
javax.inject-2.5.0-b32.jar
javax.jdo-3.2.0-m3.jar
javax.servlet-api-3.1.0.jar
javax.servlet.jsp-2.3.2.jar
javax.servlet.jsp-api-2.3.1.jar
javax.ws.rs-api-2.0.1.jar
javolution-5.5.1.jar
jaxb-api-2.2.11.jar
jcodings-1.0.18.jar
jcommander-1.32.jar
jdo-api-3.0.1.jar
jersey-client-2.25.1.jar
jersey-common-2.25.1.jar
jersey-container-servlet-core-2.25.1.jar
jersey-guava-2.25.1.jar
jersey-media-jaxb-2.25.1.jar
jersey-server-2.25.1.jar
jettison-1.1.jar
jetty-annotations-9.3.20.v20170531.jar
jetty-client-9.3.20.v20170531.jar
jetty-http-9.3.20.v20170531.jar
jetty-io-9.3.20.v20170531.jar
jetty-jaas-9.3.20.v20170531.jar
jetty-jndi-9.3.20.v20170531.jar
jetty-plus-9.3.20.v20170531.jar
jetty-rewrite-9.3.20.v20170531.jar
jetty-runner-9.3.20.v20170531.jar
jetty-schemas-3.1.jar
jetty-security-9.3.20.v20170531.jar
jetty-server-9.3.20.v20170531.jar
jetty-servlet-9.3.20.v20170531.jar
jetty-util-9.3.20.v20170531.jar
jetty-webapp-9.3.20.v20170531.jar
jetty-xml-9.3.20.v20170531.jar
jline-2.12.jar
joda-time-2.9.9.jar
joni-2.1.11.jar
jpam-1.1.jar
json-1.8.jar
jsr305-3.0.0.jar
jta-1.1.jar
libfb303-0.9.3.jar
libthrift-0.9.3.jar
log4j-1.2-api-2.10.0.jar
log4j-api-2.10.0.jar
log4j-core-2.10.0.jar
log4j-slf4j-impl-2.10.0.jar
log4j-web-2.10.0.jar
memory-0.9.0.jar
metrics-core-3.1.0.jar
metrics-json-3.1.0.jar
metrics-jvm-3.1.0.jar
mysql-metadata-storage-0.12.0.jar
netty-3.10.5.Final.jar
netty-all-4.1.17.Final.jar
netty-buffer-4.1.17.Final.jar
netty-common-4.1.17.Final.jar
opencsv-2.3.jar
orc-core-1.5.6.jar
orc-shims-1.5.6.jar
org.abego.treelayout.core-1.0.1.jar
osgi-resource-locator-1.0.1.jar
paranamer-2.3.jar
parquet-hadoop-bundle-1.10.0.jar
php
postgresql-9.4.1208.jre7.jar
postgresql-metadata-storage-0.12.0.jar
protobuf-java-2.5.0.jar
py
sketches-core-0.9.0.jar
snappy-java-1.1.4.jar
sqlline-1.3.0.jar
stax-api-1.0.1.jar
super-csv-2.2.0.jar
taglibs-standard-impl-1.2.5.jar
taglibs-standard-spec-1.2.5.jar
tempus-fugit-1.1.jar
transaction-api-1.1.jar
validation-api-1.1.0.Final.jar
velocity-1.5.jar
websocket-api-9.3.20.v20170531.jar
websocket-client-9.3.20.v20170531.jar
websocket-common-9.3.20.v20170531.jar
websocket-server-9.3.20.v20170531.jar
websocket-servlet-9.3.20.v20170531.jar
zookeeper-3.4.6.jar
vagrant@node01:/usr/local/hive/lib$
vagrant@node01:/usr/local/hive/lib$ cd
vagrant@node01:~$ groups
vagrant sudo
vagrant@node01:~$
vagrant@node01:~$ ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/home/vagrant/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /home/vagrant/.ssh/id_rsa
Your public key has been saved in /home/vagrant/.ssh/id_rsa.pub
The key fingerprint is:
SHA256:euPxi/rMnbRp7HY7tz8mliuiglFd0WO2pgtNrC1ZRsg vagrant@node01
The key's randomart image is:
+---[RSA 3072]----+
|      . ..o      |
|       E o =     |
|      . + o o    |
|     . . = o     |
|    .   S o      |
|   .   * +       |
|    o . *.o   .  |
|   . . = X+=o+.o |
|      o+B+Xo+==oo|
+----[SHA256]-----+
vagrant@node01:~$ ssh-copy-id vagrant@node01
/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/vagrant/.ssh/id_rsa.pub"
The authenticity of host 'node01 (172.16.0.11)' can't be established.
ECDSA key fingerprint is SHA256:kUhvglVCHq3IqGxyVSpeoOlGdXigfEmvacyQAdWDo88.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
vagrant@node01's password:

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'vagrant@node01'"
and check to make sure that only the key(s) you wanted were added.

vagrant@node01:~$ ssh-copy-id vagrant@node02
/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/vagrant/.ssh/id_rsa.pub"
The authenticity of host 'node02 (172.16.0.12)' can't be established.
ECDSA key fingerprint is SHA256:dfJDYeHT3agvGa3B6SF1NUqPNWARgeoNiN42sKGDMT4.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
vagrant@node02's password:

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'vagrant@node02'"
and check to make sure that only the key(s) you wanted were added.

vagrant@node01:~$ ssh-copy-id vagrant@node03
/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/vagrant/.ssh/id_rsa.pub"
The authenticity of host 'node03 (172.16.0.13)' can't be established.
ECDSA key fingerprint is SHA256:cGIn7wQ8wIkAO3Hjp3q/+BDdWRiAJjJ8u8J5uLwSYPo.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
vagrant@node03's password:
Permission denied, please try again.
vagrant@node03's password:

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'vagrant@node03'"
and check to make sure that only the key(s) you wanted were added.

vagrant@node01:~$ source /etc/environment
vagrant@node01:~$ hdfs namenode -format
WARNING: /usr/local/hadoop/logs does not exist. Creating.
2021-12-17 21:49:06,458 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = node01/172.16.0.11
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 3.2.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.5.6.jar:/usr/local/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/usr/local/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-2.9.8.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.6.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-text-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/usr/local/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-5.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/common/lib/json-smart-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.4.10.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/local/hadoop/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/local/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.18.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/local/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-kms-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/local/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/usr/local/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar
STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z
STARTUP_MSG:   java = 1.8.0_171
************************************************************/
2021-12-17 21:49:06,504 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2021-12-17 21:49:06,868 INFO namenode.NameNode: createNameNode [-format]
2021-12-17 21:49:09,091 INFO common.Util: Assuming 'file' scheme for path /usr/local/hadoop/data/nameNode in configuration.
2021-12-17 21:49:09,093 INFO common.Util: Assuming 'file' scheme for path /usr/local/hadoop/data/nameNode in configuration.
Formatting using clusterid: CID-ad536484-8fe3-49d6-9ee5-c57fb4ac8316
2021-12-17 21:49:09,236 INFO namenode.FSEditLog: Edit logging is async:true
2021-12-17 21:49:09,289 INFO namenode.FSNamesystem: KeyProvider: null
2021-12-17 21:49:09,294 INFO namenode.FSNamesystem: fsLock is fair: true
2021-12-17 21:49:09,295 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2021-12-17 21:49:09,331 INFO namenode.FSNamesystem: fsOwner             = vagrant (auth:SIMPLE)
2021-12-17 21:49:09,337 INFO namenode.FSNamesystem: supergroup          = supergroup
2021-12-17 21:49:09,338 INFO namenode.FSNamesystem: isPermissionEnabled = true
2021-12-17 21:49:09,339 INFO namenode.FSNamesystem: HA Enabled: false
2021-12-17 21:49:09,495 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2021-12-17 21:49:09,522 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2021-12-17 21:49:09,523 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2021-12-17 21:49:09,537 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2021-12-17 21:49:09,546 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Dec 17 21:49:09
2021-12-17 21:49:09,551 INFO util.GSet: Computing capacity for map BlocksMap
2021-12-17 21:49:09,551 INFO util.GSet: VM type       = 64-bit
2021-12-17 21:49:09,560 INFO util.GSet: 2.0% max memory 443 MB = 8.9 MB
2021-12-17 21:49:09,560 INFO util.GSet: capacity      = 2^20 = 1048576 entries
2021-12-17 21:49:09,582 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
2021-12-17 21:49:09,583 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
2021-12-17 21:49:09,604 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2021-12-17 21:49:09,605 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2021-12-17 21:49:09,606 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2021-12-17 21:49:09,607 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2021-12-17 21:49:09,609 INFO blockmanagement.BlockManager: defaultReplication         = 2
2021-12-17 21:49:09,609 INFO blockmanagement.BlockManager: maxReplication             = 512
2021-12-17 21:49:09,610 INFO blockmanagement.BlockManager: minReplication             = 1
2021-12-17 21:49:09,610 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
2021-12-17 21:49:09,611 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2021-12-17 21:49:09,611 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
2021-12-17 21:49:09,611 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2021-12-17 21:49:09,766 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
2021-12-17 21:49:09,766 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
2021-12-17 21:49:09,767 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
2021-12-17 21:49:09,767 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
2021-12-17 21:49:09,813 INFO util.GSet: Computing capacity for map INodeMap
2021-12-17 21:49:09,814 INFO util.GSet: VM type       = 64-bit
2021-12-17 21:49:09,815 INFO util.GSet: 1.0% max memory 443 MB = 4.4 MB
2021-12-17 21:49:09,815 INFO util.GSet: capacity      = 2^19 = 524288 entries
2021-12-17 21:49:09,819 INFO namenode.FSDirectory: ACLs enabled? false
2021-12-17 21:49:09,819 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
2021-12-17 21:49:09,820 INFO namenode.FSDirectory: XAttrs enabled? true
2021-12-17 21:49:09,821 INFO namenode.NameNode: Caching file names occurring more than 10 times
2021-12-17 21:49:09,836 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2021-12-17 21:49:09,841 INFO snapshot.SnapshotManager: SkipList is disabled
2021-12-17 21:49:09,855 INFO util.GSet: Computing capacity for map cachedBlocks
2021-12-17 21:49:09,856 INFO util.GSet: VM type       = 64-bit
2021-12-17 21:49:09,857 INFO util.GSet: 0.25% max memory 443 MB = 1.1 MB
2021-12-17 21:49:09,858 INFO util.GSet: capacity      = 2^17 = 131072 entries
2021-12-17 21:49:09,881 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2021-12-17 21:49:09,881 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2021-12-17 21:49:09,881 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2021-12-17 21:49:09,893 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
2021-12-17 21:49:09,893 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2021-12-17 21:49:09,900 INFO util.GSet: Computing capacity for map NameNodeRetryCache
2021-12-17 21:49:09,900 INFO util.GSet: VM type       = 64-bit
2021-12-17 21:49:09,901 INFO util.GSet: 0.029999999329447746% max memory 443 MB = 136.1 KB
2021-12-17 21:49:09,901 INFO util.GSet: capacity      = 2^14 = 16384 entries
2021-12-17 21:49:10,047 INFO namenode.FSImage: Allocated new BlockPoolId: BP-329011198-172.16.0.11-1639777750019
2021-12-17 21:49:10,085 INFO common.Storage: Storage directory /usr/local/hadoop/data/nameNode has been successfully formatted.
2021-12-17 21:49:10,182 INFO namenode.FSImageFormatProtobuf: Saving image file /usr/local/hadoop/data/nameNode/current/fsimage.ckpt_0000000000000000000 using no compression
2021-12-17 21:49:10,450 INFO namenode.FSImageFormatProtobuf: Image file /usr/local/hadoop/data/nameNode/current/fsimage.ckpt_0000000000000000000 of size 402 bytes saved in 0 seconds .
2021-12-17 21:49:10,489 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
2021-12-17 21:49:10,508 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
2021-12-17 21:49:10,510 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at node01/172.16.0.11
************************************************************/
vagrant@node01:~$
vagrant@node01:~$
vagrant@node01:~$ start-dfs.sh
Starting namenodes on [node01]
Starting datanodes
node03: WARNING: /usr/local/hadoop/logs does not exist. Creating.
node02: WARNING: /usr/local/hadoop/logs does not exist. Creating.
Starting secondary namenodes [node01]
vagrant@node01:~$ jps

Command 'jps' not found, but can be installed with:

sudo apt install openjdk-11-jdk-headless  # version 11.0.13+8-0ubuntu1~20.04, or
sudo apt install openjdk-16-jdk-headless  # version 16.0.1+9-1~20.04
sudo apt install openjdk-17-jdk-headless  # version 17.0.1+12-1~20.04
sudo apt install openjdk-8-jdk-headless   # version 8u312-b07-0ubuntu1~20.04
sudo apt install openjdk-13-jdk-headless  # version 13.0.7+5-0ubuntu1~20.04

vagrant@node01:~$ su - vagrant
Password:
vagrant@node01:~$ jps
12184 Jps
11769 NameNode
12031 SecondaryNameNode
vagrant@node01:~$ start-yarn.sh
WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
Starting resourcemanager
WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
Starting nodemanagers
WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
vagrant@node01:~$ jps
12578 Jps
12279 ResourceManager
11769 NameNode
12031 SecondaryNameNode
vagrant@node01:~$ yarn node -list
WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
2021-12-17 21:52:38,884 INFO client.RMProxy: Connecting to ResourceManager at node01/172.16.0.11:8032
Total Nodes:2
         Node-Id             Node-State Node-Http-Address       Number-of-Running-Containers
    node03:35187                RUNNING       node03:8042                                  0
    node02:34299                RUNNING       node02:8042                                  0
vagrant@node01:~$ yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar pi 16 1000
WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
Number of Maps  = 16
Samples per Map = 1000
2021-12-17 21:52:58,963 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #0
2021-12-17 21:53:00,675 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #1
2021-12-17 21:53:00,819 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #2
2021-12-17 21:53:00,892 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #3
2021-12-17 21:53:00,977 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #4
2021-12-17 21:53:01,053 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #5
2021-12-17 21:53:01,155 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #6
2021-12-17 21:53:01,244 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #7
2021-12-17 21:53:01,335 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #8
2021-12-17 21:53:01,420 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #9
2021-12-17 21:53:01,506 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #10
2021-12-17 21:53:01,584 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #11
2021-12-17 21:53:01,693 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #12
2021-12-17 21:53:01,783 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #13
2021-12-17 21:53:01,859 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #14
2021-12-17 21:53:01,936 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
Wrote input for Map #15
Starting Job
2021-12-17 21:53:02,268 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2021-12-17 21:53:02,470 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2021-12-17 21:53:02,471 INFO impl.MetricsSystemImpl: JobTracker metrics system started
2021-12-17 21:53:03,772 INFO input.FileInputFormat: Total input files to process : 16
2021-12-17 21:53:03,795 INFO mapreduce.JobSubmitter: number of splits:16
2021-12-17 21:53:04,514 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1098479873_0001
2021-12-17 21:53:04,515 INFO mapreduce.JobSubmitter: Executing with tokens: []
2021-12-17 21:53:04,862 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
2021-12-17 21:53:04,864 INFO mapreduce.Job: Running job: job_local1098479873_0001
2021-12-17 21:53:04,874 INFO mapred.LocalJobRunner: OutputCommitter set in config null
2021-12-17 21:53:04,892 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-17 21:53:04,893 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-17 21:53:04,897 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2021-12-17 21:53:05,047 INFO mapred.LocalJobRunner: Waiting for map tasks
2021-12-17 21:53:05,051 INFO mapred.LocalJobRunner: Starting task: attempt_local1098479873_0001_m_000000_0
2021-12-17 21:53:05,130 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-17 21:53:05,132 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-17 21:53:05,197 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-17 21:53:05,212 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/vagrant/QuasiMonteCarlo_1639777974723_256190444/in/part0:0+118
2021-12-17 21:53:05,477 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-17 21:53:05,477 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-17 21:53:05,477 INFO mapred.MapTask: soft limit at 83886080
2021-12-17 21:53:05,477 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-17 21:53:05,477 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-17 21:53:05,487 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-17 21:53:05,870 INFO mapreduce.Job: Job job_local1098479873_0001 running in uber mode : false
2021-12-17 21:53:05,873 INFO mapreduce.Job:  map 0% reduce 0%
2021-12-17 21:53:05,875 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2021-12-17 21:53:06,043 INFO mapred.LocalJobRunner:
2021-12-17 21:53:06,047 INFO mapred.MapTask: Starting flush of map output
2021-12-17 21:53:06,047 INFO mapred.MapTask: Spilling map output
2021-12-17 21:53:06,048 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-17 21:53:06,048 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-17 21:53:06,062 INFO mapred.MapTask: Finished spill 0
2021-12-17 21:53:06,098 INFO mapred.Task: Task:attempt_local1098479873_0001_m_000000_0 is done. And is in the process of committing
2021-12-17 21:53:06,111 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-17 21:53:06,112 INFO mapred.Task: Task 'attempt_local1098479873_0001_m_000000_0' done.
2021-12-17 21:53:06,136 INFO mapred.Task: Final Counters for attempt_local1098479873_0001_m_000000_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=319218
                FILE: Number of bytes written=846418
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=118
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=7
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=280494080
        File Input Format Counters
                Bytes Read=118
2021-12-17 21:53:06,156 INFO mapred.LocalJobRunner: Finishing task: attempt_local1098479873_0001_m_000000_0
2021-12-17 21:53:06,159 INFO mapred.LocalJobRunner: Starting task: attempt_local1098479873_0001_m_000001_0
2021-12-17 21:53:06,172 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-17 21:53:06,174 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-17 21:53:06,176 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-17 21:53:06,182 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/vagrant/QuasiMonteCarlo_1639777974723_256190444/in/part1:0+118
2021-12-17 21:53:06,305 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-17 21:53:06,306 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-17 21:53:06,306 INFO mapred.MapTask: soft limit at 83886080
2021-12-17 21:53:06,307 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-17 21:53:06,308 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-17 21:53:06,313 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-17 21:53:06,338 INFO mapred.LocalJobRunner:
2021-12-17 21:53:06,339 INFO mapred.MapTask: Starting flush of map output
2021-12-17 21:53:06,339 INFO mapred.MapTask: Spilling map output
2021-12-17 21:53:06,339 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-17 21:53:06,339 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-17 21:53:06,344 INFO mapred.MapTask: Finished spill 0
2021-12-17 21:53:06,355 INFO mapred.Task: Task:attempt_local1098479873_0001_m_000001_0 is done. And is in the process of committing
2021-12-17 21:53:06,366 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-17 21:53:06,367 INFO mapred.Task: Task 'attempt_local1098479873_0001_m_000001_0' done.
2021-12-17 21:53:06,375 INFO mapred.Task: Final Counters for attempt_local1098479873_0001_m_000001_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=321579
                FILE: Number of bytes written=846478
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=236
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=10
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=385875968
        File Input Format Counters
                Bytes Read=118
2021-12-17 21:53:06,376 INFO mapred.LocalJobRunner: Finishing task: attempt_local1098479873_0001_m_000001_0
2021-12-17 21:53:06,376 INFO mapred.LocalJobRunner: Starting task: attempt_local1098479873_0001_m_000002_0
2021-12-17 21:53:06,380 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-17 21:53:06,380 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-17 21:53:06,383 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-17 21:53:06,391 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/vagrant/QuasiMonteCarlo_1639777974723_256190444/in/part10:0+118
2021-12-17 21:53:06,507 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-17 21:53:06,507 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-17 21:53:06,508 INFO mapred.MapTask: soft limit at 83886080
2021-12-17 21:53:06,508 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-17 21:53:06,509 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-17 21:53:06,511 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-17 21:53:06,533 INFO mapred.LocalJobRunner:
2021-12-17 21:53:06,536 INFO mapred.MapTask: Starting flush of map output
2021-12-17 21:53:06,537 INFO mapred.MapTask: Spilling map output
2021-12-17 21:53:06,537 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-17 21:53:06,538 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-17 21:53:06,542 INFO mapred.MapTask: Finished spill 0
2021-12-17 21:53:06,547 INFO mapred.Task: Task:attempt_local1098479873_0001_m_000002_0 is done. And is in the process of committing
2021-12-17 21:53:06,560 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-17 21:53:06,560 INFO mapred.Task: Task 'attempt_local1098479873_0001_m_000002_0' done.
2021-12-17 21:53:06,562 INFO mapred.Task: Final Counters for attempt_local1098479873_0001_m_000002_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=323940
                FILE: Number of bytes written=846538
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=354
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=13
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=146
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=491257856
        File Input Format Counters
                Bytes Read=118
2021-12-17 21:53:06,564 INFO mapred.LocalJobRunner: Finishing task: attempt_local1098479873_0001_m_000002_0
2021-12-17 21:53:06,565 INFO mapred.LocalJobRunner: Starting task: attempt_local1098479873_0001_m_000003_0
2021-12-17 21:53:06,568 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-17 21:53:06,568 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-17 21:53:06,569 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-17 21:53:06,571 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/vagrant/QuasiMonteCarlo_1639777974723_256190444/in/part11:0+118
2021-12-17 21:53:06,660 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-17 21:53:06,660 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-17 21:53:06,661 INFO mapred.MapTask: soft limit at 83886080
2021-12-17 21:53:06,661 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-17 21:53:06,661 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-17 21:53:06,662 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-17 21:53:06,675 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2021-12-17 21:53:06,779 INFO mapred.LocalJobRunner:
2021-12-17 21:53:06,780 INFO mapred.MapTask: Starting flush of map output
2021-12-17 21:53:06,781 INFO mapred.MapTask: Spilling map output
2021-12-17 21:53:06,781 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-17 21:53:06,781 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-17 21:53:06,784 INFO mapred.MapTask: Finished spill 0
2021-12-17 21:53:06,788 INFO mapred.Task: Task:attempt_local1098479873_0001_m_000003_0 is done. And is in the process of committing
2021-12-17 21:53:06,795 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-17 21:53:06,795 INFO mapred.Task: Task 'attempt_local1098479873_0001_m_000003_0' done.
2021-12-17 21:53:06,797 INFO mapred.Task: Final Counters for attempt_local1098479873_0001_m_000003_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=326301
                FILE: Number of bytes written=846598
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=472
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=16
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=146
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=39
                Total committed heap usage (bytes)=495976448
        File Input Format Counters
                Bytes Read=118
2021-12-17 21:53:06,798 INFO mapred.LocalJobRunner: Finishing task: attempt_local1098479873_0001_m_000003_0
2021-12-17 21:53:06,798 INFO mapred.LocalJobRunner: Starting task: attempt_local1098479873_0001_m_000004_0
2021-12-17 21:53:06,801 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-17 21:53:06,801 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-17 21:53:06,804 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-17 21:53:06,810 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/vagrant/QuasiMonteCarlo_1639777974723_256190444/in/part12:0+118
2021-12-17 21:53:06,878 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-17 21:53:06,879 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-17 21:53:06,879 INFO mapred.MapTask: soft limit at 83886080
2021-12-17 21:53:06,880 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-17 21:53:06,880 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-17 21:53:06,884 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-17 21:53:06,887 INFO mapreduce.Job:  map 100% reduce 0%
2021-12-17 21:53:06,904 INFO mapred.LocalJobRunner:
2021-12-17 21:53:06,905 INFO mapred.MapTask: Starting flush of map output
2021-12-17 21:53:06,905 INFO mapred.MapTask: Spilling map output
2021-12-17 21:53:06,905 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-17 21:53:06,906 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-17 21:53:06,914 INFO mapred.MapTask: Finished spill 0
2021-12-17 21:53:06,921 INFO mapred.Task: Task:attempt_local1098479873_0001_m_000004_0 is done. And is in the process of committing
2021-12-17 21:53:06,931 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-17 21:53:06,932 INFO mapred.Task: Task 'attempt_local1098479873_0001_m_000004_0' done.
2021-12-17 21:53:06,933 INFO mapred.Task: Final Counters for attempt_local1098479873_0001_m_000004_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=328150
                FILE: Number of bytes written=846658
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=590
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=19
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=146
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=31
                Total committed heap usage (bytes)=514326528
        File Input Format Counters
                Bytes Read=118
2021-12-17 21:53:06,936 INFO mapred.LocalJobRunner: Finishing task: attempt_local1098479873_0001_m_000004_0
2021-12-17 21:53:06,936 INFO mapred.LocalJobRunner: Starting task: attempt_local1098479873_0001_m_000005_0
2021-12-17 21:53:06,941 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-17 21:53:06,941 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-17 21:53:06,942 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-17 21:53:06,947 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/vagrant/QuasiMonteCarlo_1639777974723_256190444/in/part13:0+118
2021-12-17 21:53:07,176 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-17 21:53:07,176 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-17 21:53:07,176 INFO mapred.MapTask: soft limit at 83886080
2021-12-17 21:53:07,176 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-17 21:53:07,176 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-17 21:53:07,178 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-17 21:53:07,194 INFO mapred.LocalJobRunner:
2021-12-17 21:53:07,195 INFO mapred.MapTask: Starting flush of map output
2021-12-17 21:53:07,196 INFO mapred.MapTask: Spilling map output
2021-12-17 21:53:07,197 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-17 21:53:07,198 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-17 21:53:07,202 INFO mapred.MapTask: Finished spill 0
2021-12-17 21:53:07,210 INFO mapred.Task: Task:attempt_local1098479873_0001_m_000005_0 is done. And is in the process of committing
2021-12-17 21:53:07,217 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-17 21:53:07,218 INFO mapred.Task: Task 'attempt_local1098479873_0001_m_000005_0' done.
2021-12-17 21:53:07,220 INFO mapred.Task: Final Counters for attempt_local1098479873_0001_m_000005_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=329999
                FILE: Number of bytes written=846718
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=708
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=22
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=146
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=193
                Total committed heap usage (bytes)=213385216
        File Input Format Counters
                Bytes Read=118
2021-12-17 21:53:07,229 INFO mapred.LocalJobRunner: Finishing task: attempt_local1098479873_0001_m_000005_0
2021-12-17 21:53:07,229 INFO mapred.LocalJobRunner: Starting task: attempt_local1098479873_0001_m_000006_0
2021-12-17 21:53:07,235 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-17 21:53:07,236 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-17 21:53:07,237 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-17 21:53:07,240 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/vagrant/QuasiMonteCarlo_1639777974723_256190444/in/part14:0+118
2021-12-17 21:53:07,330 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-17 21:53:07,330 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-17 21:53:07,330 INFO mapred.MapTask: soft limit at 83886080
2021-12-17 21:53:07,330 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-17 21:53:07,330 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-17 21:53:07,332 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-17 21:53:07,358 INFO mapred.LocalJobRunner:
2021-12-17 21:53:07,359 INFO mapred.MapTask: Starting flush of map output
2021-12-17 21:53:07,359 INFO mapred.MapTask: Spilling map output
2021-12-17 21:53:07,360 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-17 21:53:07,361 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-17 21:53:07,364 INFO mapred.MapTask: Finished spill 0
2021-12-17 21:53:07,370 INFO mapred.Task: Task:attempt_local1098479873_0001_m_000006_0 is done. And is in the process of committing
2021-12-17 21:53:07,383 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-17 21:53:07,384 INFO mapred.Task: Task 'attempt_local1098479873_0001_m_000006_0' done.
2021-12-17 21:53:07,387 INFO mapred.Task: Final Counters for attempt_local1098479873_0001_m_000006_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=331848
                FILE: Number of bytes written=846778
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=826
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=25
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=146
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=318767104
        File Input Format Counters
                Bytes Read=118
2021-12-17 21:53:07,397 INFO mapred.LocalJobRunner: Finishing task: attempt_local1098479873_0001_m_000006_0
2021-12-17 21:53:07,397 INFO mapred.LocalJobRunner: Starting task: attempt_local1098479873_0001_m_000007_0
2021-12-17 21:53:07,400 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-17 21:53:07,401 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-17 21:53:07,403 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-17 21:53:07,406 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/vagrant/QuasiMonteCarlo_1639777974723_256190444/in/part15:0+118
2021-12-17 21:53:07,513 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-17 21:53:07,514 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-17 21:53:07,514 INFO mapred.MapTask: soft limit at 83886080
2021-12-17 21:53:07,515 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-17 21:53:07,516 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-17 21:53:07,518 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-17 21:53:07,560 INFO mapred.LocalJobRunner:
2021-12-17 21:53:07,561 INFO mapred.MapTask: Starting flush of map output
2021-12-17 21:53:07,562 INFO mapred.MapTask: Spilling map output
2021-12-17 21:53:07,563 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-17 21:53:07,564 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-17 21:53:07,567 INFO mapred.MapTask: Finished spill 0
2021-12-17 21:53:07,571 INFO mapred.Task: Task:attempt_local1098479873_0001_m_000007_0 is done. And is in the process of committing
2021-12-17 21:53:07,578 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-17 21:53:07,579 INFO mapred.Task: Task 'attempt_local1098479873_0001_m_000007_0' done.
2021-12-17 21:53:07,580 INFO mapred.Task: Final Counters for attempt_local1098479873_0001_m_000007_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=333185
                FILE: Number of bytes written=846838
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=944
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=28
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=146
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=424148992
        File Input Format Counters
                Bytes Read=118
2021-12-17 21:53:07,592 INFO mapred.LocalJobRunner: Finishing task: attempt_local1098479873_0001_m_000007_0
2021-12-17 21:53:07,593 INFO mapred.LocalJobRunner: Starting task: attempt_local1098479873_0001_m_000008_0
2021-12-17 21:53:07,597 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-17 21:53:07,598 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-17 21:53:07,603 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-17 21:53:07,606 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/vagrant/QuasiMonteCarlo_1639777974723_256190444/in/part2:0+118
2021-12-17 21:53:07,716 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-17 21:53:07,717 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-17 21:53:07,718 INFO mapred.MapTask: soft limit at 83886080
2021-12-17 21:53:07,718 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-17 21:53:07,718 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-17 21:53:07,722 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-17 21:53:07,737 INFO mapred.LocalJobRunner:
2021-12-17 21:53:07,737 INFO mapred.MapTask: Starting flush of map output
2021-12-17 21:53:07,738 INFO mapred.MapTask: Spilling map output
2021-12-17 21:53:07,738 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-17 21:53:07,738 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-17 21:53:07,741 INFO mapred.MapTask: Finished spill 0
2021-12-17 21:53:07,744 INFO mapred.Task: Task:attempt_local1098479873_0001_m_000008_0 is done. And is in the process of committing
2021-12-17 21:53:07,752 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-17 21:53:07,752 INFO mapred.Task: Task 'attempt_local1098479873_0001_m_000008_0' done.
2021-12-17 21:53:07,753 INFO mapred.Task: Final Counters for attempt_local1098479873_0001_m_000008_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=334522
                FILE: Number of bytes written=846898
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1062
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=31
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=514326528
        File Input Format Counters
                Bytes Read=118
2021-12-17 21:53:07,755 INFO mapred.LocalJobRunner: Finishing task: attempt_local1098479873_0001_m_000008_0
2021-12-17 21:53:07,755 INFO mapred.LocalJobRunner: Starting task: attempt_local1098479873_0001_m_000009_0
2021-12-17 21:53:07,757 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-17 21:53:07,757 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-17 21:53:07,758 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-17 21:53:07,761 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/vagrant/QuasiMonteCarlo_1639777974723_256190444/in/part3:0+118
2021-12-17 21:53:08,004 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-17 21:53:08,004 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-17 21:53:08,005 INFO mapred.MapTask: soft limit at 83886080
2021-12-17 21:53:08,006 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-17 21:53:08,007 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-17 21:53:08,013 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-17 21:53:08,033 INFO mapred.LocalJobRunner:
2021-12-17 21:53:08,034 INFO mapred.MapTask: Starting flush of map output
2021-12-17 21:53:08,034 INFO mapred.MapTask: Spilling map output
2021-12-17 21:53:08,034 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-17 21:53:08,035 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-17 21:53:08,038 INFO mapred.MapTask: Finished spill 0
2021-12-17 21:53:08,049 INFO mapred.Task: Task:attempt_local1098479873_0001_m_000009_0 is done. And is in the process of committing
2021-12-17 21:53:08,061 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-17 21:53:08,061 INFO mapred.Task: Task 'attempt_local1098479873_0001_m_000009_0' done.
2021-12-17 21:53:08,063 INFO mapred.Task: Final Counters for attempt_local1098479873_0001_m_000009_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=335859
                FILE: Number of bytes written=846958
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1180
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=34
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=205
                Total committed heap usage (bytes)=212336640
        File Input Format Counters
                Bytes Read=118
2021-12-17 21:53:08,067 INFO mapred.LocalJobRunner: Finishing task: attempt_local1098479873_0001_m_000009_0
2021-12-17 21:53:08,070 INFO mapred.LocalJobRunner: Starting task: attempt_local1098479873_0001_m_000010_0
2021-12-17 21:53:08,079 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-17 21:53:08,079 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-17 21:53:08,081 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-17 21:53:08,084 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/vagrant/QuasiMonteCarlo_1639777974723_256190444/in/part4:0+118
2021-12-17 21:53:08,182 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-17 21:53:08,183 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-17 21:53:08,183 INFO mapred.MapTask: soft limit at 83886080
2021-12-17 21:53:08,184 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-17 21:53:08,184 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-17 21:53:08,185 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-17 21:53:08,205 INFO mapred.LocalJobRunner:
2021-12-17 21:53:08,206 INFO mapred.MapTask: Starting flush of map output
2021-12-17 21:53:08,207 INFO mapred.MapTask: Spilling map output
2021-12-17 21:53:08,207 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-17 21:53:08,207 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-17 21:53:08,210 INFO mapred.MapTask: Finished spill 0
2021-12-17 21:53:08,215 INFO mapred.Task: Task:attempt_local1098479873_0001_m_000010_0 is done. And is in the process of committing
2021-12-17 21:53:08,224 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-17 21:53:08,226 INFO mapred.Task: Task 'attempt_local1098479873_0001_m_000010_0' done.
2021-12-17 21:53:08,228 INFO mapred.Task: Final Counters for attempt_local1098479873_0001_m_000010_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=337196
                FILE: Number of bytes written=847018
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1298
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=37
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=317718528
        File Input Format Counters
                Bytes Read=118
2021-12-17 21:53:08,230 INFO mapred.LocalJobRunner: Finishing task: attempt_local1098479873_0001_m_000010_0
2021-12-17 21:53:08,231 INFO mapred.LocalJobRunner: Starting task: attempt_local1098479873_0001_m_000011_0
2021-12-17 21:53:08,247 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-17 21:53:08,247 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-17 21:53:08,248 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-17 21:53:08,252 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/vagrant/QuasiMonteCarlo_1639777974723_256190444/in/part5:0+118
2021-12-17 21:53:08,363 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-17 21:53:08,364 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-17 21:53:08,364 INFO mapred.MapTask: soft limit at 83886080
2021-12-17 21:53:08,364 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-17 21:53:08,364 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-17 21:53:08,365 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-17 21:53:08,380 INFO mapred.LocalJobRunner:
2021-12-17 21:53:08,380 INFO mapred.MapTask: Starting flush of map output
2021-12-17 21:53:08,380 INFO mapred.MapTask: Spilling map output
2021-12-17 21:53:08,380 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-17 21:53:08,380 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-17 21:53:08,385 INFO mapred.MapTask: Finished spill 0
2021-12-17 21:53:08,389 INFO mapred.Task: Task:attempt_local1098479873_0001_m_000011_0 is done. And is in the process of committing
2021-12-17 21:53:08,394 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-17 21:53:08,394 INFO mapred.Task: Task 'attempt_local1098479873_0001_m_000011_0' done.
2021-12-17 21:53:08,395 INFO mapred.Task: Final Counters for attempt_local1098479873_0001_m_000011_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=338021
                FILE: Number of bytes written=847078
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1416
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=40
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=423100416
        File Input Format Counters
                Bytes Read=118
2021-12-17 21:53:08,396 INFO mapred.LocalJobRunner: Finishing task: attempt_local1098479873_0001_m_000011_0
2021-12-17 21:53:08,397 INFO mapred.LocalJobRunner: Starting task: attempt_local1098479873_0001_m_000012_0
2021-12-17 21:53:08,401 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-17 21:53:08,402 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-17 21:53:08,403 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-17 21:53:08,406 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/vagrant/QuasiMonteCarlo_1639777974723_256190444/in/part6:0+118
2021-12-17 21:53:08,519 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-17 21:53:08,520 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-17 21:53:08,520 INFO mapred.MapTask: soft limit at 83886080
2021-12-17 21:53:08,521 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-17 21:53:08,522 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-17 21:53:08,523 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-17 21:53:08,540 INFO mapred.LocalJobRunner:
2021-12-17 21:53:08,540 INFO mapred.MapTask: Starting flush of map output
2021-12-17 21:53:08,541 INFO mapred.MapTask: Spilling map output
2021-12-17 21:53:08,542 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-17 21:53:08,542 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-17 21:53:08,546 INFO mapred.MapTask: Finished spill 0
2021-12-17 21:53:08,550 INFO mapred.Task: Task:attempt_local1098479873_0001_m_000012_0 is done. And is in the process of committing
2021-12-17 21:53:08,564 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-17 21:53:08,566 INFO mapred.Task: Task 'attempt_local1098479873_0001_m_000012_0' done.
2021-12-17 21:53:08,568 INFO mapred.Task: Final Counters for attempt_local1098479873_0001_m_000012_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=338846
                FILE: Number of bytes written=847138
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1534
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=43
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=514326528
        File Input Format Counters
                Bytes Read=118
2021-12-17 21:53:08,581 INFO mapred.LocalJobRunner: Finishing task: attempt_local1098479873_0001_m_000012_0
2021-12-17 21:53:08,581 INFO mapred.LocalJobRunner: Starting task: attempt_local1098479873_0001_m_000013_0
2021-12-17 21:53:08,592 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-17 21:53:08,592 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-17 21:53:08,594 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-17 21:53:08,601 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/vagrant/QuasiMonteCarlo_1639777974723_256190444/in/part7:0+118
2021-12-17 21:53:08,772 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-17 21:53:08,773 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-17 21:53:08,773 INFO mapred.MapTask: soft limit at 83886080
2021-12-17 21:53:08,774 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-17 21:53:08,774 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-17 21:53:08,776 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-17 21:53:08,790 INFO mapred.LocalJobRunner:
2021-12-17 21:53:08,791 INFO mapred.MapTask: Starting flush of map output
2021-12-17 21:53:08,792 INFO mapred.MapTask: Spilling map output
2021-12-17 21:53:08,792 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-17 21:53:08,793 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-17 21:53:08,795 INFO mapred.MapTask: Finished spill 0
2021-12-17 21:53:08,800 INFO mapred.Task: Task:attempt_local1098479873_0001_m_000013_0 is done. And is in the process of committing
2021-12-17 21:53:08,809 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-17 21:53:08,809 INFO mapred.Task: Task 'attempt_local1098479873_0001_m_000013_0' done.
2021-12-17 21:53:08,810 INFO mapred.Task: Final Counters for attempt_local1098479873_0001_m_000013_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=339671
                FILE: Number of bytes written=847198
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1652
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=46
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=136
                Total committed heap usage (bytes)=218628096
        File Input Format Counters
                Bytes Read=118
2021-12-17 21:53:08,811 INFO mapred.LocalJobRunner: Finishing task: attempt_local1098479873_0001_m_000013_0
2021-12-17 21:53:08,811 INFO mapred.LocalJobRunner: Starting task: attempt_local1098479873_0001_m_000014_0
2021-12-17 21:53:08,814 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-17 21:53:08,814 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-17 21:53:08,815 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-17 21:53:08,818 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/vagrant/QuasiMonteCarlo_1639777974723_256190444/in/part8:0+118
2021-12-17 21:53:08,907 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-17 21:53:08,911 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-17 21:53:08,911 INFO mapred.MapTask: soft limit at 83886080
2021-12-17 21:53:08,911 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-17 21:53:08,911 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-17 21:53:08,912 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-17 21:53:08,934 INFO mapred.LocalJobRunner:
2021-12-17 21:53:08,935 INFO mapred.MapTask: Starting flush of map output
2021-12-17 21:53:08,935 INFO mapred.MapTask: Spilling map output
2021-12-17 21:53:08,935 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-17 21:53:08,935 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-17 21:53:08,938 INFO mapred.MapTask: Finished spill 0
2021-12-17 21:53:08,942 INFO mapred.Task: Task:attempt_local1098479873_0001_m_000014_0 is done. And is in the process of committing
2021-12-17 21:53:08,949 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-17 21:53:08,949 INFO mapred.Task: Task 'attempt_local1098479873_0001_m_000014_0' done.
2021-12-17 21:53:08,950 INFO mapred.Task: Final Counters for attempt_local1098479873_0001_m_000014_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=340496
                FILE: Number of bytes written=847258
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1770
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=49
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=324009984
        File Input Format Counters
                Bytes Read=118
2021-12-17 21:53:08,958 INFO mapred.LocalJobRunner: Finishing task: attempt_local1098479873_0001_m_000014_0
2021-12-17 21:53:08,959 INFO mapred.LocalJobRunner: Starting task: attempt_local1098479873_0001_m_000015_0
2021-12-17 21:53:08,963 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-17 21:53:08,963 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-17 21:53:08,964 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-17 21:53:08,976 INFO mapred.MapTask: Processing split: hdfs://node01:9000/user/vagrant/QuasiMonteCarlo_1639777974723_256190444/in/part9:0+118
2021-12-17 21:53:09,096 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2021-12-17 21:53:09,096 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2021-12-17 21:53:09,096 INFO mapred.MapTask: soft limit at 83886080
2021-12-17 21:53:09,096 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2021-12-17 21:53:09,096 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2021-12-17 21:53:09,097 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2021-12-17 21:53:09,113 INFO mapred.LocalJobRunner:
2021-12-17 21:53:09,114 INFO mapred.MapTask: Starting flush of map output
2021-12-17 21:53:09,115 INFO mapred.MapTask: Spilling map output
2021-12-17 21:53:09,116 INFO mapred.MapTask: bufstart = 0; bufend = 18; bufvoid = 104857600
2021-12-17 21:53:09,116 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2021-12-17 21:53:09,119 INFO mapred.MapTask: Finished spill 0
2021-12-17 21:53:09,124 INFO mapred.Task: Task:attempt_local1098479873_0001_m_000015_0 is done. And is in the process of committing
2021-12-17 21:53:09,131 INFO mapred.LocalJobRunner: Generated 1000 samples.
2021-12-17 21:53:09,132 INFO mapred.Task: Task 'attempt_local1098479873_0001_m_000015_0' done.
2021-12-17 21:53:09,134 INFO mapred.Task: Final Counters for attempt_local1098479873_0001_m_000015_0: Counters: 23
        File System Counters
                FILE: Number of bytes read=340809
                FILE: Number of bytes written=847318
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1888
                HDFS: Number of bytes written=1888
                HDFS: Number of read operations=52
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=18
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=18
                Map output materialized bytes=28
                Input split bytes=145
                Combine input records=0
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=429391872
        File Input Format Counters
                Bytes Read=118
2021-12-17 21:53:09,144 INFO mapred.LocalJobRunner: Finishing task: attempt_local1098479873_0001_m_000015_0
2021-12-17 21:53:09,145 INFO mapred.LocalJobRunner: map task executor complete.
2021-12-17 21:53:09,182 INFO mapred.LocalJobRunner: Waiting for reduce tasks
2021-12-17 21:53:09,183 INFO mapred.LocalJobRunner: Starting task: attempt_local1098479873_0001_r_000000_0
2021-12-17 21:53:09,205 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2021-12-17 21:53:09,206 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2021-12-17 21:53:09,207 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2021-12-17 21:53:09,218 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7ede9ec1
2021-12-17 21:53:09,223 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2021-12-17 21:53:09,292 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=325163424, maxSingleShuffleLimit=81290856, mergeThreshold=214607872, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2021-12-17 21:53:09,299 INFO reduce.EventFetcher: attempt_local1098479873_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2021-12-17 21:53:09,361 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1098479873_0001_m_000013_0 decomp: 24 len: 28 to MEMORY
2021-12-17 21:53:09,383 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1098479873_0001_m_000013_0
2021-12-17 21:53:09,383 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->24
2021-12-17 21:53:09,392 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1098479873_0001_m_000000_0 decomp: 24 len: 28 to MEMORY
2021-12-17 21:53:09,402 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1098479873_0001_m_000000_0
2021-12-17 21:53:09,404 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 2, commitMemory -> 24, usedMemory ->48
2021-12-17 21:53:09,408 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1098479873_0001_m_000006_0 decomp: 24 len: 28 to MEMORY
2021-12-17 21:53:09,415 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1098479873_0001_m_000006_0
2021-12-17 21:53:09,416 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 3, commitMemory -> 48, usedMemory ->72
2021-12-17 21:53:09,421 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1098479873_0001_m_000012_0 decomp: 24 len: 28 to MEMORY
2021-12-17 21:53:09,426 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1098479873_0001_m_000012_0
2021-12-17 21:53:09,427 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 4, commitMemory -> 72, usedMemory ->96
2021-12-17 21:53:09,430 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1098479873_0001_m_000005_0 decomp: 24 len: 28 to MEMORY
2021-12-17 21:53:09,432 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1098479873_0001_m_000005_0
2021-12-17 21:53:09,433 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 5, commitMemory -> 96, usedMemory ->120
2021-12-17 21:53:09,436 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1098479873_0001_m_000011_0 decomp: 24 len: 28 to MEMORY
2021-12-17 21:53:09,439 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1098479873_0001_m_000011_0
2021-12-17 21:53:09,439 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 6, commitMemory -> 120, usedMemory ->144
2021-12-17 21:53:09,443 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1098479873_0001_m_000004_0 decomp: 24 len: 28 to MEMORY
2021-12-17 21:53:09,444 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1098479873_0001_m_000004_0
2021-12-17 21:53:09,445 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 7, commitMemory -> 144, usedMemory ->168
2021-12-17 21:53:09,449 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1098479873_0001_m_000010_0 decomp: 24 len: 28 to MEMORY
2021-12-17 21:53:09,451 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1098479873_0001_m_000010_0
2021-12-17 21:53:09,452 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 8, commitMemory -> 168, usedMemory ->192
2021-12-17 21:53:09,455 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1098479873_0001_m_000003_0 decomp: 24 len: 28 to MEMORY
2021-12-17 21:53:09,458 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1098479873_0001_m_000003_0
2021-12-17 21:53:09,459 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 9, commitMemory -> 192, usedMemory ->216
2021-12-17 21:53:09,462 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1098479873_0001_m_000009_0 decomp: 24 len: 28 to MEMORY
2021-12-17 21:53:09,464 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1098479873_0001_m_000009_0
2021-12-17 21:53:09,465 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 10, commitMemory -> 216, usedMemory ->240
2021-12-17 21:53:09,469 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1098479873_0001_m_000015_0 decomp: 24 len: 28 to MEMORY
2021-12-17 21:53:09,471 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1098479873_0001_m_000015_0
2021-12-17 21:53:09,472 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 11, commitMemory -> 240, usedMemory ->264
2021-12-17 21:53:09,476 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1098479873_0001_m_000002_0 decomp: 24 len: 28 to MEMORY
2021-12-17 21:53:09,480 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1098479873_0001_m_000002_0
2021-12-17 21:53:09,481 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 12, commitMemory -> 264, usedMemory ->288
2021-12-17 21:53:09,484 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1098479873_0001_m_000008_0 decomp: 24 len: 28 to MEMORY
2021-12-17 21:53:09,486 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1098479873_0001_m_000008_0
2021-12-17 21:53:09,486 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 13, commitMemory -> 288, usedMemory ->312
2021-12-17 21:53:09,490 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1098479873_0001_m_000014_0 decomp: 24 len: 28 to MEMORY
2021-12-17 21:53:09,492 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1098479873_0001_m_000014_0
2021-12-17 21:53:09,492 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 14, commitMemory -> 312, usedMemory ->336
2021-12-17 21:53:09,496 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1098479873_0001_m_000001_0 decomp: 24 len: 28 to MEMORY
2021-12-17 21:53:09,498 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1098479873_0001_m_000001_0
2021-12-17 21:53:09,499 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 15, commitMemory -> 336, usedMemory ->360
2021-12-17 21:53:09,502 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1098479873_0001_m_000007_0 decomp: 24 len: 28 to MEMORY
2021-12-17 21:53:09,504 INFO reduce.InMemoryMapOutput: Read 24 bytes from map-output for attempt_local1098479873_0001_m_000007_0
2021-12-17 21:53:09,504 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24, inMemoryMapOutputs.size() -> 16, commitMemory -> 360, usedMemory ->384
2021-12-17 21:53:09,505 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning
2021-12-17 21:53:09,507 INFO mapred.LocalJobRunner: 16 / 16 copied.
2021-12-17 21:53:09,508 INFO reduce.MergeManagerImpl: finalMerge called with 16 in-memory map-outputs and 0 on-disk map-outputs
2021-12-17 21:53:09,525 INFO mapred.Merger: Merging 16 sorted segments
2021-12-17 21:53:09,527 INFO mapred.Merger: Down to the last merge-pass, with 16 segments left of total size: 336 bytes
2021-12-17 21:53:09,535 INFO reduce.MergeManagerImpl: Merged 16 segments, 384 bytes to disk to satisfy reduce memory limit
2021-12-17 21:53:09,548 INFO reduce.MergeManagerImpl: Merging 1 files, 358 bytes from disk
2021-12-17 21:53:09,551 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
2021-12-17 21:53:09,551 INFO mapred.Merger: Merging 1 sorted segments
2021-12-17 21:53:09,553 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 351 bytes
2021-12-17 21:53:09,555 INFO mapred.LocalJobRunner: 16 / 16 copied.
2021-12-17 21:53:09,691 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2021-12-17 21:53:09,742 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2021-12-17 21:53:09,798 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2021-12-17 21:53:09,849 INFO mapred.Task: Task:attempt_local1098479873_0001_r_000000_0 is done. And is in the process of committing
2021-12-17 21:53:09,859 INFO mapred.LocalJobRunner: 16 / 16 copied.
2021-12-17 21:53:09,860 INFO mapred.Task: Task attempt_local1098479873_0001_r_000000_0 is allowed to commit now
2021-12-17 21:53:09,937 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1098479873_0001_r_000000_0' to hdfs://node01:9000/user/vagrant/QuasiMonteCarlo_1639777974723_256190444/out
2021-12-17 21:53:09,939 INFO mapred.LocalJobRunner: reduce > reduce
2021-12-17 21:53:09,940 INFO mapred.Task: Task 'attempt_local1098479873_0001_r_000000_0' done.
2021-12-17 21:53:09,941 INFO mapred.Task: Final Counters for attempt_local1098479873_0001_r_000000_0: Counters: 30
        File System Counters
                FILE: Number of bytes read=342127
                FILE: Number of bytes written=847676
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1888
                HDFS: Number of bytes written=2103
                HDFS: Number of read operations=57
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=21
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Combine input records=0
                Combine output records=0
                Reduce input groups=2
                Reduce shuffle bytes=448
                Reduce input records=32
                Reduce output records=0
                Spilled Records=32
                Shuffled Maps =16
                Failed Shuffles=0
                Merged Map outputs=16
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=429391872
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Output Format Counters
                Bytes Written=97
2021-12-17 21:53:09,955 INFO mapred.LocalJobRunner: Finishing task: attempt_local1098479873_0001_r_000000_0
2021-12-17 21:53:09,956 INFO mapred.LocalJobRunner: reduce task executor complete.
2021-12-17 21:53:09,974 INFO mapreduce.Job:  map 100% reduce 100%
2021-12-17 21:53:10,976 INFO mapreduce.Job: Job job_local1098479873_0001 completed successfully
2021-12-17 21:53:11,040 INFO mapreduce.Job: Counters: 36
        File System Counters
                FILE: Number of bytes read=5661767
                FILE: Number of bytes written=14397564
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=17936
                HDFS: Number of bytes written=32311
                HDFS: Number of read operations=529
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=309
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=16
                Map output records=32
                Map output bytes=288
                Map output materialized bytes=448
                Input split bytes=2326
                Combine input records=0
                Combine output records=0
                Reduce input groups=2
                Reduce shuffle bytes=448
                Reduce input records=32
                Reduce output records=0
                Spilled Records=64
                Shuffled Maps =16
                Failed Shuffles=0
                Merged Map outputs=16
                GC time elapsed (ms)=604
                Total committed heap usage (bytes)=6507462656
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=1888
        File Output Format Counters
                Bytes Written=97
Job Finished in 9.057 seconds
Estimated value of Pi is 3.14250000000000000000
vagrant@node01:~$
vagrant@node01:~$
vagrant@node01:~$
vagrant@node01:~$
vagrant@node01:~$ hdfs dfs -mkdir -p /tmp
vagrant@node01:~$ hdfs dfs -chmod g+w /tmp
vagrant@node01:~$ hdfs dfs -mkdir -p /user/hive/warehouse
vagrant@node01:~$ hdfs dfs -chmod g+w /user/hive/warehouse
vagrant@node01:~$ cd /usr/local/hive/scripts/metastore/upgrade/derby
vagrant@node01:/usr/local/hive/scripts/metastore/upgrade/derby$ sudo nano hive-schema-3.1.0.derby.sql
vagrant@node01:/usr/local/hive/scripts/metastore/upgrade/derby$
vagrant@node01:/usr/local/hive/scripts/metastore/upgrade/derby$
vagrant@node01:/usr/local/hive/scripts/metastore/upgrade/derby$ cd /usr/local/hive/bin
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$
vagrant@node01:/usr/local/hive/bin$ schematool -initSchema -dbType derby
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Metastore connection URL:        jdbc:derby:;databaseName=metastore_db;create=true
Metastore Connection Driver :    org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User:       APP
Starting metastore schema initialization to 3.1.0
Initialization script hive-schema-3.1.0.derby.sql































































































































































































































Initialization script completed
schemaTool completed
vagrant@node01:/usr/local/hive/bin$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = cf5f45b2-7f4c-482b-a56f-e6625463605f

Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = 7be83903-db7a-44fb-acff-6196b98162dc
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive>
    >
    >
    > CREATE TABLE master_data_science (id int, name string, address string)
    > CLUSTERED BY(id) INTO 3 BUCKETS
    > STORED AS ORC
    > TBLPROPERTIES ('transactional'='true');
OK
Time taken: 6.067 seconds
hive> insert into master_data_science(id, name, address) values (1, 'Derry', 'Ireland');
Query ID = vagrant_20211217220235_31e23716-4066-44b3-bbca-fea56647776a
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:02:48,052 Stage-1 map = 100%,  reduce = 0%
2021-12-17 22:02:49,087 Stage-1 map = 100%,  reduce = 33%
2021-12-17 22:02:50,127 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local967713608_0001
Loading data to table default.master_data_science
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:02:53,238 Stage-3 map = 0%,  reduce = 0%
2021-12-17 22:02:54,324 Stage-3 map = 100%,  reduce = 0%
2021-12-17 22:02:55,339 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_local1157731631_0002
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 0 HDFS Write: 2018 SUCCESS
Stage-Stage-3:  HDFS Read: 0 HDFS Write: 2018 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 20.846 seconds
hive> insert into master_data_science(id, name, address) values (2, 'Luke', 'Ireland');
Query ID = vagrant_20211217220256_15f96b0b-4c13-43ba-9030-f886ada0ce8d
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:02:59,342 Stage-1 map = 100%,  reduce = 67%
2021-12-17 22:03:00,378 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local240419883_0003
Loading data to table default.master_data_science
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:03:02,476 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_local1544464471_0004
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 756 HDFS Write: 5199 SUCCESS
Stage-Stage-3:  HDFS Read: 378 HDFS Write: 4026 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 6.585 seconds
hive> insert into master_data_science(id, name, address) values (3, 'Tan', 'Vietnam');
Query ID = vagrant_20211217220302_2d5e5fba-12ed-4ba8-8dc2-ec67e2cce735
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:03:05,902 Stage-1 map = 100%,  reduce = 67%
2021-12-17 22:03:06,914 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local623044311_0005
Loading data to table default.master_data_science
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:03:09,231 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_local463261782_0006
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 1512 HDFS Write: 9208 SUCCESS
Stage-Stage-3:  HDFS Read: 756 HDFS Write: 6020 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 6.589 seconds
hive> insert into master_data_science(id, name, address) values (4, 'Li', 'China');
Query ID = vagrant_20211217220309_aa59120b-54c3-4e72-b7ff-05e619faca38
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:03:12,498 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1109023103_0007
Loading data to table default.master_data_science
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:03:14,835 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_local1731220111_0008
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 2268 HDFS Write: 13184 SUCCESS
Stage-Stage-3:  HDFS Read: 1134 HDFS Write: 7990 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 5.564 seconds
hive> insert into master_data_science(id, name, address) values (5, 'Omar', 'Ireland');
Query ID = vagrant_20211217220315_067ebe4b-12ae-4056-9a47-fc70bb4f7e9d
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:03:18,081 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local417729014_0009
Loading data to table default.master_data_science
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:03:20,418 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_local934112238_0010
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 3024 HDFS Write: 17990 SUCCESS
Stage-Stage-3:  HDFS Read: 1512 HDFS Write: 10000 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 5.611 seconds
hive> insert into master_data_science(id, name, address) values (6, 'Raymonds', 'India');
Query ID = vagrant_20211217220320_801b865c-55c1-4168-8998-8acb6da63035
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:03:23,708 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1521505717_0011
Loading data to table default.master_data_science
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:03:25,988 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_local1187889419_0012
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 3780 HDFS Write: 21170 SUCCESS
Stage-Stage-3:  HDFS Read: 1890 HDFS Write: 12022 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 5.406 seconds
hive> insert into master_data_science(id, name, address) values (7, 'Karina', 'Ireland');
Query ID = vagrant_20211217220326_36537131-15ef-4aba-b620-c3b6e46fc756
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:03:28,962 Stage-1 map = 100%,  reduce = 67%
2021-12-17 22:03:29,974 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local121237610_0013
Loading data to table default.master_data_science
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:03:32,578 Stage-3 map = 0%,  reduce = 0%
2021-12-17 22:03:33,589 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_local80515090_0014
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 4536 HDFS Write: 26891 SUCCESS
Stage-Stage-3:  HDFS Read: 2268 HDFS Write: 14026 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 7.574 seconds
hive> insert into master_data_science(id, name, address) values (8, 'Nikkil', 'India');
Query ID = vagrant_20211217220333_e63b1328-49e5-40e1-a6b9-a3c386938398
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:03:36,726 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1899843400_0015
Loading data to table default.master_data_science
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:03:39,112 Stage-3 map = 0%,  reduce = 0%
2021-12-17 22:03:40,129 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_local173434760_0016
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 5292 HDFS Write: 30050 SUCCESS
Stage-Stage-3:  HDFS Read: 2646 HDFS Write: 16024 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 6.521 seconds
hive> insert into master_data_science(id, name, address) values (9, 'Abilash', 'India');
Query ID = vagrant_20211217220340_ea85f084-8c75-496d-a94e-2d9c32ebcf43
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:03:43,199 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1753032776_0017
Loading data to table default.master_data_science
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:03:45,056 Stage-3 map = 0%,  reduce = 0%
2021-12-17 22:03:46,066 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_local1133915161_0018
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 6048 HDFS Write: 34898 SUCCESS
Stage-Stage-3:  HDFS Read: 3024 HDFS Write: 18030 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 5.9 seconds
hive> insert into master_data_science(id, name, address) values (10, 'Taoqi', 'China');
Query ID = vagrant_20211217220346_ac4d777d-5120-4f1e-a218-e370fd32f818
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:03:49,286 Stage-1 map = 0%,  reduce = 0%
2021-12-17 22:03:50,303 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local737785251_0019
Loading data to table default.master_data_science
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:03:52,630 Stage-3 map = 0%,  reduce = 0%
Ended Job = job_local336947977_0020 with errors
Error during job, obtaining debugging information...
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 6804 HDFS Write: 38030 SUCCESS
Stage-Stage-3:  HDFS Read: 0 HDFS Write: 0 FAIL
Total MapReduce CPU Time Spent: 0 msec
hive> insert into master_data_science(id, name, address) values (11, 'Ilia', 'Russia');
Query ID = vagrant_20211217220352_1cdd1a13-46a6-4150-aeba-cb61fcaed12e
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:03:55,133 Stage-1 map = 0%,  reduce = 0%
Ended Job = job_local170458412_0021 with errors
Error during job, obtaining debugging information...
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 FAIL
Total MapReduce CPU Time Spent: 0 msec
hive> insert into master_data_science(id, name, address) values (12, 'Marcelino', 'India');
Query ID = vagrant_20211217220401_7202795b-40d7-4d38-ab5c-c8b9761d6d77
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:04:04,465 Stage-1 map = 0%,  reduce = 0%
Ended Job = job_local1418915_0022 with errors
Error during job, obtaining debugging information...
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 FAIL
Total MapReduce CPU Time Spent: 0 msec
hive>
    > select * from master_data_science;
OK
1       Derry   Ireland
2       Luke    Ireland
3       Tan     Vietnam
4       Li      China
5       Omar    Ireland
6       Raymonds        India
7       Karina  Ireland
8       Nikkil  India
9       Abilash India
Time taken: 1.006 seconds, Fetched: 9 row(s)
hive> update master_data_science set address = 'Asia' where id in (3,4,6,8,9);
Query ID = vagrant_20211217220909_a0dcd0b6-c9a9-4e0e-ab95-ce2bd154f057
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:09:12,818 Stage-1 map = 0%,  reduce = 0%
Ended Job = job_local2140011548_0023 with errors
Error during job, obtaining debugging information...
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 17016 HDFS Write: 10000 FAIL
Total MapReduce CPU Time Spent: 0 msec
hive> update master_data_science set address = 'Asia' where id=3;
Query ID = vagrant_20211217220942_e81ef591-3bf8-45cd-b1a7-81a2b311b2e5
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:09:44,798 Stage-1 map = 0%,  reduce = 0%
Ended Job = job_local1164954064_0024 with errors
Error during job, obtaining debugging information...
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 FAIL
Total MapReduce CPU Time Spent: 0 msec
hive> select * from master_data_science;
OK
1       Derry   Ireland
2       Luke    Ireland
3       Tan     Vietnam
4       Li      China
5       Omar    Ireland
6       Raymonds        India
7       Karina  Ireland
8       Nikkil  India
9       Abilash India
Time taken: 0.62 seconds, Fetched: 9 row(s)
hive> delete from master_data_science where id = 9;
Query ID = vagrant_20211217221514_c65727b2-a63e-4ff6-9a19-b5cd6c2d2dd3
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:15:17,326 Stage-1 map = 0%,  reduce = 0%
Ended Job = job_local262241769_0025 with errors
Error during job, obtaining debugging information...
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 FAIL
Total MapReduce CPU Time Spent: 0 msec
hive> SET hive.exec.max.dynamic.partitions = 10000;
hive> SET hive.exec.max.dynamic.partitions.pernode = 1000;
hive> update master_data_science set address = 'Asia' where id in (3,4,6,8,9);
Query ID = vagrant_20211217221640_04aecfcf-81a6-4eef-8a94-fcb193a42550
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:16:43,599 Stage-1 map = 0%,  reduce = 0%
Ended Job = job_local1842726848_0026 with errors
Error during job, obtaining debugging information...
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 52985 HDFS Write: 10000 FAIL
Total MapReduce CPU Time Spent: 0 msec
hive> SET hive.exec.dynamic.partition = true;
hive> SET hive.exec.dynamic.partition.mode = nonstrict;
hive> SET hive.exec.max.dynamic.partitions = 10000;
hive> SET hive.exec.max.dynamic.partitions.pernode = 1000;
hive> update master_data_science set address = 'Asia' where id in (3,4,6,8,9);
Query ID = vagrant_20211217221713_845dd8c1-736e-48b9-80ae-f19b1b76f328
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:17:16,242 Stage-1 map = 0%,  reduce = 0%
Ended Job = job_local1875318665_0027 with errors
Error during job, obtaining debugging information...
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 58697 HDFS Write: 10000 FAIL
Total MapReduce CPU Time Spent: 0 msec
hive> drop table master_data_science;
OK
Time taken: 1.765 seconds
hive> CREATE TABLE master_data_science (id int, name string, address string)
    > STORED AS ORC
    > TBLPROPERTIES ('transactional'='true');
OK
Time taken: 0.407 seconds
hive> insert into master_data_science(id, name, address) values (1, 'Derry', 'Ireland');
Query ID = vagrant_20211217222410_a91f3c0b-f5d1-4024-b371-058994284b61
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 22:24:13,797 Stage-1 map = 0%,  reduce = 0%
Ended Job = job_local12941207_0028 with errors
Error during job, obtaining debugging information...
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 FAIL
Total MapReduce CPU Time Spent: 0 msec
hive>
    > show tables;
OK
master_data_science
Time taken: 0.091 seconds, Fetched: 1 row(s)
hive>
    > quit;
vagrant@node01:/usr/local/hive/bin$ cd $HADOOP_HOME
vagrant@node01:/usr/local/hadoop$ ls
LICENSE.txt  README.txt  data  include  libexec  sbin
NOTICE.txt   bin         etc   lib      logs     share
vagrant@node01:/usr/local/hadoop$ cd logs
vagrant@node01:/usr/local/hadoop/logs$ ls
SecurityAuth-vagrant.audit
hadoop-vagrant-namenode-node01.log
hadoop-vagrant-namenode-node01.out
hadoop-vagrant-resourcemanager-node01.log
hadoop-vagrant-resourcemanager-node01.out
hadoop-vagrant-secondarynamenode-node01.log
hadoop-vagrant-secondarynamenode-node01.out
vagrant@node01:/usr/local/hadoop/logs$
vagrant@node01:/usr/local/hadoop/logs$ cd ..
vagrant@node01:/usr/local/hadoop$ ls
LICENSE.txt  README.txt  data  include  libexec  sbin
NOTICE.txt   bin         etc   lib      logs     share
vagrant@node01:/usr/local/hadoop$ cd etc/
vagrant@node01:/usr/local/hadoop/etc$ cd hadoop/
vagrant@node01:/usr/local/hadoop/etc/hadoop$ ls
capacity-scheduler.xml            kms-log4j.properties
configuration.xsl                 kms-site.xml
container-executor.cfg            log4j.properties
core-site.xml                     mapred-env.cmd
hadoop-env.cmd                    mapred-env.sh
hadoop-env.sh                     mapred-queues.xml.template
hadoop-metrics2.properties        mapred-site.xml
hadoop-policy.xml                 shellprofile.d
hadoop-user-functions.sh.example  ssl-client.xml.example
hdfs-site.xml                     ssl-server.xml.example
httpfs-env.sh                     user_ec_policies.xml.template
httpfs-log4j.properties           workers
httpfs-signature.secret           yarn-env.cmd
httpfs-site.xml                   yarn-env.sh
kms-acls.xml                      yarn-site.xml
kms-env.sh                        yarnservice-log4j.properties
vagrant@node01:/usr/local/hadoop/etc/hadoop$ more mapred-site.xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>

</configuration>
vagrant@node01:/usr/local/hadoop/etc/hadoop$ more yarn-site.xml
<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>node01</value>
  </property>
</configuration>
vagrant@node01:/usr/local/hadoop/etc/hadoop$ ls
capacity-scheduler.xml            kms-log4j.properties
configuration.xsl                 kms-site.xml
container-executor.cfg            log4j.properties
core-site.xml                     mapred-env.cmd
hadoop-env.cmd                    mapred-env.sh
hadoop-env.sh                     mapred-queues.xml.template
hadoop-metrics2.properties        mapred-site.xml
hadoop-policy.xml                 shellprofile.d
hadoop-user-functions.sh.example  ssl-client.xml.example
hdfs-site.xml                     ssl-server.xml.example
httpfs-env.sh                     user_ec_policies.xml.template
httpfs-log4j.properties           workers
httpfs-signature.secret           yarn-env.cmd
httpfs-site.xml                   yarn-env.sh
kms-acls.xml                      yarn-site.xml
kms-env.sh                        yarnservice-log4j.properties
vagrant@node01:/usr/local/hadoop/etc/hadoop$ sudo cp -f /app/code/yarn-site.xml /usr/local/hadoop/etc/hadoop/yarn-site.xml
vagrant@node01:/usr/local/hadoop/etc/hadoop$ sudo cp -f /app/code/mapred-site.xml /usr/local/hadoop/etc/hadoop/mapred-site.xml
cp: cannot stat '/app/code/mapred-site.xml': No such file or directory
vagrant@node01:/usr/local/hadoop/etc/hadoop$ sudo cp -f /app/code/mapred-site.xml /usr/local/hadoop/etc/hadoop/mapred-site.xml
vagrant@node01:/usr/local/hadoop/etc/hadoop$
vagrant@node01:/usr/local/hadoop/etc/hadoop$
vagrant@node01:/usr/local/hadoop/etc/hadoop$
vagrant@node01:/usr/local/hadoop/etc/hadoop$ cd $HIVE_HOME
vagrant@node01:/usr/local/hive$ cd bin
vagrant@node01:/usr/local/hive/bin$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = cf757015-bc24-4524-a3cc-f2266508a92b

Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = 042436c5-0a6e-4da1-8e14-e4ddb02b8b6f
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive> show databases;
OK
default
Time taken: 2.963 seconds, Fetched: 1 row(s)
hive> show tables;
OK
master_data_science
Time taken: 0.112 seconds, Fetched: 1 row(s)
hive>
    > drop table master_data_science;
OK
Time taken: 4.48 seconds
hive>
    >
    > CREATE TABLE master_data_science (id int, name string, address string)
    > STORED AS ORC
    > TBLPROPERTIES ('transactional'='true');
OK
Time taken: 1.886 seconds
hive> insert into master_data_science(id, name, address) values (1, 'Derry', 'Ireland');
Query ID = vagrant_20211217230450_304f4303-54c3-4702-9788-955b7cdd0230
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 23:05:02,158 Stage-1 map = 0%,  reduce = 0%
2021-12-17 23:05:04,378 Stage-1 map = 100%,  reduce = 0%
2021-12-17 23:05:05,401 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local442712346_0001
Loading data to table default.master_data_science
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 0 HDFS Write: 1794 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 15.934 seconds
hive> update master_data_science set address = 'Republic of Ireland' where id = 1;
Query ID = vagrant_20211217230534_61019d4d-12d4-47a4-a977-a6b2f95416f6
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 23:05:37,324 Stage-1 map = 0%,  reduce = 0%
2021-12-17 23:05:38,342 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local656535502_0002
Loading data to table default.master_data_science
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 3690 HDFS Write: 3483 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 4.71 seconds
hive> select * from master_data_science;
OK
1       Derry   Republic of Ireland
Time taken: 0.991 seconds, Fetched: 1 row(s)
hive> insert into master_data_science(id, name, address) values (2, 'Luke', 'Ireland');
Query ID = vagrant_20211217230608_7027547e-fc16-49a7-8492-be652e6f0466
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 23:06:10,941 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local2019668785_0003
Loading data to table default.master_data_science
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 11302 HDFS Write: 6976 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 3.176 seconds
hive> insert into master_data_science(id, name, address) values (3, 'Tan', 'Vietnam');
Query ID = vagrant_20211217230611_93cd073a-7113-4bc8-938f-4c73653c8954
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 23:06:14,281 Stage-1 map = 0%,  reduce = 0%
2021-12-17 23:06:15,307 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1142799033_0004
Loading data to table default.master_data_science
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 11468 HDFS Write: 8762 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 4.406 seconds
hive> insert into master_data_science(id, name, address) values (4, 'Li', 'China');
Query ID = vagrant_20211217230615_4acb86e9-6fdb-4d04-9237-6e209e52ae6b
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 23:06:18,599 Stage-1 map = 100%,  reduce = 0%
2021-12-17 23:06:19,606 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1331624385_0005
Loading data to table default.master_data_science
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 11634 HDFS Write: 10524 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 4.233 seconds
hive> insert into master_data_science(id, name, address) values (5, 'Omar', 'Ireland');
Query ID = vagrant_20211217230620_66dff697-4bff-4d60-bcca-f9353b30d5f9
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 23:06:23,617 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local2029348098_0006
Loading data to table default.master_data_science
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 11800 HDFS Write: 12328 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 3.589 seconds
hive> insert into master_data_science(id, name, address) values (6, 'Raymonds', 'India');
Query ID = vagrant_20211217230624_88f14436-5338-4221-b181-c84013c1a3db
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 23:06:26,771 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1952125717_0007
Loading data to table default.master_data_science
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 11966 HDFS Write: 14146 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 3.128 seconds
hive> insert into master_data_science(id, name, address) values (7, 'Karina', 'Ireland');
Query ID = vagrant_20211217230627_48c98695-d9c3-4557-ae0c-3982b0ff2918
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 23:06:30,361 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1957510371_0008
Loading data to table default.master_data_science
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 12132 HDFS Write: 15952 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 3.505 seconds
hive> insert into master_data_science(id, name, address) values (8, 'Nikkil', 'India');
Query ID = vagrant_20211217230630_e73d4c3b-8829-4692-8fc9-f42c239604e5
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 23:06:33,472 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1800018807_0009
Loading data to table default.master_data_science
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 12298 HDFS Write: 17746 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 3.135 seconds
hive> insert into master_data_science(id, name, address) values (9, 'Abilash', 'India');
Query ID = vagrant_20211217230634_c6c0f4ed-fa74-4552-89a8-6dbde69879f9
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 23:06:36,969 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1085865420_0010
Loading data to table default.master_data_science
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 12464 HDFS Write: 19560 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 3.253 seconds
hive> insert into master_data_science(id, name, address) values (10, 'Taoqi', 'China');
Query ID = vagrant_20211217230637_d1c00590-845f-431f-99a3-34fb13fde49e
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 23:06:40,191 Stage-1 map = 0%,  reduce = 0%
2021-12-17 23:06:41,197 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1393072704_0011
Loading data to table default.master_data_science
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 12630 HDFS Write: 21326 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 4.288 seconds
hive> insert into master_data_science(id, name, address) values (11, 'Ilia', 'Russia');
Query ID = vagrant_20211217230641_b01f322e-87b0-41bc-87cc-8a166369026e
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 23:06:44,424 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1269673692_0012
Loading data to table default.master_data_science
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 12796 HDFS Write: 23110 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 3.254 seconds
hive> insert into master_data_science(id, name, address) values (12, 'Marcelino', 'India');
Query ID = vagrant_20211217230646_cf6693c9-1bfb-4690-a22e-41f3002916d9
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 23:06:50,031 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1095426713_0013
Loading data to table default.master_data_science
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 12962 HDFS Write: 24936 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 3.475 seconds
hive> select * from master_data_science;
OK
1       Derry   Republic of Ireland
2       Luke    Ireland
3       Tan     Vietnam
4       Li      China
5       Omar    Ireland
6       Raymonds        India
7       Karina  Ireland
8       Nikkil  India
9       Abilash India
10      Taoqi   China
11      Ilia    Russia
12      Marcelino       India
Time taken: 0.426 seconds, Fetched: 12 row(s)
hive> update master_data_science set address = 'Republic of Ireland' where address =
    > 'Ireland';
Query ID = vagrant_20211217230728_0559c008-757e-4c5e-8d7a-841314e20725
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 23:07:31,125 Stage-1 map = 100%,  reduce = 0%
2021-12-17 23:07:37,409 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1726180833_0014
Loading data to table default.master_data_science
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 616171 HDFS Write: 176296 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 9.778 seconds
hive> select * from master_data_science;
OK
1       Derry   Republic of Ireland
3       Tan     Vietnam
4       Li      China
6       Raymonds        India
8       Nikkil  India
9       Abilash India
10      Taoqi   China
11      Ilia    Russia
12      Marcelino       India
2       Luke    Republic of Ireland
5       Omar    Republic of Ireland
7       Karina  Republic of Ireland
Time taken: 0.383 seconds, Fetched: 12 row(s)
hive> delete from master_data_science where id in (8,12);
Query ID = vagrant_20211217230804_f84f0107-35a3-4e04-9095-773f1ed7a68f
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 23:08:07,037 Stage-1 map = 100%,  reduce = 0%
2021-12-17 23:08:15,890 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local2002335412_0015
Loading data to table default.master_data_science
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 1700242 HDFS Write: 214018 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 12.176 seconds
hive> select * from master_data_science;
OK
1       Derry   Republic of Ireland
3       Tan     Vietnam
4       Li      China
6       Raymonds        India
9       Abilash India
10      Taoqi   China
11      Ilia    Russia
2       Luke    Republic of Ireland
5       Omar    Republic of Ireland
7       Karina  Republic of Ireland
Time taken: 0.468 seconds, Fetched: 10 row(s)
hive> select * from master_data_science order by id;
Query ID = vagrant_20211217230827_208701fa-b0b0-4f5a-b9bc-c5b9b13feb4e
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2021-12-17 23:08:30,634 Stage-1 map = 100%,  reduce = 0%
2021-12-17 23:08:38,448 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local729625903_0016
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 3126836 HDFS Write: 225750 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
1       Derry   Republic of Ireland
2       Luke    Republic of Ireland
3       Tan     Vietnam
4       Li      China
5       Omar    Republic of Ireland
6       Raymonds        India
7       Karina  Republic of Ireland
9       Abilash India
10      Taoqi   China
11      Ilia    Russia
Time taken: 11.173 seconds, Fetched: 10 row(s)
hive>

-----

vagrant@node01:/app/code$ cat /app/code/student.csv
13,Michael,Ireland
14,Paul,Canada
15,Greg,USA
vagrant@node01:/usr/local/hadoop$ hdfs dfs -ls /user/hive/warehouse
Found 1 items
drwxr-xr-x   - vagrant supergroup          0 2021-12-17 23:08 /user/hive/warehouse/master_data_science
vagrant@node01:/usr/local/hadoop$
vagrant@node01:/usr/local/hadoop$
vagrant@node01:/usr/local/hadoop$
vagrant@node01:/usr/local/hadoop$ hdfs dfs -put /app/code/student.csv /user/hive/warehouse/
2021-12-17 23:31:18,376 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
vagrant@node01:/usr/local/hadoop$ hdfs dfs -ls /user/hive/warehouse             Found 2 items
drwxr-xr-x   - vagrant supergroup          0 2021-12-17 23:08 /user/hive/warehouse/master_data_science
-rw-r--r--   2 vagrant supergroup         46 2021-12-17 23:31 /user/hive/warehouse/student.csv
vagrant@node01:/usr/local/hadoop$


hive> LOAD DATA LOCAL INPATH '/app/code/student.csv' INTO TABLE master_data_science;
FAILED: SemanticException Unable to load data to destination table. Error: The file that you are trying to load does not match the file format of the destination table.
hive> CREATE EXTERNAL TABLE IF NOT EXISTS student
    > (id string,name string, address string)
    > ROW FORMAT DELIMITED
    > FIELDS TERMINATED BY ','
    > STORED AS TEXTFILE
    > LOCATION '/user/hive/warehouse';
OK
Time taken: 1.308 seconds
hive> select * from student;
OK
Failed with exception java.io.IOException:java.io.IOException: Not a file: hdfs://node01:9000/user/hive/warehouse/master_data_science
Time taken: 0.538 seconds
hive> show tables;

